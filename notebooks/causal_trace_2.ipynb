{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing\n",
    "\n",
    "A demonstration of the double-intervention causal tracing method.\n",
    "\n",
    "The strategy used by causal tracing is to understand important\n",
    "states within a transfomer by doing two interventions simultaneously:\n",
    "\n",
    "1. Corrupt a subset of the input.  In our paper, we corrupt the subject tokens\n",
    "   to frustrate the ability of the transformer to accurately complete factual\n",
    "   prompts about the subject.\n",
    "2. Restore a subset of the internal hidden states.  In our paper, we scan\n",
    "   hidden states at all layers and all tokens, searching for individual states\n",
    "   that carry the necessary information for the transformer to recover its\n",
    "   capability to complete the factual prompt.\n",
    "\n",
    "The traces of decisive states can be shown on a heatmap.  This notebook\n",
    "demonstrates the code for conducting causal traces and creating these heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `experiments.causal_trace` module contains a set of functions for running causal traces.\n",
    "\n",
    "In this notebook, we reproduce, demonstrate and discuss the interesting functions.\n",
    "\n",
    "We begin by importing several utility functions that deal with tokens and transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json\n",
    "import string\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import defaultdict, Counter\n",
    "from util import nethook\n",
    "from util.globals import DATA_DIR\n",
    "from experiments.causal_trace import (\n",
    "    ModelAndTokenizer,\n",
    "    layername,\n",
    "    guess_subject,\n",
    "    plot_trace_heatmap,\n",
    ")\n",
    "from experiments.causal_trace import (\n",
    "    make_inputs,\n",
    "    decode_tokens,\n",
    "    find_token_range,\n",
    "    predict_token,\n",
    "    predict_from_input,\n",
    "    collect_embedding_std,\n",
    ")\n",
    "from dsets import KnownsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f02f4549280>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# from uskg.models.unified.prefixtuning import Model\n",
    "from uskg.models.unified import finetune, prefixtuning\n",
    "from uskg.utils.configue import Configure\n",
    "from uskg.utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "from uskg.seq2seq_construction import spider as s2s_spider\n",
    "from uskg.third_party.spider.preprocess.get_tables import dump_db_json_schema\n",
    "from uskg.third_party.spider import evaluation as sp_eval\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# import stanza\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "\n",
    "from experiments import causal_trace_uskg as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer_uskg: hkunlp/from_all_T5_large_prefix_spider_with_cell_value2\n",
      "Using tokenizer_fast: t5-large\n",
      "prefix-tuning sequence length is 10.\n"
     ]
    }
   ],
   "source": [
    "mt_uskg = ctu.ModelAndTokenizer_USKG('t5-large-prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('constructor', 'seq2seq_construction.spider'),\n",
       " ('schema_serialization_with_db_content', True),\n",
       " ('target_with_db_id', False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mt_uskg.task_args.seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.model.pretrain_model.encoder.embed_tokens is mt_uskg.model.pretrain_model.shared, \\\n",
    "mt_uskg.model.pretrain_model.decoder.embed_tokens is mt_uskg.model.pretrain_model.shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.model.preseqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [k for k,v in mt_uskg.model.named_parameters()]\n",
    "# [k for k,v in mt_uskg.model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    enc_sentences=[\"Translate to German: My name is Wolfgang and I live in Berlin\"],\n",
    "    dec_prompts=[\"Mein Name ist Wolfgang\"],\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ctu.run_model_forward_uskg(mt_uskg.model, **inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state']),\n",
       " torch.Size([1, 5, 32102]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys(), out['logits'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32102,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = out[\"logits\"][0, -1].detach().cpu().numpy()\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, -1.8642352),\n",
       " (6, -9.727753),\n",
       " (5, -10.966707),\n",
       " (27, -11.037394),\n",
       " (213, -12.864212)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5 = sorted(list(enumerate(logits)), key=lambda p: -p[1])[:5]\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', ',', '.', 'I', 'where']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mt_uskg.tokenizer.decode([p[0]]) for p in top_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spider dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_train_path = '/home/yshao/Projects/SDR-analysis/data/spider/train+ratsql_graph.json'\n",
    "spider_dev_path = '/home/yshao/Projects/SDR-analysis/data/spider/dev+ratsql_graph.json'\n",
    "spider_db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_spider_dev = ctu.load_raw_dataset(\n",
    "    data_filepath = spider_dev_path,\n",
    "    db_path=spider_db_dir,\n",
    "#     schema_cache=SCHEMA_CACHE\n",
    ")\n",
    "len(raw_spider_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_spider_dev[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.task_args.dataset.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_spider_dev = s2s_spider.DevDataset(\n",
    "    args=mt_uskg.task_args,\n",
    "    raw_datasets=raw_spider_dev,\n",
    "    cache_root='../cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What are the names of all European countries with at least 3 manufacturers?',\n",
       " '| car_1 | continents : contid , continent ( europe ) | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " \"select t1.countryname from countries as t1 join continents as t2 on t1.continent = t2.contid join car_makers as t3 on t1.countryid = t3.country where t2.continent = 'europe' group by t1.countryname having count(*) >= 3;\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = 130\n",
    "processed_spider_dev[_id]['text_in'], \\\n",
    "processed_spider_dev[_id]['struct_in'], \\\n",
    "processed_spider_dev[_id]['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_enc_sentence = f\"{processed_spider_dev[_id]['text_in']}; structed knowledge: {processed_spider_dev[_id]['struct_in']}\"\n",
    "_toks = mt_uskg.tokenizer.tokenize(_enc_sentence)\n",
    "len(_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # _occ_punct = set()\n",
    "\n",
    "# for _id in range(len(processed_spider_dev)):\n",
    "#     ex = processed_spider_dev[_id]\n",
    "# #     _occ_punct.update(set(string.punctuation) & set(ex['seq_out']))\n",
    "#     if '_(' in ex['struct_in']:\n",
    "#         print(_id, ex['question'])\n",
    "#         print(ex['struct_in'])\n",
    "#         print(ex['seq_out'])\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Train set\n",
    "\n",
    "# raw_spider_train = ctu.load_raw_dataset(\n",
    "#     data_filepath = spider_train_path,\n",
    "#     db_path=spider_db_dir,\n",
    "# )\n",
    "# processed_spider_train = s2s_spider.TrainDataset(\n",
    "#     args=mt_uskg.task_args,\n",
    "#     raw_datasets=raw_spider_train,\n",
    "#     cache_root='../cache')\n",
    "# len(processed_spider_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_spider_train[5441]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis sample 1 (ID = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many singers do we have?', 'select count(*) from singer')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = processed_spider_dev[0]\n",
    "ex['question'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "dec_prompt = 'select count(*) from'\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    enc_sentences=[enc_sentence],\n",
    "    dec_prompts=[dec_prompt],\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many singers do we have?; structed knowledge: | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id</s>',\n",
       " '<pad> select count(*) from')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.decode(inp['input_ids'][0]), mt_uskg.tokenizer.decode(inp['decoder_input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ctu.run_model_forward_uskg(mt_uskg.model, **inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state']),\n",
       " torch.Size([1, 7, 32102]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys(), out['logits'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32102,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = out[\"logits\"][0, -1].detach().cpu().numpy()\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7634, -5.629301),\n",
       " (6721, -15.1248665),\n",
       " (10159, -17.77869),\n",
       " (2377, -18.263933),\n",
       " (8782, -18.631098)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5 = sorted(list(enumerate(logits)), key=lambda p: -p[1])[:5]\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['singer', 'vocal', 'sing', 'artist', 'singing']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mt_uskg.tokenizer.decode([p[0]]) for p in top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_range = ctu.find_token_range(mt_uskg.tokenizer, inp[\"input_ids\"][0], 'singer')\n",
    "# e_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 8), (15, 125))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_range, struct_range = ctu.find_text_struct_in_range(mt_uskg.tokenizer, inp[\"input_ids\"][0])\n",
    "text_range, struct_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many singers do we have?; structed knowledge: | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id</s>',\n",
       " 'How many singers do we have?',\n",
       " '| concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb, te = text_range\n",
    "sb, se = struct_range\n",
    "mt_uskg.tokenizer.decode(inp['input_ids'][0]), \\\n",
    "mt_uskg.tokenizer.decode(inp['input_ids'][0][tb:te]), \\\n",
    "mt_uskg.tokenizer.decode(inp['input_ids'][0][sb:se])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_hidden_flow_uskg(): corrupted input: *How *many *singer *s *do *we *have *? *; * *struct *e *d *knowledge *: *| *concert *_ *s *inger *| *stadium * *: *stadium *_ *i *d * *, *location * *, *name * *, *capacity * *, *highest * *, *lowest * *, *average *| *singer * *: *singer *_ *i *d * *, *name * *, *country * *, *song *_ *name * *, *song *_ *release *_ *year * *, *age * *, *is *_ *male *| *concert * *: *concert *_ *i *d * *, *concert *_ *name * *, *theme * *, *stadium *_ *i *d * *, *year *| *singer *_ *in *_ *conce *r *t * *: *concert *_ *i *d * *, *singer *_ *i *d *</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a6ab59e50d4835b614db67be994427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.encoder:   0%|          | 0/1512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3493e4be9d4a49a9a8120a648d6df427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.decoder:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = ctu.calculate_hidden_flow_uskg(\n",
    "    mt_uskg,\n",
    "    enc_sentence=enc_sentence,\n",
    "    dec_prompt=dec_prompt,\n",
    "    subject='singer',\n",
    "    replace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['scores', 'low_score', 'high_score', 'input_ids', 'input_tokens', 'dec_input_ids', 'dec_input_tokens', 'subject_range', 'answer', 'window', 'correct_prediction', 'kind']),\n",
       " True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys(), result['correct_prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2487089356436627e-06"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['low_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctu.plot_trace_heatmap_t5(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "- merged in create_analysis_sample_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = '/home/yshao/Projects/language/language/xsp/data/spider/tables.json'\n",
    "db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmaps = sp_eval.build_foreign_key_map_from_json(table_path)\n",
    "evaluator = sp_eval.Evaluator(db_dir=db_dir, kmaps=kmaps, etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctu.evaluate_hardness.evaluator = evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0, 0, 'hard')"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "_sql_str = 'select t1.birth_date from people as t1 join poker_player as t2 on t1.people_id = t2.people_id order by t2.earnings asc limit 1'\n",
    "db_name = 'poker_player'\n",
    "schema = evaluator.schemas[db_name]\n",
    "_sql = sp_eval.get_sql(schema, _sql_str)\n",
    "sp_eval.count_component1(_sql), sp_eval.count_component2(_sql), sp_eval.count_others(_sql), \\\n",
    "evaluator.eval_hardness(_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hard'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.evaluate_hardness(_sql_str, db_name, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<uskg.third_party.spider.evaluation.Evaluator at 0x7f4a744a8d90>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.evaluate_hardness.evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_prompt = 'select avg(age), min(age), max(age) from'\n",
    "ctu.detect_node_role(dec_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, ['car_makers', 'cars_data', 'car_names', 'model_list'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dicts = ctu.create_analysis_sample_dicts(\n",
    "    mt=mt_uskg,\n",
    "    ex=processed_spider_dev[100],\n",
    "    subject_type='table'\n",
    ")\n",
    "len(a_dicts), [d['expect'] for d in a_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'partial'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex = a_dicts[2]\n",
    "ctu.check_table_text_match(a_ex, 'car_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the name of the different car makers who produced a car in 1970?'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex['text_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp0: Study the influence of corrupting a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the average, minimum, and maximum age for all French singers?',\n",
       " \"select avg(age), min(age), max(age) from singer where country = 'France'\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = 5\n",
    "ex = processed_spider_dev[_id]\n",
    "ex['question'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('select avg(age), min(age), max(age) from singer where', 'country')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "\n",
    "# expect = 'singer'\n",
    "# expect = 'age'\n",
    "expect = 'country'\n",
    "dec_prompt = ctu.make_dec_prompt(ex['seq_out'], expect)\n",
    "\n",
    "ex['enc_sentence'] = enc_sentence\n",
    "ex['dec_prompt'] = dec_prompt\n",
    "ex['expect'] = expect\n",
    "dec_prompt, expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_toks = decode_tokens(mt_uskg.tokenizer, mt_uskg.tokenizer.encode(expect, add_special_tokens=False))\n",
    "ans_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999990463256836, 'country')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence],\n",
    "    [dec_prompt],\n",
    "    answer=expect,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "answer_len = len(mt_uskg.tokenizer.tokenize(expect))\n",
    "with torch.no_grad():\n",
    "    answers_t, base_score = [d[0] for d in ctu.predict_from_input_uskg_multi_token(mt_uskg.model, inp, pred_len=answer_len)]\n",
    "base_score = base_score.min().item()\n",
    "answer = ctu.decode_sentences(mt_uskg.tokenizer, answers_t)\n",
    "base_score, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([684], device='cuda:0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0bf9cb15134342bb9b3c4d40ebd864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Corrupt effect: columns:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15951d5486094c779eaf34568678d9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Corrupt effect: tables:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_d = ctu.token_corruption_influence_uskg(\n",
    "    mt_uskg,\n",
    "#     enc_sentence=enc_sentence,\n",
    "#     dec_prompt=dec_prompt,\n",
    "#     expect=expect,\n",
    "    ex,\n",
    "    replace=True,\n",
    "    use_tqdm=True,\n",
    "    skips=('token',)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['enc_sentence', 'dec_prompt', 'expect', 'base_score', 'answers_t', 'answer', 'res_list'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = result_d['res_list']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[d for d in result if d['corrpt_type'] != 'token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID = 503\n",
    "\n",
    "_seq_len = len([d for d in result if d['corrpt_type'] == 'token'])\n",
    "_tags = [[False, False, False] for _ in range(_seq_len)]  # (is_span_start, is_span_end, is_unit)\n",
    "for i, d in enumerate(result):\n",
    "    if d['corrpt_drop'] > 0.5:\n",
    "        s, e = d['corrpt_idx']\n",
    "        if e - s > 1:\n",
    "            _tags[s][0] = True\n",
    "            _tags[e-1][1] = True\n",
    "        else:\n",
    "            _tags[s][2] = True\n",
    "\n",
    "l = []\n",
    "for i in range(_seq_len):\n",
    "    id_str = str(i)\n",
    "    if _tags[i][2]:\n",
    "        id_str = f'*{i}*'\n",
    "    if _tags[i][0]:\n",
    "        id_str = 'S' + id_str\n",
    "    if _tags[i][1]:\n",
    "        id_str = id_str + 'E'\n",
    "    \n",
    "    d = result[i]\n",
    "    l.append(f'{id_str}\\t{d[\"corrpt_token\"]}\\t{d[\"corrpt_drop\"]}')\n",
    "#         l.append(f'{i}\\t{d[\"corrpt_token\"]}\\t{d[\"corrpt_drop\"]}')\n",
    "print('\\n'.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp3 (moved to py script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "expect_type = 'column'\n",
    "\n",
    "res_dir = '/home/yshao/Projects/rome/results/exp3_relational_nodes_mutual'\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "res_path = os.path.join(res_dir, f'exp=3.0_dev_{expect_type}.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(res_path, 'r') as f:\n",
    "    all_results = [json.loads(l) for l in f]\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['enc_sentence', 'seq_out', 'dec_prompt', 'expect', 'expect_type', 'db_id', 'expect_input_ranges', 'expect_table', 'answer', 'base_score', 'answers_t', 'correct_prediction', 'category', 'res_list', 'ex_id']),\n",
       " dict_keys(['corrpt_type', 'corrpt_idx', 'corrpt_token', 'corrpt_score', 'corrpt_drop']))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[0].keys(), all_results[0]['res_list'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-time patches\n",
    "# # patch 1, for those d without \"correct_prediction\", set it True \n",
    "# # patch 2, for each d, add \"expect_table\" which is table of \"expect\" column \n",
    "# # patch 3, for each d, add ex_id \n",
    "# # patch 4, remove d where db_id = 'orchestra' and expect = 'orchestra' (it is duplicated in struct, should be skipped for now)\n",
    "\n",
    "# in_path = os.path.join(res_dir, 'exp=3.0_dev_column_old.jsonl')\n",
    "# out_path = os.path.join(res_dir, 'exp=3.0_dev_column.jsonl')\n",
    "\n",
    "\n",
    "# with open(in_path, 'r') as f:\n",
    "#     all_results = [json.loads(l) for l in f]\n",
    "\n",
    "# keep_results = []\n",
    "# for d in all_results:\n",
    "#     if (d['db_id'] == 'orchestra') and (d['expect'] == 'orchestra'):\n",
    "#         continue\n",
    "#     keep_results.append(d)\n",
    "\n",
    "# # ex_id = 0\n",
    "# # for d in all_results:\n",
    "# #     d_text_in = d['enc_sentence'].split(ctu.USKG_SPLITTER)[0]\n",
    "# #     while True:\n",
    "# #         spider_ex = processed_spider_dev[ex_id]\n",
    "# #         if spider_ex['text_in'] != d_text_in:\n",
    "# #             ex_id += 1\n",
    "# #         else:\n",
    "# #             break\n",
    "# # #     db_id = spider_ex['db_id']\n",
    "# # #     col2table = db_col2table_cache[db_id]\n",
    "# # #     d['expect_table'] = col2table[d['expect']][0]\n",
    "# #     d['ex_id'] = ex_id\n",
    "\n",
    "# with open(out_path, 'w') as f:\n",
    "#     for d in keep_results:\n",
    "#         f.write(json.dumps(d, indent=None) + '\\n')\n",
    "\n",
    "# # ex_id\n",
    "# len(keep_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('self', 1445),\n",
       " ('self_table', 1445),\n",
       " ('self_col', 1445),\n",
       " ('other_col', 23164),\n",
       " ('other_table', 4787),\n",
       " ('self_col_max', 1445),\n",
       " ('other_col_max', 1445),\n",
       " ('other_table_max', 1445)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_col2table_cache = dict()   # db_id -> col2table\n",
    "\n",
    "scores_per_rel = {\n",
    "    k: [] \n",
    "    for k in ['self', 'self_table', 'self_col', \n",
    "              'other_col', 'other_table', 'self_col_max', 'other_col_max', 'other_table_max']}\n",
    "n_corr_pred = 0\n",
    "\n",
    "for d in all_results:\n",
    "    if not d['correct_prediction']:\n",
    "        continue\n",
    "        \n",
    "    n_corr_pred += 1\n",
    "    ex_id = d['ex_id']\n",
    "    spider_ex = processed_spider_dev[ex_id]\n",
    "    db_id = spider_ex['db_id']\n",
    "    \n",
    "    # TODO: make this a function: get_col2table(struct_in)\n",
    "    if db_col2table_cache.get(db_id) is None:\n",
    "        struct_in = spider_ex['struct_in']\n",
    "        parsed_struct_in = ctu.parse_struct_in(struct_in)\n",
    "        # parsed_struct_in_cache[db_id] = parsed_struct_in\n",
    "    \n",
    "        col2table = defaultdict(list)\n",
    "        db_id_t, tables = parsed_struct_in\n",
    "        for table_name_t, cols in tables:\n",
    "            for col_name_t, vals in cols:\n",
    "                _, table_name, _ = table_name_t\n",
    "                _, col_name, _ = col_name_t\n",
    "                col2table[col_name].append(table_name)\n",
    "        db_col2table_cache[db_id] = col2table\n",
    "    col2table = db_col2table_cache[db_id]\n",
    "    \n",
    "    expect = d['expect']\n",
    "    tab = d['expect_table']\n",
    "    \n",
    "    corrupt_drop_dict = dict()\n",
    "    \n",
    "    corrupt_drop_dict['self'] = None\n",
    "    corrupt_drop_dict['self_table'] = None\n",
    "    corrupt_drop_dict['self_col'] = []\n",
    "    corrupt_drop_dict['other_col'] = []\n",
    "    corrupt_drop_dict['other_table'] = []\n",
    "    for res in d['res_list']:\n",
    "        _drop = res['corrpt_drop']\n",
    "        _is_other = True\n",
    "        if res['corrpt_token'] == expect:\n",
    "            corrupt_drop_dict['self'] = _drop\n",
    "            _is_other = False\n",
    "            if (res['corrpt_type'] == 'column') and (d['expect_type'] == 'column'):\n",
    "                corrupt_drop_dict['self_col'].append(_drop)\n",
    "        if res['corrpt_token'] == tab:\n",
    "            corrupt_drop_dict['self_table'] = _drop\n",
    "            _is_other = False\n",
    "        if (res['corrpt_type'] == 'column') and (d['expect_type'] == 'table'):\n",
    "            _c = res['corrpt_token']\n",
    "            _t = expect\n",
    "            if _t in col2table[_c]:\n",
    "                # corrupted is a column of expect table \n",
    "                corrupt_drop_dict['self_col'].append(_drop)\n",
    "                # print(_c, _t)\n",
    "                is_other = False\n",
    "        \n",
    "        if not _is_other:\n",
    "            continue    \n",
    "        if res['corrpt_type'] == 'column':\n",
    "            corrupt_drop_dict['other_col'].append(_drop)\n",
    "        elif res['corrpt_type'] == 'table':\n",
    "            corrupt_drop_dict['other_table'].append(_drop)\n",
    "            \n",
    "    # corrupt_drop_dict['self_col_max'] = max(corrupt_drop_dict['self_col']) if corrupt_drop_dict['self_col'] else None\n",
    "    if corrupt_drop_dict['self_col']:\n",
    "        corrupt_drop_dict['self_col_max'] = max(corrupt_drop_dict['self_col'])\n",
    "    corrupt_drop_dict['other_col_max'] = max(corrupt_drop_dict['other_col'])\n",
    "    corrupt_drop_dict['other_table_max'] = max(corrupt_drop_dict['other_table'])\n",
    "    \n",
    "#     scores_per_rel['self'].append(corrupt_drop_dict['self'])\n",
    "#     scores_per_rel['self_table'].append(corrupt_drop_dict['self_table'])\n",
    "#     scores_per_rel['self_col'].extend(corrupt_drop_dict['self_col'])\n",
    "#     scores_per_rel['other_col'].extend(corrupt_drop_dict['other_col'])\n",
    "#     scores_per_rel['other_table'].extend(corrupt_drop_dict['other_table'])\n",
    "#     scores_per_rel['other_col_max'].append(corrupt_drop_dict['other_col_max'])\n",
    "#     scores_per_rel['other_table_max'].append(corrupt_drop_dict['other_table_max'])\n",
    "#     if corrupt_drop_dict['self_col_max'] is not None:\n",
    "#         scores_per_rel['self_col_max'].append(corrupt_drop_dict['self_col_max'])\n",
    "    for k, v in corrupt_drop_dict.items():\n",
    "        scores_per_rel[k].extend(ctu.ensure_list(v))\n",
    "    \n",
    "    d['corrupt_drop_dict'] = corrupt_drop_dict\n",
    "    \n",
    "[(k, len(scores_per_rel[k])) for k in scores_per_rel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('other_col', False),\n",
       " ('other_col_max', False),\n",
       " ('other_table', False),\n",
       " ('other_table_max', False),\n",
       " ('self', False),\n",
       " ('self_col', False),\n",
       " ('self_col_max', False),\n",
       " ('self_table', False)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(k, (None in l)) for k, l in scores_per_rel.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('self', 0.6091220908314937),\n",
       " ('self_table', 0.06457461528439588),\n",
       " ('self_col', 0.6098740620477409),\n",
       " ('other_col', 0.0032386777438926126),\n",
       " ('other_table', 0.008444325884166957),\n",
       " ('self_col_max', 0.6098740620477409),\n",
       " ('other_col_max', 0.039504879676591824),\n",
       " ('other_table_max', 0.027120517446089714)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, np.mean(scores_per_rel[k])) for k in scores_per_rel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# self_table_effective_ids = []\n",
    "\n",
    "# for i, d in enumerate(all_results):\n",
    "#     self_table_drop = d.get('self_table_drop', 0.0)\n",
    "#     if self_table_drop > 0.5:\n",
    "#         self_table_effective_ids.append(i)\n",
    "# len(self_table_effective_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_col_effective_ids = []\n",
    "\n",
    "for i, d in enumerate(all_results):\n",
    "    self_col_drop = d.get('self_col_max_drop', 0.0)\n",
    "    if (self_col_drop is not None) and (self_col_drop > 0.5):\n",
    "        self_col_effective_ids.append(i)\n",
    "len(self_col_effective_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[135, 184, 190, 191, 194, 196, 200, 203, 204, 206, 207, 209, 224, 226, 235, 246, 247, 268, 273, 275, 278, 280, 281, 282, 285, 287, 294, 298, 316, 395, 402, 423, 446, 459, 463, 467, 471, 489, 491, 495, 531, 638, 642, 711, 719, 720, 750, 763, 764, 781, 794, 795, 842, 843, 848, 977, 978, 1020, 1022, 1025, 1038, 1039, 1040, 1044, 1061, 1065, 1071, 1118, 1302, 1304, 1308, 1310, 1358, 1359, 1360, 1414, 1416, 1418, 1422, 1424, 1440, 1442, 1443, 1456, 1469, 1474, 1486, 1489, 1499, 1641, 1644, 1667, 1671, 1689, 1690]\n"
     ]
    }
   ],
   "source": [
    "print(self_col_effective_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Any type of samples make self-table effective? More with JOIN \n",
    "\n",
    "for i in self_col_effective_ids:\n",
    "    res_d = all_results[i]\n",
    "    print(i, res_d['dec_prompt'], '-->', res_d['expect'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_id = 263\n",
    "res_d = all_results[_id]\n",
    "result = res_d['res_list']\n",
    "# _seq_len = max([d['corrpt_idx'][1] - 1 for d in result])\n",
    "# _tags = [[False, False, False] for _ in range(_seq_len)]  # (is_span_start, is_span_end, is_unit)\n",
    "# for i, d in enumerate(result):\n",
    "#     if d['corrpt_drop'] > 0.5:\n",
    "#         s, e = d['corrpt_idx']\n",
    "#         if e - s > 1:\n",
    "#             _tags[s][0] = True\n",
    "#             _tags[e-1][1] = True\n",
    "#         else:\n",
    "#             _tags[s][2] = True\n",
    "\n",
    "# spider_ex = processed_spider_dev[res_d['ex_id']]\n",
    "print(res_d['enc_sentence'])\n",
    "print(res_d['dec_prompt'], '-->', res_d['expect'])\n",
    "print()\n",
    "\n",
    "l = []\n",
    "for d in result:\n",
    "    _prefix = ''\n",
    "    db_id = spider_ex['db_id']\n",
    "    \n",
    "    if d[\"corrpt_type\"] == 'column' and d[\"corrpt_token\"] == res_d['expect']:\n",
    "        _prefix = '**'\n",
    "    elif d[\"corrpt_type\"] == 'table' and d[\"corrpt_token\"] == res_d['expect_table']:\n",
    "        _prefix = '*>'\n",
    "    l.append(f'{_prefix}{d[\"corrpt_type\"]}\\t{d[\"corrpt_token\"]}\\t{d[\"corrpt_drop\"]}')\n",
    "#         l.append(f'{i}\\t{d[\"corrpt_token\"]}\\t{d[\"corrpt_drop\"]}')\n",
    "print('\\n'.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by different aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_rel_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))      # [asp_k, asp_v, rel] -> scores \n",
    "scores_avg_per_rel_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(float))) # [asp_k, asp_v, rel] -> avg \n",
    "scores_cnt_per_rel_by_aspect = defaultdict(lambda: defaultdict(int))  # [asp_k, asp_v] -> scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in all_results:\n",
    "    if 'corrupt_drop_dict' not in d:\n",
    "        continue\n",
    "    for asp_k, asp_v in d['category'].items():\n",
    "        scores_cnt_per_rel_by_aspect[asp_k][asp_v] += 1\n",
    "        for rel_k, rel_score in d['corrupt_drop_dict'].items():\n",
    "            scores_per_rel_by_aspect[asp_k][asp_v][rel_k].extend(ctu.ensure_list(rel_score))\n",
    "\n",
    "for asp_k, d1 in scores_per_rel_by_aspect.items():\n",
    "    for asp_v, d2 in d1.items():\n",
    "        for rel_k, rel_scores in d2.items():\n",
    "            avg_score = np.mean(rel_scores)\n",
    "            scores_avg_per_rel_by_aspect[asp_k][asp_v][rel_k] = avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asp_k, asp_v, rel_k, rel_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'sql_hardness': defaultdict(int,\n",
       "                         {'easy': 252,\n",
       "                          'medium': 587,\n",
       "                          'hard': 304,\n",
       "                          'extra': 413}),\n",
       "             'node_role': defaultdict(int,\n",
       "                         {'from': 1096, 'join': 454, 'select': 4, 'where': 2}),\n",
       "             'text_match': defaultdict(int,\n",
       "                         {'exact': 1051, 'no-match': 362, 'partial': 143})})"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cnt_per_rel_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_avg_per_rel_by_aspect['overall'] = {k : np.mean(scores_per_rel[k]) for k in scores_per_rel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_avg_per_rel_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_d = ctu.nested_json_processing(scores_avg_per_rel_by_aspect, func=lambda x: np.format_float_positional(x, precision=4, min_digits=4))\n",
    "\n",
    "for asp_k, d1 in dump_d.items():\n",
    "    if asp_k == 'overall':\n",
    "        d1['n_samples'] = len(scores_per_rel['self'])\n",
    "        continue\n",
    "    for asp_v, d2 in d1.items():\n",
    "        d2['n_samples'] = scores_cnt_per_rel_by_aspect[asp_k][asp_v]\n",
    "        \n",
    "dump_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '/home/yshao/Projects/rome/results/exp3_relational_nodes_mutual'\n",
    "dump_path = os.path.join(res_dir, f'summ-exp=3.0_dev_{expect_type}.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dump_path, 'w') as f:\n",
    "    json.dump(dump_d, f, indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('easy', 252), ('medium', 587), ('hard', 304), ('extra', 413)]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hardness\n",
    "\n",
    "samples_by_hardness = defaultdict(list)\n",
    "\n",
    "for res_d in all_results:\n",
    "    if not res_d['correct_prediction']:\n",
    "        continue\n",
    "#     hardness = spider_id2hardness[res_d['ex_id']]\n",
    "    hardness = res_d['category']['sql_hardness']\n",
    "    samples_by_hardness[hardness].append(res_d)\n",
    "\n",
    "[(h, len(samples)) for h, samples in samples_by_hardness.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardness: easy (252)\n",
      "self\t0.7473\t\n",
      "self_table\t0.7473\t\n",
      "self_col\t0.0067\t\n",
      "other_col\t0.0035\t\n",
      "other_table\t0.0007\t\n",
      "self_col_max\t0.0349\t\n",
      "other_col_max\t0.0460\t\n",
      "other_table_max\t0.0028\t\n",
      "\n",
      "Hardness: medium (587)\n",
      "self\t0.7537\t\n",
      "self_table\t0.7537\t\n",
      "self_col\t0.0147\t\n",
      "other_col\t0.0062\t\n",
      "other_table\t0.0072\t\n",
      "self_col_max\t0.0711\t\n",
      "other_col_max\t0.0741\t\n",
      "other_table_max\t0.0274\t\n",
      "\n",
      "Hardness: hard (304)\n",
      "self\t0.7050\t\n",
      "self_table\t0.7050\t\n",
      "self_col\t0.0154\t\n",
      "other_col\t0.0084\t\n",
      "other_table\t0.0087\t\n",
      "self_col_max\t0.0787\t\n",
      "other_col_max\t0.0883\t\n",
      "other_table_max\t0.0377\t\n",
      "\n",
      "Hardness: extra (413)\n",
      "self\t0.7371\t\n",
      "self_table\t0.7371\t\n",
      "self_col\t0.0203\t\n",
      "other_col\t0.0076\t\n",
      "other_table\t0.0035\t\n",
      "self_col_max\t0.0814\t\n",
      "other_col_max\t0.0832\t\n",
      "other_table_max\t0.0290\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for h in ['easy', 'medium', 'hard', 'extra']:\n",
    "    samples = samples_by_hardness[h]\n",
    "    print(f'Hardness: {h} ({len(samples)})')\n",
    "    # print('[Example sql]', samples[0]['seq_out'])\n",
    "    \n",
    "    for rel in scores_per_rel.keys():\n",
    "        _rel_k = rel + '_drop'\n",
    "        rel_scores = [x for res_d in samples for x in ctu.ensure_list(res_d[_rel_k]) if x is not None]\n",
    "        avg_score = np.mean(rel_scores, )\n",
    "        print(f'{rel}\\t{avg_score:.4f}\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rel_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('from', 1096), ('join', 454), ('select', 4), ('where', 2)]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Node role\n",
    "\n",
    "samples_by_node_role = defaultdict(list)\n",
    "\n",
    "for res_d in all_results:\n",
    "    if not res_d['correct_prediction']:\n",
    "        continue\n",
    "#     col_role_kw = _detect_column_role(res_d['dec_prompt'])\n",
    "    role_kw = res_d['category']['node_role']\n",
    "    samples_by_node_role[role_kw].append(res_d)\n",
    "\n",
    "[(r, len(samples)) for r, samples in samples_by_node_role.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column role: from (1096)\n",
      "self\t0.7255\t\n",
      "self_table\t0.7255\t\n",
      "self_col\t0.0109\t\n",
      "other_col\t0.0058\t\n",
      "other_table\t0.0004\t\n",
      "self_col_max\t0.0608\t\n",
      "other_col_max\t0.0686\t\n",
      "other_table_max\t0.0081\t\n",
      "\n",
      "Column role: join (454)\n",
      "self\t0.7804\t\n",
      "self_table\t0.7804\t\n",
      "self_col\t0.0288\t\n",
      "other_col\t0.0086\t\n",
      "other_table\t0.0168\t\n",
      "self_col_max\t0.0866\t\n",
      "other_col_max\t0.0907\t\n",
      "other_table_max\t0.0691\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r, samples in samples_by_node_role.items():\n",
    "    if len(samples) < 10:\n",
    "        continue\n",
    "    print(f'Column role: {r} ({len(samples)})')\n",
    "    # print('[Example sql]', samples[0]['seq_out'])\n",
    "    \n",
    "    for rel in scores_per_rel.keys():\n",
    "        _rel_k = rel + '_drop'\n",
    "        rel_scores = [x for res_d in samples for x in ctu.ensure_list(res_d[_rel_k]) if x is not None]\n",
    "        avg_score = np.mean(rel_scores)\n",
    "        print(f'{rel}\\t{avg_score:.4f}\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exact', 1051), ('partial', 143), ('no-match', 362)]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text match \n",
    "\n",
    "samples_by_text_match = {k: [] for k in ['exact', 'partial', 'no-match']}\n",
    "\n",
    "for res_d in all_results:\n",
    "    if not res_d['correct_prediction']:\n",
    "        continue\n",
    "#     spider_ex = processed_spider_dev[res_d['ex_id']]\n",
    "#     text_match = ctu.check_text_match(spider_ex, col=res_d['expect'], tab=res_d['expect_table'])\n",
    "    text_match = res_d['category']['text_match']\n",
    "    samples_by_text_match[text_match].append(res_d)\n",
    "\n",
    "[(m, len(samples)) for m, samples in samples_by_text_match.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-match: exact (1051)\n",
      "self\t0.6815\t\n",
      "self_table\t0.6815\t\n",
      "self_col\t0.0061\t\n",
      "other_col\t0.0038\t\n",
      "other_table\t0.0039\t\n",
      "self_col_max\t0.0303\t\n",
      "other_col_max\t0.0380\t\n",
      "other_table_max\t0.0168\t\n",
      "\n",
      "Text-match: partial (143)\n",
      "self\t0.8419\t\n",
      "self_table\t0.8419\t\n",
      "self_col\t0.0656\t\n",
      "other_col\t0.0130\t\n",
      "other_table\t0.0028\t\n",
      "self_col_max\t0.2214\t\n",
      "other_col_max\t0.1938\t\n",
      "other_table_max\t0.0644\t\n",
      "\n",
      "Text-match: no-match (362)\n",
      "self\t0.8642\t\n",
      "self_table\t0.8642\t\n",
      "self_col\t0.0259\t\n",
      "other_col\t0.0116\t\n",
      "other_table\t0.0116\t\n",
      "self_col_max\t0.1361\t\n",
      "other_col_max\t0.1344\t\n",
      "other_table_max\t0.0370\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m, samples in samples_by_text_match.items():\n",
    "    print(f'Text-match: {m} ({len(samples)})')\n",
    "    # print('[Example sql]', samples[0]['seq_out'])\n",
    "    \n",
    "    for rel in scores_per_rel.keys():\n",
    "        _rel_k = rel + '_drop'\n",
    "        rel_scores = [x for res_d in samples for x in ctu.ensure_list(res_d[_rel_k]) if x is not None]\n",
    "        avg_score = np.mean(rel_scores)\n",
    "        print(f'{rel}\\t{avg_score:.4f}\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-1: column/table corruption\n",
    "- In py script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'dev_table_alias_encoder-tmp'\n",
    "\n",
    "res_json_path = f'/home/yshao/Projects/rome/results/exp1_struct_node_restore/{exp_name}.jsonl'\n",
    "with open(res_json_path, 'r') as f:\n",
    "    res_dicts = [json.loads(l) for l in f if l]\n",
    "#     all_str = f.read()\n",
    "# all_str = all_str.replace('{\"ex_id\":', '\\n{\"ex_id\":').strip()\n",
    "# res_dicts = [json.loads(l) for l in all_str.split('\\n')]\n",
    "len(res_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([r for d in res_dicts for r in d['trace_results']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_trace_results = []\n",
    "for d in res_dicts:\n",
    "    for r in d['trace_results']:\n",
    "        if r['correct_prediction'] and r['is_good_sample']:\n",
    "            r['ex_id'] = d['ex_id']\n",
    "            good_trace_results.append(r)\n",
    "len(good_trace_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['ex_id', 'trace_results']),\n",
       " dict_keys(['enc_sentence', 'seq_out', 'dec_prompt', 'expect', 'expect_type', 'db_id', 'expect_input_ranges', 'category', 'low_score', 'high_score', 'input_ids', 'input_tokens', 'dec_input_ids', 'dec_input_tokens', 'subject_range', 'subject_range_individual_indices', 'answer', 'window', 'correct_prediction', 'kind', 'sever_kind', 'scores', 'is_good_sample', 'ex_id']),\n",
       " list)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys(), good_trace_results[0].keys(), type(good_trace_results[0]['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccd9ea85445469dbb43564e8e0af1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.\n"
     ]
    }
   ],
   "source": [
    "# Generating all plots\n",
    "fig_save_dir = f'/home/yshao/Projects/rome/results/figs/exp1_struct_node_restore/{exp_name}'\n",
    "\n",
    "for i, r in enumerate(tqdm(good_trace_results)):\n",
    "    result = dict(r)\n",
    "\n",
    "    enc_s, dec_s = result['scores']\n",
    "    enc_s = np.array(enc_s)\n",
    "    dec_s = np.array(dec_s)\n",
    "    result['scores'] = [enc_s, dec_s]\n",
    "\n",
    "    ex_id = r['ex_id']\n",
    "    ctu.plot_trace_heatmap_t5(result, savepdf=os.path.join(fig_save_dir, f'{i}-ex_id={ex_id}.pdf'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding special samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57-ex_id=69\n",
      "60-ex_id=70\n",
      "83-ex_id=139\n",
      "87-ex_id=143\n",
      "88-ex_id=144\n",
      "89-ex_id=145\n",
      "91-ex_id=153\n"
     ]
    }
   ],
   "source": [
    "for i, r in enumerate(good_trace_results):\n",
    "    _, dec_s = r['scores']\n",
    "    ## dec_s: (n_toks, n_layers)\n",
    "    non_last_token_s = np.array(dec_s)[:-1]\n",
    "    if (non_last_token_s > 0.5).any():\n",
    "        print(f'{i}-ex_id={r[\"ex_id\"]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-1.1: severing decoder cross-attention\n",
    "- TODO: merge to py script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single sample (ID = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = 2\n",
    "ex = processed_spider_dev[_id]\n",
    "ex['question'], ex['struct_in'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "enc_tokenized = mt_uskg.tokenizer(enc_sentence)\n",
    "\n",
    "token_ranges_dict = ctu.find_struct_name_ranges(mt_uskg.tokenizer, enc_tokenized['input_ids'], struct_in)\n",
    "token_ranges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'name'\n",
    "tok_ranges = token_ranges_dict['col_name_ranges'][col]\n",
    "tok_indices = [i for s, e in tok_ranges for i in range(s, e)]\n",
    "\n",
    "dec_prompt = make_dec_prompt(ex['seq_out'], col)\n",
    "\n",
    "dec_prompt, tok_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ctu.calculate_hidden_flow_uskg(\n",
    "    mt_uskg,\n",
    "    enc_sentence=enc_sentence,\n",
    "    dec_prompt=dec_prompt,\n",
    "    expect=col,\n",
    "    e_range=tok_indices,\n",
    "    enc_token_range=[],    # no analysis\n",
    "    dec_token_range=None,  # full analysis\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    "    sever_kind='self_attn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no sever\n",
    "ctu.plot_trace_heatmap_t5(result)\n",
    "print([f\"{s:.2f}\" for s in result['scores'][1][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sever cross_attn\n",
    "ctu.plot_trace_heatmap_t5(result)\n",
    "print([f\"{s:.2f}\" for s in result['scores'][1][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sever mlp; a bit better than no severing??\n",
    "ctu.plot_trace_heatmap_t5(result)\n",
    "print([f\"{s:.2f}\" for s in result['scores'][1][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sever self_attn\n",
    "ctu.plot_trace_heatmap_t5(result)\n",
    "print([f\"{s:.2f}\" for s in result['scores'][1][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['scores'][1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-2: dirty text recovery\n",
    "- Corrupt the text, restore different parts of encoder final output\n",
    "- Idea is to check the existence of contextual understanding (incorporating text info into struct representation)\n",
    "- (In py script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single sample\n",
    "- ID = 2, col = 'name': (name exact match with text)\n",
    "    - none: wrong; text: correct; struct: correct; col: correct\n",
    "- ID = 5, col = 'country': (no exact match, but value match with text (French) )\n",
    "    - none: wrong; text: correct; struct: correct; col: correct\n",
    "- ID = 145, col = 'year': (no exact match, but value match with text (1980) )\n",
    "    - none: wrong; text: correct; struct: correct; col: correct\n",
    "- ID = 7, col = 'song_release_year': (multi-token, and with confusing columns in table (song_name) )\n",
    "    - none: wrong; text: wrong; struct: correct; col: wrong\n",
    "    - hypothesis: need clean struct to avoid confusion\n",
    "- ID = 185, col = 'airportcode': (multi-token, with confusing col (airportname), but have match with text (airport code) )\n",
    "    - none: wrong; text: correct; struct: wrong (2nd token); col: wrong\n",
    "    - differs from hypothesis... clean struct should know sql info about the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('concert_singer',\n",
       " 'Show the stadium name and capacity with most number of concerts in year 2014 or after.',\n",
       " '| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id',\n",
       " 'select t2.name, t2.capacity from concert as t1 join stadium as t2 on t1.stadium_id = t2.stadium_id where t1.year >= 2014 group by t2.stadium_id order by count(*) desc limit 1')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = 24\n",
    "ex = processed_spider_dev[_id]\n",
    "ex['db_id'], ex['question'], ex['struct_in'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 17), (24, 134))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "enc_tokenized = mt_uskg.tokenizer(enc_sentence)\n",
    "\n",
    "text_range, struct_range = ctu.find_text_struct_in_range(mt_uskg.tokenizer, enc_tokenized['input_ids'])\n",
    "text_range, struct_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name', 'nationality'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ranges_dict = ctu.find_struct_name_ranges(mt_uskg.tokenizer, enc_tokenized['input_ids'], struct_in)\n",
    "\n",
    "col_name_ranges = token_ranges_dict['col_name_ranges']\n",
    "\n",
    "sql_tokens = ctu.separate_punct(ex['seq_out']).split(' ')\n",
    "\n",
    "sql_cols = set()\n",
    "for t in sql_tokens:\n",
    "    if t in col_name_ranges:\n",
    "        sql_cols.add(t)\n",
    "sql_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 'orchestra', 'orchestra'),\n",
       " [((3, 'conductor', 'conductor'),\n",
       "   [[(5, 'conductor_id', 'conductor_id'), []],\n",
       "    [(7, 'name', 'name'), []],\n",
       "    [(9, 'age', 'age'), []],\n",
       "    [(11, 'nationality', 'nationality ( USA )'), [(13, 'USA', 'USA')]],\n",
       "    [(16, 'year_of_work', 'year_of_work'), []]]),\n",
       "  ((18, 'orchestra', 'orchestra'),\n",
       "   [[(20, 'orchestra_id', 'orchestra_id'), []],\n",
       "    [(22, 'orchestra', 'orchestra'), []],\n",
       "    [(24, 'conductor_id', 'conductor_id'), []],\n",
       "    [(26, 'record_company', 'record_company'), []],\n",
       "    [(28, 'year_of_founded', 'year_of_founded'), []],\n",
       "    [(30, 'major_record_format', 'major_record_format'), []]]),\n",
       "  ((32, 'performance', 'performance'),\n",
       "   [[(34, 'performance_id', 'performance_id'), []],\n",
       "    [(36, 'orchestra_id', 'orchestra_id'), []],\n",
       "    [(38, 'type', 'type'), []],\n",
       "    [(40, 'date', 'date'), []],\n",
       "    [(42, 'official_ratings_(millions)', 'official_ratings_(millions)'), []],\n",
       "    [(44, 'weekly_rank', 'weekly_rank'), []],\n",
       "    [(46, 'share', 'share'), []]]),\n",
       "  ((48, 'show', 'show'),\n",
       "   [[(50, 'show_id', 'show_id'), []],\n",
       "    [(52, 'performance_id', 'performance_id'), []],\n",
       "    [(54, 'if_first_show', 'if_first_show'), []],\n",
       "    [(56, 'result', 'result'), []],\n",
       "    [(58, 'attendance', 'attendance'), []]])])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_struct_in = ctu.parse_struct_in(struct_in)\n",
    "parsed_struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('select name from conductor where', 'nationality')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = 'nationality'\n",
    "# tok_ranges = token_ranges_dict['col_name_ranges'][col]\n",
    "# tok_indices = [i for s, e in tok_ranges for i in range(s, e)]\n",
    "dec_prompt = ctu.make_dec_prompt(ex['seq_out'], col)\n",
    "expect = col\n",
    "dec_prompt, expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nationality', [(45, 51)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_toks = token_ranges_dict['col_name_ranges'][col]\n",
    "col, col_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = ctu.calculate_hidden_flow_uskg(\n",
    "#     mt_uskg,\n",
    "#     enc_sentence=enc_sentence,\n",
    "#     dec_prompt=dec_prompt,\n",
    "#     expect=col,\n",
    "#     e_range=text_range,\n",
    "#     enc_token_range=[],    # no analysis\n",
    "#     dec_token_range=None,  # full analysis\n",
    "#     tokens_to_mix_individual_indices=False,\n",
    "#     replace=True,\n",
    "#     sever_kind=None,\n",
    "# )\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence] * 11,\n",
    "    [dec_prompt] * 11,\n",
    "    answer=expect)\n",
    "\n",
    "encoder_text_last_layer_states = [\n",
    "    (tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "    for tnum in range(*text_range)\n",
    "]\n",
    "\n",
    "encoder_struct_last_layer_states = [\n",
    "    (tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "    for tnum in range(*struct_range)\n",
    "]\n",
    "\n",
    "encoder_col_last_layer_states = [\n",
    "    (tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "    for tnum in col_toks\n",
    "]\n",
    "\n",
    "answer_len = len(mt_uskg.tokenizer.tokenize(expect))\n",
    "answers_t, base_score = [d[0] for d in ctu.predict_from_input_uskg_multi_token(mt_uskg.model, inp, pred_len=answer_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.1734, device='cuda:0'), tensor(1.0000, device='cuda:0')],\n",
       " tensor(0.1734, device='cuda:0'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Corrupting text: expect wrong pred \n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(1.0000, device='cuda:0'), tensor(0.9999, device='cuda:0')],\n",
       " tensor(0.9999, device='cuda:0'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Restoring text encoding for decoder, but struct encoding are with dirty text encoding \n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=encoder_text_last_layer_states,\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(1.0000, device='cuda:0'), tensor(1., device='cuda:0')],\n",
       " tensor(1.0000, device='cuda:0'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Restoring clean struct encoding but dirty text encoding for decoder\n",
    "## Prediction being correct means encoder final output has \"contextual\" or \"semantic\" understanding of struct_in \n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=encoder_struct_last_layer_states,\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(1.0000, device='cuda:0'), tensor(1.0000, device='cuda:0')],\n",
       " tensor(1.0000, device='cuda:0'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Restoring clean col_name encoding but dirty text encoding for decoder (stricter than above)\n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=encoder_col_last_layer_states,\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(1.0000, device='cuda:0'), tensor(1.0000, device='cuda:0')],\n",
       " tensor(1.0000, device='cuda:0'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For exp-2.1: mutual corruption\n",
    "# First pass: corrupt text (no restore)\n",
    "# Second pass: corrupt struct, no restore, reset struct output to first pass\n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=encoder_struct_last_layer_states,\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix_1st_pass=text_range,\n",
    "    tokens_to_mix=struct_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_path = '/home/yshao/Projects/rome/results/exp2_text_struct_interaction/exp=2_dev_table.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_samples[5439]['ex_id'], all_samples[5440]['ex_id'], all_samples[-1]['ex_id'], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5\n",
    "\n",
    "base_scores = []\n",
    "low_scores = []\n",
    "\n",
    "## len = n_good_samples\n",
    "restore_scores_dict = {\n",
    "    'text': [],\n",
    "    'struct': [],\n",
    "    'node': [],\n",
    "    'struct_no_node': [],\n",
    "    'node_corrupt_all': [],\n",
    "    # 2.0.1: cancelled\n",
    "    # 'ctname': [],      # col name + table name (col belongs to) \n",
    "    # 'catname': [],     # col name + all table names \n",
    "    # 'full_table': [],  # full table (col belongs to) \n",
    "    # 'all_col': [],     # all col names (regardless of table)\n",
    "}\n",
    "\n",
    "## len = total_samples\n",
    "mutual_scores_dict = {\n",
    "    f'{text}-{struct}': []\n",
    "    for text in ['clean_t', 'dc_t', 'dirty_t']\n",
    "    for struct in ['clean_s', 'dc_s', 'dirty_s']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1725, (821, 821), 141, 763)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['enc_sentence', 'seq_out', 'dec_prompt', 'expect', 'expect_type', 'db_id', 'expect_input_ranges', 'expect_table', 'answer', 'base_score', 'answers_t', 'correct_prediction', 'category', 'mutual_scores', 'low_score', 'is_good_sample']),\n",
       " dict_keys(['clean_t-clean_s', 'clean_t-dc_s', 'clean_t-dirty_s', 'dc_t-clean_s', 'dc_t-dc_s', 'dc_t-dirty_s', 'dirty_t-clean_s', 'dirty_t-dc_s', 'dirty_t-dirty_s']))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys(), d['mutual_scores'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    base_scores.append(d['base_score'])\n",
    "    low_scores.append(d['low_score'])\n",
    "    for k in restore_scores_dict.keys():\n",
    "        restore_scores_dict[k].append(d[f'r_{k}_score'])\n",
    "    for k in mutual_scores_dict.keys():\n",
    "        mutual_scores_dict[k].append(d['mutual_scores'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1084, 1084, 1084)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_scores), len(restore_scores_dict['text']), len(mutual_scores_dict['dc_t-dc_s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\t0.6152\n",
      "struct\t0.8505\n",
      "node\t0.7290\n",
      "struct_no_node\t0.1517\n",
      "node_corrupt_all\t0.7480\n"
     ]
    }
   ],
   "source": [
    "# results for exp2\n",
    "for k, scores in restore_scores_dict.items():\n",
    "    avg = np.mean(scores)\n",
    "    print(f'{k}\\t{avg:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score\tavg_gain\tperc_recover\n",
      "base_scores\t0.9416\t1.0000\n",
      "r_text\t0.5663\t0.6033\n",
      "r_struct\t0.8016\t0.8542\n",
      "r_node\t0.6801\t0.7352\n",
      "r_struct_no_node\t0.1028\t0.1153\n",
      "r_node_corrupt_all\t0.6990\t0.7445\n"
     ]
    }
   ],
   "source": [
    "print(f'Score\\tavg_gain\\tperc_recover')\n",
    "for score_label, high_scores in [\n",
    "    ('base_scores', base_scores),\n",
    "    ('r_text', restore_scores_dict['text']),\n",
    "    ('r_struct', restore_scores_dict['struct']),\n",
    "    ('r_node', restore_scores_dict['node']),\n",
    "    ('r_struct_no_node', restore_scores_dict['struct_no_node']),\n",
    "    ('r_node_corrupt_all', restore_scores_dict['node_corrupt_all']),\n",
    "]:\n",
    "    avg_gain = np.mean([h - l for h, l in zip(high_scores, low_scores)])\n",
    "    perc_recover = np.mean([h - l > 0.5 for h, l in zip(high_scores, low_scores)])\n",
    "    print(f'{score_label}\\t{avg_gain:.4f}\\t{perc_recover:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        clean_t dc_t    dirty_t \n",
      "clean_s 0.9905  0.9611  0.8505  \n",
      "dc_s    0.6152  0.4952  0.0489  \n",
      "dirty_s 0.3916  0.3469  0.0241  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results for exp2.1.1 \n",
    "msg = ' '*8\n",
    "for k_t in ['clean_t', 'dc_t', 'dirty_t']:\n",
    "    msg += f'{k_t:8s}'\n",
    "msg += '\\n'\n",
    "for k_s in ['clean_s', 'dc_s', 'dirty_s']:\n",
    "    msg += f'{k_s:8s}'\n",
    "    for k_t in ['clean_t', 'dc_t', 'dirty_t']:\n",
    "        k = f'{k_t}-{k_s}'\n",
    "        scores = mutual_scores_dict[k]\n",
    "        avg = np.mean(scores)\n",
    "        msg += f'{avg:.4f}  '\n",
    "    msg += '\\n'\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp2 + exp2.1.1 \n",
    "# res_path = '/home/yshao/Projects/rome/results/exp2_text_struct_interaction/exp=2_dev_column.jsonl'\n",
    "\n",
    "# with open(res_path, 'r') as f:\n",
    "#     all_samples = [json.loads(l) for l in f]\n",
    "# len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_samples = []\n",
    "\n",
    "# for i, ex in enumerate(all_samples):\n",
    "#     for d in ex['trace_results']:\n",
    "#         if d['is_good_sample']:\n",
    "#             d['ex_id'] = i\n",
    "#             good_samples.append(d)\n",
    "# len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_by_hardness = defaultdict(list)\n",
    "\n",
    "# for ex in all_samples:\n",
    "#     for d in ex['trace_results']:\n",
    "#         if not d['is_good_sample']:\n",
    "#             continue\n",
    "\n",
    "for d in good_samples:\n",
    "#     sql_str = d['seq_out']\n",
    "#     db_id = d['struct_in'].split('|')[1].strip()    # TODO: add db_id to result during experiment main run \n",
    "#     sql = sp_eval.get_sql(evaluator.schemas[db_id], sql_str)\n",
    "#     hardness = evaluator.eval_hardness(sql)\n",
    "#     hardness = spider_id2hardness[d['ex_id']]\n",
    "    hardness = d['category']['sql_hardness']\n",
    "    samples_by_hardness[hardness].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('easy', 140), ('medium', 307), ('hard', 168), ('extra', 206)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(h, len(samples)) for h, samples in samples_by_hardness.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardness: easy (140)\n",
      "[Example sql] select count(*) from singer\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.8812\t1.0000\n",
      "r_text_score\t0.3415\t0.3643\n",
      "r_struct_score\t0.8589\t0.9714\n",
      "r_node_score\t0.6934\t0.7714\n",
      "r_struct_no_node_score\t0.4167\t0.4429\n",
      "r_node_corrupt_all_score\t0.8762\t1.0000\n",
      "\n",
      "Hardness: medium (307)\n",
      "[Example sql] select name, country, age from singer order by age desc\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9013\t1.0000\n",
      "r_text_score\t0.3883\t0.4365\n",
      "r_struct_score\t0.8948\t0.9935\n",
      "r_node_score\t0.8010\t0.8925\n",
      "r_struct_no_node_score\t0.3597\t0.4202\n",
      "r_node_corrupt_all_score\t0.8096\t0.8990\n",
      "\n",
      "Hardness: hard (168)\n",
      "[Example sql] select song_name from singer where age > (select avg(age) from singer)\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.8995\t1.0000\n",
      "r_text_score\t0.3341\t0.3750\n",
      "r_struct_score\t0.8428\t0.9286\n",
      "r_node_score\t0.7446\t0.8393\n",
      "r_struct_no_node_score\t0.2623\t0.2798\n",
      "r_node_corrupt_all_score\t0.7794\t0.8690\n",
      "\n",
      "Hardness: extra (206)\n",
      "[Example sql] select t2.name, t2.capacity from concert as t1 join stadium as t2 on t1.stadium_id = t2.stadium_id where t1.year >= 2014 group by t2.stadium_id order by count(*) desc limit 1\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.8656\t1.0000\n",
      "r_text_score\t0.2924\t0.2816\n",
      "r_struct_score\t0.8494\t0.9612\n",
      "r_node_score\t0.7783\t0.8689\n",
      "r_struct_no_node_score\t0.2708\t0.3058\n",
      "r_node_corrupt_all_score\t0.7442\t0.8350\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for h in ['easy', 'medium', 'hard', 'extra']:\n",
    "    samples = samples_by_hardness[h]\n",
    "    print(f'Hardness: {h} ({len(samples)})')\n",
    "    print('[Example sql]', samples[0]['seq_out'])\n",
    "    print(f'Score\\tavg_gain\\tperc_recover')\n",
    "    \n",
    "    low_scores = [d['low_score'] for d in samples]\n",
    "    for score_label in [\n",
    "        'base_score',\n",
    "        'r_text_score',\n",
    "        'r_struct_score',\n",
    "        'r_node_score',\n",
    "        'r_struct_no_node_score',\n",
    "        'r_node_corrupt_all_score'\n",
    "    ]:\n",
    "        high_scores = [d[score_label] for d in samples]\n",
    "        avg_gain = np.mean([h - l for h, l in zip(high_scores, low_scores)])\n",
    "        perc_recover = np.mean([h - l > 0.5 for h, l in zip(high_scores, low_scores)])\n",
    "        print(f'{score_label}\\t{avg_gain:.4f}\\t{perc_recover:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by node role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# role_keyword_pattern = r'\\W(select|where|join|group by|having|order by)\\W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _sql_str = samples_by_hardness['extra'][0]['seq_out']\n",
    "# _sql_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall(role_keyword_pattern, ' ' + _sql_str + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('from', 533), ('join', 282), ('select', 6)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_by_node_role = defaultdict(list)\n",
    "\n",
    "for d in good_samples:\n",
    "#     all_kws = re.findall(role_keyword_pattern, ' ' + d['dec_prompt'] + ' ')\n",
    "#     assert len(all_kws) > 0, d['dec_prompt']\n",
    "#     col_role_kw = all_kws[-1]\n",
    "#     col_role_kw = _detect_column_role(d['dec_prompt'])\n",
    "    role_kw = d['category']['node_role']\n",
    "    samples_by_node_role[role_kw].append(d)\n",
    "\n",
    "[(r, len(samples)) for r, samples in samples_by_node_role.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column role: from (533)\n",
      "[Example prompt] select count(*) from\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.8889\t1.0000\n",
      "r_text_score\t0.3948\t0.4447\n",
      "r_struct_score\t0.8652\t0.9681\n",
      "r_node_score\t0.7885\t0.8799\n",
      "r_struct_no_node_score\t0.3151\t0.3452\n",
      "r_node_corrupt_all_score\t0.8623\t0.9644\n",
      "\n",
      "Column role: join (282)\n",
      "[Example prompt] select t2.name, t2.capacity from concert as t1 join\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.8870\t1.0000\n",
      "r_text_score\t0.2458\t0.2305\n",
      "r_struct_score\t0.8752\t0.9752\n",
      "r_node_score\t0.7379\t0.8262\n",
      "r_struct_no_node_score\t0.3443\t0.4007\n",
      "r_node_corrupt_all_score\t0.6962\t0.7801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in samples_by_node_role.keys():\n",
    "    samples = samples_by_node_role[r]\n",
    "    if len(samples) < 10:\n",
    "        continue\n",
    "    print(f'Column role: {r} ({len(samples)})')\n",
    "    print('[Example prompt]', samples[0]['dec_prompt'])\n",
    "    print(f'Score\\tavg_gain\\tperc_recover')\n",
    "    \n",
    "    low_scores = [d['low_score'] for d in samples]\n",
    "    for score_label in [\n",
    "        'base_score',\n",
    "        'r_text_score',\n",
    "        'r_struct_score',\n",
    "        'r_node_score',\n",
    "        'r_struct_no_node_score',\n",
    "        'r_node_corrupt_all_score'\n",
    "    ]:\n",
    "        high_scores = [d[score_label] for d in samples]\n",
    "        avg_gain = np.mean([h - l for h, l in zip(high_scores, low_scores)])\n",
    "        perc_recover = np.mean([h - l > 0.5 for h, l in zip(high_scores, low_scores)])\n",
    "        print(f'{score_label}\\t{avg_gain:.4f}\\t{perc_recover:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_path = '/home/yshao/Projects/rome/results/exp2_text_struct_interaction/exp=2_dev_column.jsonl'\n",
    "# with open(res_path, 'r') as f:\n",
    "#     all_samples = [json.loads(l) for l in f]\n",
    "# len(all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by text match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = good_samples[244]\n",
    "# spider_ex = processed_spider_dev[d['ex_id']]\n",
    "# col = d['expect']\n",
    "# tab = d['table']\n",
    "# d['table'], d['expect'], d['text_in'], ctu.check_text_match(spider_ex, col, tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exact', 551), ('partial', 98), ('no-match', 172)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_by_text_match = {k: [] for k in ['exact', 'partial', 'no-match']}\n",
    "\n",
    "for d in good_samples:\n",
    "#     spider_ex = processed_spider_dev[d['ex_id']]\n",
    "#     text_match = _check_text_match(spider_ex, d)\n",
    "    text_match = d['category']['text_match']\n",
    "    samples_by_text_match[text_match].append(d)\n",
    "\n",
    "[(m, len(samples)) for m, samples in samples_by_text_match.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text match: exact (551)\n",
      "[Example] How many singers do we have?; structed knowledge: | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\n",
      "[Example] select count(*) from *singer*\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.8927\t1.0000\n",
      "r_text_score\t0.4673\t0.5245\n",
      "r_struct_score\t0.8768\t0.9800\n",
      "r_node_score\t0.7754\t0.8621\n",
      "r_struct_no_node_score\t0.3959\t0.4410\n",
      "r_node_corrupt_all_score\t0.8129\t0.9129\n",
      "\n",
      "Text match: partial (98)\n",
      "[Example] Show the name and theme for all concerts and the number of singers in each concert.; structed knowledge: | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\n",
      "[Example] select t2.concert_name, t2.theme, count(*) from *singer_in_concert*\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.8966\t1.0000\n",
      "r_text_score\t0.0700\t0.0408\n",
      "r_struct_score\t0.8741\t0.9592\n",
      "r_node_score\t0.8173\t0.8980\n",
      "r_struct_no_node_score\t0.1588\t0.2041\n",
      "r_node_corrupt_all_score\t0.8866\t0.9592\n",
      "\n",
      "Text match: no-match (172)\n",
      "[Example] What are the locations and names of all stations with capacity between 5000 and 10000?; structed knowledge: | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\n",
      "[Example] select location, name from *stadium*\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.8708\t1.0000\n",
      "r_text_score\t0.1106\t0.0756\n",
      "r_struct_score\t0.8298\t0.9360\n",
      "r_node_score\t0.7039\t0.8081\n",
      "r_struct_no_node_score\t0.2030\t0.2209\n",
      "r_node_corrupt_all_score\t0.7015\t0.7965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in ['exact', 'partial', 'no-match']:\n",
    "    samples = samples_by_text_match[m]\n",
    "    print(f'Text match: {m} ({len(samples)})')\n",
    "    print('[Example]', samples[0]['enc_sentence'])\n",
    "    print(f\"[Example] {samples[0]['dec_prompt']} *{samples[0]['expect']}*\")\n",
    "    print(f'Score\\tavg_gain\\tperc_recover')\n",
    "    \n",
    "    low_scores = [d['low_score'] for d in samples]\n",
    "    for score_label in [\n",
    "        'base_score',\n",
    "        'r_text_score',\n",
    "        'r_struct_score',\n",
    "        'r_node_score',\n",
    "        'r_struct_no_node_score',\n",
    "        'r_node_corrupt_all_score'\n",
    "    ]:\n",
    "        high_scores = [d[score_label] for d in samples]\n",
    "        avg_gain = np.mean([h - l for h, l in zip(high_scores, low_scores)])\n",
    "        perc_recover = np.mean([h - l > 0.5 for h, l in zip(high_scores, low_scores)])\n",
    "        print(f'{score_label}\\t{avg_gain:.4f}\\t{perc_recover:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = samples_by_text_match['no-match'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What are the names of all European countries with at least 3 manufacturers? The database is as following: | car_1 | continents : contid , continent ( europe ) | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year => select t1.countryname from countries as t1 join continents as t2 on t1.continent = t2.contid join car_makers as t3 on t1.countryid = t3.country where t2.continent = 'europe' group by t1.countryname having count(*) >= 3;\""
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in} The database is as following: {struct_in}\"\n",
    "\n",
    "f\"{enc_sentence} => {ex['seq_out']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['ex_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-2.2: dirty text struct restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 1084)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect_type = 'column'\n",
    "\n",
    "res_json_path = f'/home/yshao/Projects/rome/results/exp2.2_dirty_text_struct_restore/exp=2.2_dev_{expect_type}.jsonl'\n",
    "\n",
    "with open(res_json_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "\n",
    "good_trace_results = []\n",
    "for ex in all_samples:\n",
    "    for r in ex['trace_results']:\n",
    "        if r['is_good_sample']:\n",
    "            r['ex_id'] = ex['ex_id']\n",
    "            good_trace_results.append(r)\n",
    "    \n",
    "len(all_samples), len(good_trace_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['enc_sentence', 'seq_out', 'dec_prompt', 'expect', 'expect_type', 'db_id', 'expect_input_ranges', 'expect_table', 'category', 'low_score', 'high_score', 'input_ids', 'input_tokens', 'dec_input_ids', 'dec_input_tokens', 'subject_range', 'subject_range_individual_indices', 'answer', 'window', 'correct_prediction', 'kind', 'sever_kind', 'scores', 'is_good_sample', 'part', 'ex_id'])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_trace_results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generating all plots\n",
    "# fig_save_dir = f'/home/yshao/Projects/rome/results/figs/exp2.2_dirty_text_struct_restore/exp=2.2_dev_column'\n",
    "\n",
    "# for i, r in enumerate(tqdm(good_trace_results)):\n",
    "#     result = dict(r)\n",
    "\n",
    "#     enc_s, dec_s = result['scores']\n",
    "    \n",
    "#     # enc_s = np.array(enc_s)\n",
    "#     enc_layers_vec = np.array(enc_s[0])\n",
    "#     assert enc_layers_vec.shape == (mt_uskg.num_enc_layers,)\n",
    "#     enc_token_range, = result['enc_token_range']\n",
    "    \n",
    "#     enc_s = np.zeros((len(result['input_tokens']), mt_uskg.num_enc_layers))\n",
    "#     enc_s[enc_token_range] = enc_layers_vec\n",
    "    \n",
    "#     dec_s = np.zeros((len(result['dec_input_tokens']), mt_uskg.num_dec_layers))\n",
    "#     result['scores'] = [enc_s, dec_s]\n",
    "\n",
    "#     ex_id = r['ex_id']\n",
    "#     ctu.plot_trace_heatmap_t5(result, savepdf=os.path.join(fig_save_dir, f'{i}-ex_id={ex_id}.pdf'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check avg per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_layer = [[] for _ in range(mt_uskg.num_enc_layers)]\n",
    "\n",
    "for r in good_trace_results:\n",
    "    enc_s, dec_s = r['scores']\n",
    "    for l, s in enumerate(enc_s[0]):\n",
    "        scores_per_layer[l].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04998245431660366,\n",
       " 0.05325091241746874,\n",
       " 0.05578506940298435,\n",
       " 0.056500998754504546,\n",
       " 0.05632673384162137,\n",
       " 0.060122467615442834,\n",
       " 0.0743413356685932,\n",
       " 0.07793125936657581,\n",
       " 0.1187241473144563,\n",
       " 0.1544875932789873,\n",
       " 0.18513770163235047,\n",
       " 0.21917309280363884,\n",
       " 0.28553838302240797,\n",
       " 0.3615626793599896,\n",
       " 0.3617946804873726,\n",
       " 0.3946494784126291,\n",
       " 0.47267543055045763,\n",
       " 0.44948675620885103,\n",
       " 0.4737027594501244,\n",
       " 0.5257112130350594,\n",
       " 0.5352272429749859,\n",
       " 0.6249380804599364,\n",
       " 0.670510063510466,\n",
       " 0.7290238733215495]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_avg_by_layer = [np.mean(scores) for scores in scores_per_layer]\n",
    "all_avg_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer\tAvg. Score\n",
      "0    \t0.0500\n",
      "1    \t0.0533\n",
      "2    \t0.0558\n",
      "3    \t0.0565\n",
      "4    \t0.0563\n",
      "5    \t0.0601\n",
      "6    \t0.0743\n",
      "7    \t0.0779\n",
      "8    \t0.1187\n",
      "9    \t0.1545\n",
      "10   \t0.1851\n",
      "11   \t0.2192\n",
      "12   \t0.2855\n",
      "13   \t0.3616\n",
      "14   \t0.3618\n",
      "15   \t0.3946\n",
      "16   \t0.4727\n",
      "17   \t0.4495\n",
      "18   \t0.4737\n",
      "19   \t0.5257\n",
      "20   \t0.5352\n",
      "21   \t0.6249\n",
      "22   \t0.6705\n",
      "23   \t0.7290\n"
     ]
    }
   ],
   "source": [
    "print('Layer\\tAvg. Score')\n",
    "for l, avg in enumerate(all_avg_by_layer):\n",
    "    print(f'{l:<5d}\\t{avg:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(all_avg_by_layer, label='All')\n",
    "ax.set_xlabel('Layer to restore')\n",
    "ax.set_ylabel('Avg. P(correct answer)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  by hardness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('easy', 191), ('medium', 516), ('hard', 216), ('extra', 161)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {h : [[score], ...]}\n",
    "scores_by_hardness = {hardness : [] for hardness in ['easy', 'medium', 'hard', 'extra']}   \n",
    "\n",
    "for r in good_trace_results:\n",
    "    enc_s, dec_s = r['scores']\n",
    "    scores, = enc_s\n",
    "#     hardness = spider_id2hardness[r['ex_id']]\n",
    "    hardness = r['category']['sql_hardness']\n",
    "    scores_by_hardness[hardness].append(scores)\n",
    "\n",
    "[(h, len(scores)) for h, scores in scores_by_hardness.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy\n",
      "['0.05', '0.05', '0.06', '0.06', '0.06', '0.06', '0.08', '0.09', '0.14', '0.20', '0.24', '0.29', '0.35', '0.41', '0.40', '0.43', '0.49', '0.48', '0.50', '0.54', '0.55', '0.65', '0.71', '0.78']\n",
      "\n",
      "medium\n",
      "['0.05', '0.05', '0.06', '0.06', '0.06', '0.06', '0.07', '0.07', '0.11', '0.14', '0.17', '0.20', '0.27', '0.35', '0.35', '0.38', '0.45', '0.42', '0.45', '0.50', '0.51', '0.60', '0.65', '0.71']\n",
      "\n",
      "hard\n",
      "['0.04', '0.05', '0.05', '0.05', '0.05', '0.05', '0.06', '0.07', '0.11', '0.14', '0.17', '0.21', '0.27', '0.35', '0.37', '0.41', '0.49', '0.47', '0.48', '0.54', '0.54', '0.60', '0.65', '0.71']\n",
      "\n",
      "extra\n",
      "['0.06', '0.06', '0.07', '0.07', '0.07', '0.07', '0.09', '0.10', '0.13', '0.16', '0.19', '0.22', '0.28', '0.35', '0.35', '0.38', '0.50', '0.48', '0.52', '0.58', '0.60', '0.70', '0.72', '0.74']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for h, sample_scores in scores_by_hardness.items():\n",
    "    print(h)\n",
    "    scores_by_layer = zip(*sample_scores)\n",
    "    avg_by_layer = [np.mean(scores) for scores in scores_by_layer]\n",
    "    print([f'{avg:.2f}' for avg in avg_by_layer])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(all_avg_by_layer, label='All')\n",
    "for h, sample_scores in scores_by_hardness.items():\n",
    "    h_scores_by_layer = zip(*sample_scores)\n",
    "    h_avg_by_layer = [np.mean(scores) for scores in h_scores_by_layer]\n",
    "    ax.plot(h_avg_by_layer, label=h)\n",
    "ax.set_xlabel('Layer to restore')\n",
    "ax.set_ylabel('Avg. P(correct answer)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  by node role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('select', 642),\n",
       " ('order by', 73),\n",
       " ('where', 267),\n",
       " ('group by', 26),\n",
       " ('join', 62),\n",
       " ('having', 4),\n",
       " ('from', 10)]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {h : [[score], ...]}\n",
    "scores_by_node_role = defaultdict(list)\n",
    "\n",
    "for r in good_trace_results:\n",
    "    enc_s, dec_s = r['scores']\n",
    "    scores, = enc_s\n",
    "#     role = _detect_node_role(' '.join(r['dec_input_tokens']))\n",
    "    role = r['category']['node_role']\n",
    "    scores_by_node_role[role].append(scores)\n",
    "\n",
    "[(r, len(scores)) for r, scores in scores_by_node_role.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select\n",
      "['0.03', '0.03', '0.03', '0.03', '0.03', '0.03', '0.04', '0.04', '0.04', '0.06', '0.07', '0.09', '0.13', '0.18', '0.18', '0.20', '0.28', '0.26', '0.29', '0.35', '0.37', '0.54', '0.62', '0.72']\n",
      "\n",
      "order by\n",
      "['0.05', '0.05', '0.05', '0.05', '0.05', '0.05', '0.06', '0.06', '0.07', '0.09', '0.13', '0.17', '0.29', '0.50', '0.49', '0.59', '0.71', '0.63', '0.65', '0.74', '0.75', '0.76', '0.75', '0.77']\n",
      "\n",
      "where\n",
      "['0.08', '0.09', '0.10', '0.10', '0.10', '0.12', '0.16', '0.17', '0.30', '0.40', '0.47', '0.54', '0.65', '0.75', '0.76', '0.81', '0.89', '0.87', '0.87', '0.91', '0.91', '0.88', '0.88', '0.88']\n",
      "\n",
      "group by\n",
      "['0.15', '0.16', '0.18', '0.18', '0.18', '0.18', '0.20', '0.21', '0.30', '0.40', '0.46', '0.50', '0.65', '0.74', '0.71', '0.77', '0.87', '0.81', '0.85', '0.88', '0.86', '0.80', '0.85', '0.88']\n",
      "\n",
      "join\n",
      "['0.06', '0.06', '0.06', '0.06', '0.06', '0.06', '0.06', '0.06', '0.06', '0.07', '0.07', '0.09', '0.15', '0.22', '0.23', '0.25', '0.33', '0.30', '0.31', '0.33', '0.30', '0.27', '0.26', '0.24']\n",
      "\n",
      "from\n",
      "['0.18', '0.18', '0.18', '0.18', '0.18', '0.18', '0.18', '0.18', '0.19', '0.20', '0.22', '0.22', '0.26', '0.29', '0.28', '0.30', '0.29', '0.28', '0.28', '0.29', '0.27', '0.26', '0.26', '0.24']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r, sample_scores in scores_by_node_role.items():\n",
    "    if len(sample_scores) < 10:\n",
    "        continue\n",
    "    print(r)\n",
    "    scores_by_layer = zip(*sample_scores)\n",
    "    avg_by_layer = [np.mean(scores) for scores in scores_by_layer]\n",
    "    print([f'{avg:.2f}' for avg in avg_by_layer])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(all_avg_by_layer, label='All')\n",
    "for r, sample_scores in scores_by_node_role.items():\n",
    "    if len(sample_scores) < 50:\n",
    "        continue\n",
    "    r_scores_by_layer = zip(*sample_scores)\n",
    "    r_avg_by_layer = [np.mean(scores) for scores in r_scores_by_layer]\n",
    "    ax.plot(r_avg_by_layer, label=r)\n",
    "ax.set_xlabel('Layer to restore')\n",
    "ax.set_ylabel('Avg. P(correct answer)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by text matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exact', 613), ('partial', 170), ('no-match', 301)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_by_text_matching = {k: [] for k in ['exact', 'partial', 'no-match']}\n",
    "\n",
    "for r in good_trace_results:\n",
    "#     spider_ex = processed_spider_dev[r['ex_id']]\n",
    "#     col = r['target_node']\n",
    "#     tab = r['target_node_table']\n",
    "#     m = ctu.check_text_match(spider_ex, col, tab)\n",
    "    m = r['category']['text_match']\n",
    "\n",
    "    enc_s, dec_s = r['scores']\n",
    "    scores, = enc_s\n",
    "    scores_by_text_matching[m].append(scores)\n",
    "\n",
    "[(m, len(samples)) for m, samples in scores_by_text_matching.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact\n",
      "['0.05', '0.06', '0.06', '0.06', '0.06', '0.06', '0.08', '0.08', '0.13', '0.17', '0.20', '0.24', '0.29', '0.35', '0.35', '0.38', '0.46', '0.43', '0.45', '0.51', '0.53', '0.63', '0.68', '0.75']\n",
      "\n",
      "partial\n",
      "['0.05', '0.05', '0.05', '0.05', '0.05', '0.05', '0.05', '0.06', '0.07', '0.09', '0.12', '0.14', '0.22', '0.32', '0.32', '0.36', '0.44', '0.44', '0.47', '0.52', '0.52', '0.62', '0.67', '0.73']\n",
      "\n",
      "no-match\n",
      "['0.05', '0.05', '0.05', '0.05', '0.05', '0.06', '0.08', '0.08', '0.13', '0.15', '0.19', '0.23', '0.31', '0.41', '0.41', '0.44', '0.52', '0.50', '0.52', '0.57', '0.56', '0.61', '0.65', '0.70']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r, sample_scores in scores_by_text_matching.items():\n",
    "    if len(sample_scores) < 10:\n",
    "        continue\n",
    "    print(r)\n",
    "    scores_by_layer = zip(*sample_scores)\n",
    "    avg_by_layer = [np.mean(scores) for scores in scores_by_layer]\n",
    "    print([f'{avg:.2f}' for avg in avg_by_layer])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(all_avg_by_layer, label='All')\n",
    "for m, sample_scores in scores_by_text_matching.items():\n",
    "    if len(sample_scores) < 10:\n",
    "        continue\n",
    "    r_scores_by_layer = zip(*sample_scores)\n",
    "    r_avg_by_layer = [np.mean(scores) for scores in r_scores_by_layer]\n",
    "    ax.plot(r_avg_by_layer, label=m)\n",
    "ax.set_xlabel('Layer to restore')\n",
    "ax.set_ylabel('Avg. P(correct answer)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-3.1: dirty struct context restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_path = '/home/yshao/Projects/rome/results/exp3.1_dirty_struct_context_restore/exp=3.1_dev_table-tmp.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex_id': 0,\n",
       " 'trace_results': [{'enc_sentence': 'How many singers do we have?; structed knowledge: | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id',\n",
       "   'seq_out': 'select count(*) from singer',\n",
       "   'dec_prompt': 'select count(*) from',\n",
       "   'expect': 'singer',\n",
       "   'expect_type': 'table',\n",
       "   'db_id': 'concert_singer',\n",
       "   'expect_input_ranges': [[47, 48]],\n",
       "   'expect_table': 'singer',\n",
       "   'answer': 'singer',\n",
       "   'base_score': 0.9999998807907104,\n",
       "   'answers_t': [7634],\n",
       "   'correct_prediction': True,\n",
       "   'category': {'sql_hardness': 'easy',\n",
       "    'node_role': 'from',\n",
       "    'text_match': 'exact'},\n",
       "   'self_ranges': [[46, 50]],\n",
       "   'struct_context_ranges': [[15, 46], [50, 125]],\n",
       "   'trace_scores': {'single_layer_corrupt': {},\n",
       "    'low_layers_restore': {},\n",
       "    'high_layers_restore': {},\n",
       "    'single_layer_restore': {},\n",
       "    'temp1': {}},\n",
       "   'low_score': 1.0,\n",
       "   'is_good_sample': False}]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, (0, 0), 0, 17, 17)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    print(d['enc_sentence'])\n",
    "    print(d['dec_prompt'], '-->', d['expect'])\n",
    "    print(json.dumps(d['trace_scores'], indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in bad_samples:\n",
    "    print(d['enc_sentence'])\n",
    "    print(d['dec_prompt'], '-->', d['expect'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-3.2: dirty struct context compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_path = '/home/yshao/Projects/rome/results/exp3.2_dirty_struct_context_compare/exp=3.2_dev_table.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ex_id': 0,\n",
       " 'trace_results': [{'enc_sentence': 'How many singers do we have?; structed knowledge: | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id',\n",
       "   'seq_out': 'select count(*) from singer',\n",
       "   'dec_prompt': 'select count(*) from',\n",
       "   'expect': 'singer',\n",
       "   'expect_type': 'table',\n",
       "   'db_id': 'concert_singer',\n",
       "   'expect_input_ranges': [[47, 48]],\n",
       "   'expect_table': 'singer',\n",
       "   'answer': 'singer',\n",
       "   'base_score': 0.9999998807907104,\n",
       "   'answers_t': [7634],\n",
       "   'correct_prediction': True,\n",
       "   'category': {'sql_hardness': 'easy',\n",
       "    'node_role': 'from',\n",
       "    'text_match': 'exact'},\n",
       "   'self_ranges': [[46, 50]],\n",
       "   'struct_context_ranges': [[15, 46], [50, 125]],\n",
       "   'scc_score': 1.0,\n",
       "   'scc_answers_t': [7634],\n",
       "   'scc_answer': 'singer',\n",
       "   'scc_correct_prediction': True,\n",
       "   'compare': {'correctness_compare': 'both'}}]}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'both': 1424, 'clean+': 123, 'scc+': 105, 'none': 31})"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correctness_compare_counter = Counter([d['compare']['correctness_compare'] for ex in all_samples for d in ex['trace_results']])\n",
    "correctness_compare_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        _comp = d['compare']['correctness_compare']\n",
    "#         if _comp in {'scc+', 'clean+'}:\n",
    "        if (_comp == 'scc+') and (d['category']['node_role'] != 'join'):\n",
    "            print(f'#{i}:', d['enc_sentence'])\n",
    "            print('SQL:', d['dec_prompt'], '-->', d['expect'])\n",
    "            print('Clean:', d['answer'])\n",
    "            print('SCC:', d['scc_answer'])\n",
    "            print('COMP:', d['compare']['correctness_compare'])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'join': 83, 'from': 40})"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_clean_node_role_cnt = Counter()\n",
    "# TODO: make a larger counter: {comp -> category_k -> category_v -> count}\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        if d['compare']['correctness_compare'] == 'clean+':\n",
    "            _role = d['category']['node_role']\n",
    "            _clean_node_role_cnt[_role] += 1\n",
    "\n",
    "_clean_node_role_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'from': 73, 'join': 32})"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_scc_node_role_cnt = Counter()\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        if d['compare']['correctness_compare'] == 'scc+':\n",
    "            _role = d['category']['node_role']\n",
    "            _scc_node_role_cnt[_role] += 1\n",
    "\n",
    "_scc_node_role_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-4: inspect attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_type = 'column'\n",
    "\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp4_inspect_attention/exp=4_dev_{exp_type}_both.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['enc_sentence', 'seq_out', 'dec_prompt', 'expect', 'expect_type', 'db_id', 'expect_input_ranges', 'expect_table', 'category', 'answer', 'probs', 'base_score', 'answers_t', 'correct_prediction', 'attentions'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples[1]['trace_results'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"What are the students' first names who have both cats and dogs as pets?; structed knowledge: | pets_1 | student : stuid , lname , fname , age , sex , major , advisor , city_code | has_pet : stuid , petid | pets : petid , pettype ( dog , cat ) , pet_age , weight\",\n",
       " 'select t1.',\n",
       " 'fname')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = all_samples[6]['trace_results'][0]\n",
    "d['enc_sentence'], d['dec_prompt'], d['expect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['enc_sentence', 'seq_out', 'dec_prompt', 'expect', 'expect_type', 'db_id', 'expect_input_ranges', 'expect_table', 'category', 'answer', 'probs', 'base_score', 'answers_t', 'correct_prediction', 'attentions']),\n",
       " dict_keys(['enc', 'cross', 'dec']),\n",
       " dict_keys(['attn', 'head_tokens', 'cand_tokens']))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys(), d['attentions'].keys(), d['attentions']['enc'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_single_plot_2(ax, val_mat, x_labels=None, y_labels=None, title=None):\n",
    "    \"\"\"\n",
    "    X: # heads\n",
    "    Y: cand tokens\n",
    "    Title: head token (together with full expect)\n",
    "    \"\"\"\n",
    "    if isinstance(val_mat, list):\n",
    "        val_mat = numpy.array(val_mat)\n",
    "    h = ax.pcolormesh(\n",
    "        val_mat,\n",
    "        cmap=\"Reds\",\n",
    "        vmax=1.0,\n",
    "        vmin=0.0,\n",
    "    )\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_yticks([0.5 + i for i in range(val_mat.shape[0])])\n",
    "    ax.set_xticks([0.5 + i for i in range(val_mat.shape[1])])\n",
    "    if x_labels is not None:\n",
    "        ax.set_xticklabels(x_labels, fontsize=8)\n",
    "    if y_labels is not None:\n",
    "        ax.set_yticklabels(y_labels, fontsize=8)\n",
    "\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_xlabel(f\"# Head\")\n",
    "    ax.set_ylabel(f\"Attention candidate tokens\")\n",
    "    \n",
    "    # cb = plt.colorbar(h)\n",
    "    # divider = make_axes_locatable(ax)\n",
    "    # cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    # cb = fig.colorbar(h, cax=cax)\n",
    "    cb = plt.colorbar(h, ax=ax)\n",
    "\n",
    "#     if xlabel is not None:\n",
    "#         ax.set_xlabel(xlabel)\n",
    "#     elif answer is not None:\n",
    "#         # The following should be cb.ax.set_xlabel, but this is broken in matplotlib 3.5.1.\n",
    "#         cb.ax.set_title(f\"p({str(answer).strip()})\", y=-0.16, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uskg_attention(d, att_part, inspect_layers=None, savepdf=None):\n",
    "    \"\"\"\n",
    "    Assume 16 heads, 24 layers (T5 large config)\n",
    "    \n",
    "    att_part: enc, cross, dec \n",
    "    \"\"\"\n",
    "    \n",
    "    ## encoder self attention \n",
    "    if inspect_layers is None:\n",
    "        inspect_layers = [0, 6, 12, 18, 23]\n",
    "    elif inspect_layers == 'all':\n",
    "        inspect_layers = [i for i in range(24)]\n",
    "    att_dict = d['attentions'][att_part]\n",
    "    \n",
    "    cand_len = len(att_dict['cand_tokens'])\n",
    "    head_len = len(att_dict['head_tokens'])\n",
    "    prompt_tokens = d['attentions']['dec']['cand_tokens']\n",
    "    prompt_len = len(prompt_tokens)\n",
    "\n",
    "    fig_w = len(inspect_layers) * 4 + 2\n",
    "    fig_h = (0.11*cand_len + 1) * head_len + 2\n",
    "    fig, ax_list = plt.subplots(\n",
    "        nrows=head_len,\n",
    "        ncols=len(inspect_layers),\n",
    "        squeeze=False,\n",
    "        figsize=(fig_w, fig_h))\n",
    "\n",
    "    att_mat = ctu.nested_list_processing(att_dict['attn'], func=float)\n",
    "    att_mat = np.array(att_mat)\n",
    "    \n",
    "    for expect_i in range(len(att_dict['head_tokens'])):\n",
    "        for l_id, layer in enumerate(inspect_layers):\n",
    "            val_mat = att_mat[layer, :, expect_i, :]  # layer, all heads, expect tok i -> all toks \n",
    "            val_mat = val_mat.transpose()    # (cand_toks, n_heads)\n",
    "            x_labels = range(val_mat.shape[1])\n",
    "            y_labels = att_dict['cand_tokens']\n",
    "            if att_part == 'enc':\n",
    "                # enc: correct tokens\n",
    "                title_toks = att_dict['head_tokens'][:expect_i] + [f\"*{att_dict['head_tokens'][expect_i]}*\"]\n",
    "            else:\n",
    "                # cross / dec: use gold tokens from dec_prompt for previous steps and predicted token at this step\n",
    "                # (dec_prompt ends with the first (head_len-1) tokens of the target node)\n",
    "                title_toks = prompt_tokens[prompt_len - (head_len-1) : prompt_len - (head_len-1) + expect_i] + [f\"*{att_dict['head_tokens'][expect_i]}*\"]\n",
    "            \n",
    "            title = f\"L{layer}  Head token: {' '.join(title_toks)}\\n\"\n",
    "            \n",
    "            ax = ax_list[expect_i, l_id]\n",
    "            _draw_single_plot_2(ax,\n",
    "                                val_mat=val_mat, \n",
    "                                x_labels=x_labels, \n",
    "                                y_labels=y_labels,\n",
    "                                title=title)\n",
    "            \n",
    "    fig.tight_layout()\n",
    "    if savepdf:\n",
    "        plt.savefig(savepdf, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savepdf_tmpl = '/home/yshao/Projects/rome/results/figs/exp4_inspect_attention/tmp-6-{}.pdf'\n",
    "# for att_part in ['enc', 'cross', 'dec']:\n",
    "#     plot_uskg_attention(d, att_part, savepdf=savepdf_tmpl.format(att_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f3a2445a1046e5b8f110bacceee316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Run for all! \n",
    "\n",
    "fig_dir  = f'/home/yshao/Projects/rome/results/figs/exp4_inspect_attention/dev_{exp_type}'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "global_ex_id = 0\n",
    "for ex_id in tqdm(range(len(all_samples))):\n",
    "    for a_ex_id in range(len(all_samples[ex_id]['trace_results'])):\n",
    "        d = all_samples[ex_id]['trace_results'][a_ex_id]\n",
    "        for att_part in ['enc', 'cross', 'dec']:\n",
    "            savepdf_path = os.path.join(fig_dir, f'{global_ex_id}-ex={ex_id}.{a_ex_id}-{att_part}.pdf')\n",
    "            plot_uskg_attention(d, att_part, savepdf=savepdf_path)\n",
    "        # print(f'{global_ex_id}-ex={ex_id}.{a_ex_id}')\n",
    "        global_ex_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1), (5, 2), (6, 0), (6, 1), (6, 2), (6, 3), (9, 0), (9, 1), (9, 2), (10, 0), (10, 1), (11, 0), (12, 2), (13, 1), (13, 2), (13, 3), (13, 4), (15, 0), (17, 1), (20, 0), (20, 1), (21, 0), (21, 1), (21, 2), (22, 0), (22, 1), (22, 2), (22, 3), (23, 0), (24, 0), (24, 1), (24, 2), (24, 3), (25, 0), (25, 1), (39, 0), (39, 1), (45, 0), (45, 1), (47, 2), (55, 2), (55, 3), (55, 4), (58, 0), (58, 1), (61, 0), (63, 0), (63, 1), (64, 2), (67, 1), (71, 1), (72, 0), (72, 1), (73, 1), (74, 0), (75, 0), (75, 2), (76, 0), (77, 0), (77, 1), (77, 2), (79, 0), (79, 1), (80, 0), (80, 1), (81, 1), (81, 2), (82, 1), (85, 0), (85, 1), (89, 0), (89, 1), (90, 0), (90, 1), (91, 0), (94, 0), (96, 0), (96, 1)]\n"
     ]
    }
   ],
   "source": [
    "_sel_samples = []\n",
    "\n",
    "for ex_id in (range(len(all_samples))):\n",
    "    for a_ex_id in range(len(all_samples[ex_id]['trace_results'])):\n",
    "        d = all_samples[ex_id]['trace_results'][a_ex_id]\n",
    "        if d['category']['text_match'] == 'no-match':\n",
    "            _sel_samples.append((ex_id, a_ex_id))\n",
    "\n",
    "print(_sel_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run for separate samples \n",
    "\n",
    "fig_dir  = f'/home/yshao/Projects/rome/results/figs/exp4_inspect_attention/detail_subset'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "ex_id = 17\n",
    "a_ex_id = 1\n",
    "d = all_samples[ex_id]['trace_results'][a_ex_id]\n",
    "for att_part in ['enc']:\n",
    "    savepdf_path = os.path.join(fig_dir, f'{exp_type}-ex={ex_id}.{a_ex_id}-{att_part}.pdf')\n",
    "    plot_uskg_attention(d, att_part, inspect_layers='all', savepdf=savepdf_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp-4.0.1: Prefix attention samples\n",
    "- Check the samples with high attention on a given layer & head & prefix token\n",
    "- Haven't found any insights..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 680)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_a_ex = 0\n",
    "total_tok_sample = 0\n",
    "\n",
    "for ex_id in (range(len(all_samples))):\n",
    "    for a_ex_id in range(len(all_samples[ex_id]['trace_results'])):\n",
    "        d = all_samples[ex_id]['trace_results'][a_ex_id]\n",
    "        _expect_len = len([i for s, e in d['expect_input_ranges'] for i in range(s, e)])\n",
    "        \n",
    "        total_a_ex += 1\n",
    "        total_tok_sample += _expect_len\n",
    "\n",
    "total_a_ex, total_tok_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"What are the students' first names who have both cats and dogs as pets?; structed knowledge: | pets_1 | student : stuid , lname , fname , age , sex , major , advisor , city_code | has_pet : stuid , petid | pets : petid , pettype ( dog , cat ) , pet_age , weight\",\n",
       " 'select t1.',\n",
       " 'fname')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = all_samples[6]['trace_results'][0]\n",
    "d['enc_sentence'], d['dec_prompt'], d['expect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[41, 44]], 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_expect_len = len([i for s, e in d['expect_input_ranges'] for i in range(s, e)])\n",
    "d['expect_input_ranges'], _expect_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e8f3eab2c3411087afc827f49d7745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enc_prefix_att_samples = defaultdict(list)  # [(l_id, head_id, prefix_id) -> [dict(ex_id, a_ex_id, tok_id, ...)]]\n",
    "\n",
    "for ex_id in tqdm(range(len(all_samples))):\n",
    "    for a_ex_id in range(len(all_samples[ex_id]['trace_results'])):\n",
    "# for ex_id in [6]:\n",
    "#     for a_ex_id in [0]:\n",
    "        d = all_samples[ex_id]['trace_results'][a_ex_id]\n",
    "        \n",
    "        att_mat = ctu.nested_list_processing(d['attentions']['enc']['attn'], func=float)\n",
    "        # att_mat: (n_layers, n_heads, n_expect_toks, n_input_toks)\n",
    "        att_mat = np.array(att_mat)\n",
    "        n_layers, n_heads, n_expect_toks, n_input_toks = att_mat.shape\n",
    "        \n",
    "        # (n_layers, n_heads, expect_toks)\n",
    "#         input_att_positions = np.argmax(att_mat, axis=-1)\n",
    "#         for l_id in range(n_layers):\n",
    "#             for h_id in range(n_heads):\n",
    "#                 for tok_id in range(n_expect_toks):\n",
    "#                     _max_att_p = input_att_positions[l_id, h_id, tok_id]\n",
    "\n",
    "        input_att_positions = np.where(att_mat > 0.5)\n",
    "        for l_id, h_id, tok_id, _max_att_p in zip(*input_att_positions):\n",
    "            if _max_att_p < 10:\n",
    "                # is prefix \n",
    "                enc_prefix_att_samples[(l_id, h_id, _max_att_p)].append({\n",
    "                    'ex_id': ex_id,\n",
    "                    'a_ex_id': a_ex_id,\n",
    "                    'expect_tok_id': tok_id,\n",
    "                    'enc_sentence': d['enc_sentence'],\n",
    "                    'dec_prompt': d['dec_prompt'],\n",
    "                    'expect': d['expect'],\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1873"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_prefix_att_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_profile_counter = Counter([len(v) for k, v in enc_prefix_att_samples.items()])\n",
    "sorted(len_profile_counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 5, 4) 192\n",
      "(18, 6, 2) 145\n",
      "(20, 0, 7) 136\n",
      "(20, 4, 8) 198\n",
      "(23, 7, 0) 125\n",
      "(8, 5, 0) 140\n",
      "(15, 5, 4) 235\n",
      "(17, 1, 9) 220\n",
      "(19, 10, 1) 177\n",
      "(21, 12, 5) 131\n",
      "(3, 4, 8) 126\n",
      "(11, 8, 4) 124\n",
      "(21, 0, 1) 129\n"
     ]
    }
   ],
   "source": [
    "for k, v in enc_prefix_att_samples.items():\n",
    "    if len(v) > 120:\n",
    "        print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc_prefix_att_samples[(23, 7, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-5.0: dirty attention vector effect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect_type = 'table_alias'\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp5_0_dirty_attention_vector_effect/exp=5_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2039, (164, 164), 339, 1536, 1875, 1700)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "#         # TEMP adjustment for column results \n",
    "#         d['low_score'] = d['trace_scores']['high_layers_corrupt'].get(\"0\", 0.0)  # \"0\" is key (for layer 0), 0.0 is default \n",
    "#         if d['base_score'] - d['low_score'] < 0.5:\n",
    "#             d['is_good_sample'] = False\n",
    "#         # END_TEMP\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5, (i, d)\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples), \\\n",
    "n_good_samples + n_too_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s for s in bad_samples if s['correct_prediction']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg = {k: {str(l): 0 for l in range(24)} for k in good_samples[0]['trace_scores'].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    for k, layer_d in d['trace_scores'].items():\n",
    "        for l, s in layer_d.items():\n",
    "            trace_scores_avg[k][l] += s\n",
    "\n",
    "for k, layer_d in trace_scores_avg.items():\n",
    "    for l, s in layer_d.items():\n",
    "        layer_d[l] = s / len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg by aspects (category)\n",
    "- Still kind of linear, as in exp-2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_hardness': 'hard', 'node_role': 'where', 'text_match': 'partial'}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: (trace_key, aspect, asp_val, layer) -> [scores]\n",
    "trace_scores_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n",
    "trace_scores_avg_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "trace_scores_cnt_by_aspect = defaultdict(lambda: defaultdict(int))  # no trace key & layer key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    for trace_k, trace_layer_d in d['trace_scores'].items():\n",
    "        for aspect, asp_val in d['category'].items():\n",
    "            for l, s in trace_layer_d.items():\n",
    "                trace_scores_by_aspect[trace_k][aspect][asp_val][l].append(s)\n",
    "\n",
    "for trace_k, d1 in trace_scores_by_aspect.items():\n",
    "    for asp_k, d2 in d1.items():\n",
    "        for asp_v, d3 in d2.items():\n",
    "            for l, s in d3.items():\n",
    "                trace_scores_avg_by_aspect[trace_k][asp_k][asp_v][l] = np.mean(s)\n",
    "                trace_scores_cnt_by_aspect[asp_k][asp_v] = len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'sql_hardness': defaultdict(int,\n",
       "                         {'medium': 400,\n",
       "                          'hard': 155,\n",
       "                          'easy': 142,\n",
       "                          'extra': 170}),\n",
       "             'node_role': defaultdict(int,\n",
       "                         {'where': 268,\n",
       "                          'select': 412,\n",
       "                          'order by': 66,\n",
       "                          'join': 92,\n",
       "                          'group by': 25,\n",
       "                          'having': 4}),\n",
       "             'text_match': defaultdict(int,\n",
       "                         {'no-match': 360, 'partial': 148, 'exact': 359})})"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_cnt_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg_by_aspect['high_layers_corrupt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-5.2: attention section removal effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect_type = 'table'\n",
    "\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/exp=5.2.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1683, (1207, 1207), 136, 340, 476, 'good / correct = 1207 / 1547')"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples),\\\n",
    "f'good / correct = {n_good_samples} / {n_good_samples + n_too_easy}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enc_sentence': 'Find the number of concerts happened in the stadium with the highest capacity .; structed knowledge: | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id',\n",
       " 'seq_out': 'select count(*) from concert where stadium_id = (select stadium_id from stadium order by capacity desc limit 1)',\n",
       " 'dec_prompt': 'select count(*) from',\n",
       " 'expect': 'concert',\n",
       " 'expect_type': 'table',\n",
       " 'db_id': 'concert_singer',\n",
       " 'expect_input_ranges': [[88, 89]],\n",
       " 'expect_table': 'concert',\n",
       " 'answer': 'stadium',\n",
       " 'base_score': 0.9941871166229248,\n",
       " 'answers_t': [14939],\n",
       " 'correct_prediction': False,\n",
       " 'category': {'sql_hardness': 'hard',\n",
       "  'node_role': 'from',\n",
       "  'text_match': 'exact'},\n",
       " 'self_ranges': [[87, 91]],\n",
       " 'struct_context_ranges': [[22, 87], [91, 132]],\n",
       " 'is_good_sample': False}"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in bad_samples if not s['correct_prediction']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg = {sect_k : defaultdict(int) for sect_k in good_samples[0]['trace_scores'].keys()}\n",
    "\n",
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for k, v in sect_d.items():\n",
    "            if k == 'window':\n",
    "                for l, s in v.items():\n",
    "                    trace_scores_avg[sect_k][f'{k}-{l}'] += s\n",
    "            else:\n",
    "                s = v\n",
    "                trace_scores_avg[sect_k][k] += s\n",
    "\n",
    "for sect_k, sect_d in trace_scores_avg.items():\n",
    "    for k, s in sect_d.items():\n",
    "        sect_d[k] = s / len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg by aspects (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP patch for node_len category \n",
    "for d in good_samples + bad_samples:\n",
    "    node_len = len(d['answers_t'])\n",
    "    assert len(mt_uskg.tokenizer.tokenize(d['expect'])) == node_len, (d['expect'], node_len)\n",
    "    d['category']['node_len'] = str(node_len) if node_len <= 3 else '4+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_hardness': 'medium',\n",
       " 'node_role': 'from',\n",
       " 'text_match': 'exact',\n",
       " 'node_len': '1'}"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: (sect_k, aspect, asp_val, layer) -> [scores]\n",
    "trace_scores_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n",
    "trace_scores_avg_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "trace_scores_cnt_by_aspect = defaultdict(lambda: defaultdict(int))  # no sect key & layer key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for aspect, asp_val in d['category'].items():\n",
    "            for k, v in sect_d.items():\n",
    "                if k == 'window':\n",
    "                    for l, s in v.items():\n",
    "                        if not (int(l) % 4 == 3): continue\n",
    "                        layer_k = f'{k}-{l}'\n",
    "                        trace_scores_by_aspect[sect_k][aspect][asp_val][layer_k].append(s)\n",
    "                else:\n",
    "                    layer_k = k\n",
    "                    s = v\n",
    "                    trace_scores_by_aspect[sect_k][aspect][asp_val][layer_k].append(s)\n",
    "                    \n",
    "for sect_k, d1 in trace_scores_by_aspect.items():\n",
    "    for asp_k, d2 in d1.items():\n",
    "        for asp_v, d3 in d2.items():\n",
    "            for layer_k, s in d3.items():\n",
    "                trace_scores_avg_by_aspect[sect_k][asp_k][asp_v][layer_k] = np.mean(s)\n",
    "                trace_scores_cnt_by_aspect[asp_k][asp_v] = len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sect_k, sect_d in trace_scores_avg_by_aspect.items():\n",
    "    sect_d['overall'] = dict()\n",
    "    for layer_k, s in trace_scores_avg[sect_k].items():\n",
    "        if layer_k.startswith('window'):\n",
    "            # only keep a subset of layers \n",
    "            _, l = layer_k.split('-')\n",
    "            if not (int(l) % 4 == 3): continue\n",
    "        sect_d['overall'][layer_k] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'sql_hardness': defaultdict(int,\n",
       "                         {'medium': 378,\n",
       "                          'hard': 148,\n",
       "                          'easy': 134,\n",
       "                          'extra': 160}),\n",
       "             'node_role': defaultdict(int,\n",
       "                         {'where': 248,\n",
       "                          'select': 393,\n",
       "                          'order by': 63,\n",
       "                          'join': 91,\n",
       "                          'group by': 21,\n",
       "                          'having': 4}),\n",
       "             'text_match': defaultdict(int,\n",
       "                         {'no-match': 345, 'partial': 142, 'exact': 333}),\n",
       "             'node_len': defaultdict(int,\n",
       "                         {'1': 329, '3': 238, '4+': 160, '2': 93})})"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_cnt_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_scores_avg_by_aspect['self']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_d = ctu.nested_json_processing(trace_scores_avg_by_aspect, func=lambda x: np.format_float_positional(x, precision=4, min_digits=4))\n",
    "# dump_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/summ-exp=5.2.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(dump_path, 'w') as f:\n",
    "    json.dump(dump_d, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (one-time temp patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect_type = 'table_alias'\n",
    "# orig_res_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/no_structcontext-exp=5.2.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "# add_res_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/exp=5.2.1+structcontext_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "# merge_res_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/exp=5.2.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(orig_res_path, 'r') as f:\n",
    "#     orig_all_samples = [json.loads(l) for l in f]\n",
    "# with open(add_res_path, 'r') as f:\n",
    "#     add_all_samples = [json.loads(l) for l in f]\n",
    "\n",
    "# f = open(merge_res_path, 'w')\n",
    "    \n",
    "# for i, (orig_ex, add_ex) in enumerate(zip(orig_all_samples, add_all_samples)):\n",
    "#     assert len(orig_ex['trace_results']) == len(add_ex['trace_results']), i\n",
    "#     # There is randomness in the order of expected node (from set()), thus sorting here \n",
    "#     orig_ex['trace_results'].sort(key=lambda d: len(d['dec_prompt']))\n",
    "#     add_ex['trace_results'].sort(key=lambda d: len(d['dec_prompt']))\n",
    "#     for j, (orig_d, add_d) in enumerate(zip(orig_ex['trace_results'], add_ex['trace_results'])):\n",
    "#         assert orig_d['is_good_sample'] == add_d['is_good_sample'], (i, j)\n",
    "#         if not orig_d['is_good_sample']:\n",
    "#             continue\n",
    "            \n",
    "#         # is good sample: add the new sections \n",
    "#         orig_d['trace_scores']['struct_context'] = add_d['trace_scores']['struct_context']\n",
    "#         orig_d['trace_scores']['text+struct_context'] = add_d['trace_scores']['text+struct_context']\n",
    "        \n",
    "#     f.write(json.dumps(orig_ex, indent=None) + '\\n')\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single samples observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prefix', 'text', 'struct', 'text+struct', 'all', 'self', 'struct_context', 'text+struct_context'])"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples[0]['trace_scores'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = 0\n",
    "\n",
    "d = good_samples[_id]\n",
    "\n",
    "check_info_d = defaultdict(dict)\n",
    "\n",
    "for sect_k, sect_d in d['trace_scores'].items():\n",
    "    for layer_k, s in sect_d.items():\n",
    "        if layer_k == 'window':\n",
    "            layer_k = 'window-19'\n",
    "            s = s['19']\n",
    "        if s < 0.5:\n",
    "            check_info_d[sect_k][layer_k] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text+struct\": {\n",
      "    \"all_layers\": 0.3990614414215088\n",
      "  },\n",
      "  \"all\": {\n",
      "    \"all_layers\": 0.4772564172744751\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(check_info_d, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check \"breaking\" window layer, i.e. those with sudden changes \n",
    "### For now: single layer drop > _th\n",
    "\n",
    "_th = 0.4\n",
    "check_info_l = []\n",
    "for i, d in enumerate(good_samples):\n",
    "#     for sect_k, sect_d in d['trace_scores'].items():\n",
    "    sect_k = 'all'\n",
    "    sect_d = d['trace_scores'][sect_k]\n",
    "    window_d = sect_d['window']\n",
    "    for l in range(1, 24):\n",
    "        if window_d[str(l-1)] - window_d[str(l)] > _th:\n",
    "            _info_d = {\n",
    "                'id': i,\n",
    "                'sect_k': sect_k,\n",
    "                'layer': l,\n",
    "                'last_layer_score': window_d[str(l-1)],\n",
    "                'this_layer_score': window_d[str(l)],\n",
    "            }\n",
    "            check_info_l.append(_info_d)\n",
    "            break\n",
    "len(check_info_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844, 1207)"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check_info_l), len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10),\n",
       " (2, 25),\n",
       " (3, 31),\n",
       " (4, 81),\n",
       " (5, 24),\n",
       " (6, 3),\n",
       " (7, 15),\n",
       " (8, 87),\n",
       " (9, 69),\n",
       " (10, 19),\n",
       " (11, 38),\n",
       " (12, 36),\n",
       " (13, 70),\n",
       " (14, 101),\n",
       " (15, 27),\n",
       " (16, 31),\n",
       " (17, 31),\n",
       " (18, 79),\n",
       " (19, 67)]"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "break_layer_counter = Counter([_d['layer'] for _d in check_info_l])\n",
    "sorted(break_layer_counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info_d in check_info_l:\n",
    "    if info_d['layer'] < 6:\n",
    "        print(info_d)\n",
    "        sample_id = info_d['id']\n",
    "        d = good_samples[sample_id]\n",
    "        print(d['enc_sentence'])\n",
    "        print(d['dec_prompt'], '---->', d['expect'])\n",
    "        print('Categories:', d['category'])\n",
    "        print('--' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counter_by_aspect = defaultdict(Counter)  # [asp_k, asp_v] -> count \n",
    "sample_counter = Counter()\n",
    "\n",
    "for info_d in check_info_l:\n",
    "    if info_d['layer'] < 6:\n",
    "        sample_id = info_d['id']\n",
    "        d = good_samples[sample_id]\n",
    "        text_match = d['category']['text_match']\n",
    "        node_len = d['category']['node_len']\n",
    "        sample_counter[(text_match, node_len)] += 1\n",
    "        \n",
    "        for asp_k, asp_v in d['category'].items():\n",
    "            sample_counter_by_aspect[asp_k][asp_v] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'sql_hardness': Counter({'medium': 58,\n",
       "                      'hard': 42,\n",
       "                      'extra': 51,\n",
       "                      'easy': 20}),\n",
       "             'node_role': Counter({'from': 105, 'join': 66}),\n",
       "             'text_match': Counter({'partial': 59,\n",
       "                      'no-match': 74,\n",
       "                      'exact': 38}),\n",
       "             'node_len': Counter({'4+': 60, '3': 51, '2': 24, '1': 36})})"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_counter_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('partial', '3'), 30),\n",
       " (('partial', '4+'), 29),\n",
       " (('exact', '1'), 22),\n",
       " (('no-match', '3'), 21),\n",
       " (('no-match', '4+'), 21),\n",
       " (('no-match', '2'), 18),\n",
       " (('no-match', '1'), 14),\n",
       " (('exact', '4+'), 10),\n",
       " (('exact', '2'), 6)]"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counter_by_aspect = defaultdict(Counter)  # [asp_k, asp_v] -> count \n",
    "sample_counter = Counter()\n",
    "\n",
    "for info_d in check_info_l:\n",
    "    if info_d['layer'] > 18:\n",
    "        sample_id = info_d['id']\n",
    "        d = good_samples[sample_id]\n",
    "        text_match = d['category']['text_match']\n",
    "        node_len = d['category']['node_len']\n",
    "        sample_counter[(text_match, node_len)] += 1\n",
    "        \n",
    "        for asp_k, asp_v in d['category'].items():\n",
    "            sample_counter_by_aspect[asp_k][asp_v] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'sql_hardness': Counter({'hard': 9,\n",
       "                      'medium': 27,\n",
       "                      'easy': 11,\n",
       "                      'extra': 20}),\n",
       "             'node_role': Counter({'join': 21, 'from': 46}),\n",
       "             'text_match': Counter({'exact': 46,\n",
       "                      'no-match': 20,\n",
       "                      'partial': 1}),\n",
       "             'node_len': Counter({'3': 12, '1': 46, '2': 6, '4+': 3})})"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_counter_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('exact', '1'), 38),\n",
       " (('no-match', '3'), 9),\n",
       " (('no-match', '1'), 8),\n",
       " (('exact', '3'), 3),\n",
       " (('no-match', '2'), 3),\n",
       " (('exact', '2'), 3),\n",
       " (('exact', '4+'), 2),\n",
       " (('partial', '4+'), 1)]"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.385318918917694e-12"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _min_p = 1.0\n",
    "\n",
    "# for i, d in enumerate(good_samples):\n",
    "#     sect_k = 'text'\n",
    "#     sect_d = d['trace_scores'][sect_k]\n",
    "#     _min_p = min(_min_p, sect_d['all_layers'])\n",
    "\n",
    "# _min_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 916, 1207, 1207)"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Systematic \n",
    "\n",
    "ob_sect_k = 'all'\n",
    "\n",
    "_th = 0.4\n",
    "\n",
    "check_info_l = []\n",
    "all_layers_eff_cnt = 0  # for this section to observe, how many samples are effective with all_layers\n",
    "window_eff_cnt = 0      # for this section to observe, how many samples are effective with any window \n",
    "\n",
    "for i, d in enumerate(good_samples):\n",
    "#     for sect_k, sect_d in d['trace_scores'].items():\n",
    "    sect_k = ob_sect_k\n",
    "    sect_d = d['trace_scores'][sect_k]\n",
    "    if sect_d['all_layers'] > 0.5:\n",
    "        # not effective\n",
    "        continue\n",
    "    else:\n",
    "        all_layers_eff_cnt += 1\n",
    "        \n",
    "    if min(sect_d['window'].values()) > 0.5:\n",
    "        # not effective\n",
    "        continue\n",
    "    else:\n",
    "        window_eff_cnt += 1\n",
    "        \n",
    "    window_d = sect_d['window']\n",
    "    for l in range(1, 24):\n",
    "        if window_d[str(l-1)] - window_d[str(l)] > _th:\n",
    "            _info_d = {\n",
    "                'id': i,\n",
    "                'sect_k': sect_k,\n",
    "                'layer': l,\n",
    "                'last_layer_score': window_d[str(l-1)],\n",
    "                'this_layer_score': window_d[str(l)],\n",
    "            }\n",
    "            check_info_l.append(_info_d)\n",
    "            break\n",
    "len(check_info_l), window_eff_cnt, all_layers_eff_cnt, len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctg_list = [(tm, nl) for tm in ['exact', 'partial', 'no-match'] for nl in ['1', '2', '3', '4+']]\n",
    "layer_list = [str(l) for l in range(1, 24)]\n",
    "\n",
    "ctg_elem2id = {elem : i for i, elem in enumerate(ctg_list)}\n",
    "layer_elem2id = {elem : i for i, elem in enumerate(layer_list)}\n",
    "\n",
    "cnt_matrix = np.zeros((len(ctg_list), len(layer_list)), int)\n",
    "\n",
    "for info_d in check_info_l:\n",
    "    sample_id = info_d['id']\n",
    "    d = good_samples[sample_id]\n",
    "    text_match = d['category']['text_match']\n",
    "    node_len = d['category']['node_len']\n",
    "    _ctg = (text_match, node_len)\n",
    "    _layer = str(info_d['layer'])\n",
    "    \n",
    "    _ctg_idx = ctg_elem2id[_ctg]\n",
    "    _layer_idx = layer_elem2id[_layer]\n",
    "    cnt_matrix[_ctg_idx, _layer_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Display the matrix using imshow\n",
    "im = ax.imshow(cnt_matrix, cmap='Blues')\n",
    "\n",
    "# Set the tick labels for the first and second dimensions\n",
    "ax.set_xticks(np.arange(len(layer_list)))\n",
    "ax.set_yticks(np.arange(len(ctg_list)))\n",
    "\n",
    "# Set the tick labels using the ctg_list and layer_list\n",
    "ax.set_xticklabels(layer_list)\n",
    "ax.set_yticklabels(ctg_list)\n",
    "\n",
    "ax.set_title(f'Section: {ob_sect_k}\\n')\n",
    "\n",
    "# Rotate the x-axis tick labels if needed\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Display the matrix using imshow\n",
    "im = ax.imshow(cnt_matrix, cmap='Blues')\n",
    "\n",
    "# Set the tick labels for the first and second dimensions\n",
    "ax.set_xticks(np.arange(len(layer_list)))\n",
    "ax.set_yticks(np.arange(len(ctg_list)))\n",
    "\n",
    "# Set the tick labels using the ctg_list and layer_list\n",
    "ax.set_xticklabels(layer_list)\n",
    "ax.set_yticklabels(ctg_list)\n",
    "\n",
    "ax.set_title(f'Section: {ob_sect_k}\\n')\n",
    "\n",
    "# Rotate the x-axis tick labels if needed\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-5.3: attention section mutual removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect_type = 'table_alias'\n",
    "\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/exp=5.3.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2039, (364, 364), 339, 1336, 1675, 'good / correct = 364 / 1700')"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples), \\\n",
    "f'good / correct = {n_good_samples} / {n_good_samples + n_too_easy}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg = {sect_k : defaultdict(int) for sect_k in good_samples[0]['trace_scores'].keys()}\n",
    "\n",
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for k, v in sect_d.items():\n",
    "            if k == 'window':\n",
    "                for l, s in v.items():\n",
    "                    trace_scores_avg[sect_k][f'{k}-{l}'] += s\n",
    "            else:\n",
    "                s = v\n",
    "                trace_scores_avg[sect_k][k] += s\n",
    "\n",
    "for sect_k, sect_d in trace_scores_avg.items():\n",
    "    for k, s in sect_d.items():\n",
    "        sect_d[k] = s / len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg by aspects (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_hardness': 'medium', 'node_role': 'where', 'text_match': 'no-match'}"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP patch for node_len category \n",
    "for d in good_samples + bad_samples:\n",
    "    node_len = len(d['answers_t'])\n",
    "    assert len(mt_uskg.tokenizer.tokenize(d['expect'])) == node_len, (d['expect'], node_len)\n",
    "    d['category']['node_len'] = str(node_len) if node_len <= 3 else '4+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_hardness': 'medium',\n",
       " 'node_role': 'group by',\n",
       " 'text_match': 'exact',\n",
       " 'node_len': '3'}"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: (sect_k, aspect, asp_val, layer) -> [scores]\n",
    "trace_scores_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n",
    "trace_scores_avg_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "trace_scores_cnt_by_aspect = defaultdict(lambda: defaultdict(int))  # no sect key & layer key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for aspect, asp_val in d['category'].items():\n",
    "            for k, v in sect_d.items():\n",
    "                if k == 'window':\n",
    "                    for l, s in v.items():\n",
    "                        if not (int(l) % 4 == 3): continue\n",
    "                        layer_k = f'{k}-{l}'\n",
    "                        trace_scores_by_aspect[sect_k][aspect][asp_val][layer_k].append(s)\n",
    "                else:\n",
    "                    layer_k = k\n",
    "                    s = v\n",
    "                    trace_scores_by_aspect[sect_k][aspect][asp_val][layer_k].append(s)\n",
    "                    \n",
    "for sect_k, d1 in trace_scores_by_aspect.items():\n",
    "    for asp_k, d2 in d1.items():\n",
    "        for asp_v, d3 in d2.items():\n",
    "            for layer_k, s in d3.items():\n",
    "                trace_scores_avg_by_aspect[sect_k][asp_k][asp_v][layer_k] = np.mean(s)\n",
    "                trace_scores_cnt_by_aspect[asp_k][asp_v] = len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sect_k, sect_d in trace_scores_avg_by_aspect.items():\n",
    "    sect_d['overall'] = dict()\n",
    "    for layer_k, s in trace_scores_avg[sect_k].items():\n",
    "        if layer_k.startswith('window'):\n",
    "            # only keep a subset of layers \n",
    "            _, l = layer_k.split('-')\n",
    "            if not (int(l) % 4 == 3): continue\n",
    "        sect_d['overall'][layer_k] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'sql_hardness': defaultdict(int,\n",
       "                         {'medium': 123, 'extra': 175, 'hard': 64, 'easy': 2}),\n",
       "             'node_role': defaultdict(int,\n",
       "                         {'select': 191,\n",
       "                          'group by': 33,\n",
       "                          'join': 12,\n",
       "                          'where': 111,\n",
       "                          'order by': 17}),\n",
       "             'text_match': defaultdict(int,\n",
       "                         {'exact': 230, 'partial': 28, 'no-match': 106}),\n",
       "             'node_len': defaultdict(int, {'3': 364})})"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_cnt_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_scores_avg_by_aspect['c->p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_d = ctu.nested_json_processing(trace_scores_avg_by_aspect, func=lambda x: np.format_float_positional(x, precision=4, min_digits=4))\n",
    "# dump_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/summ-exp=5.3.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(dump_path, 'w') as f:\n",
    "    json.dump(dump_d, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (one-time temp patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect_type = 'table_alias'\n",
    "# orig_res_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/no_c2p_exp=5.3.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "# add_res_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/exp=5.3.1+c2p_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "# merge_res_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/exp=5.3.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(orig_res_path, 'r') as f:\n",
    "#     orig_all_samples = [json.loads(l) for l in f]\n",
    "# with open(add_res_path, 'r') as f:\n",
    "#     add_all_samples = [json.loads(l) for l in f]\n",
    "\n",
    "# f = open(merge_res_path, 'w')\n",
    "    \n",
    "# for i, (orig_ex, add_ex) in enumerate(zip(orig_all_samples, add_all_samples)):\n",
    "#     assert len(orig_ex['trace_results']) == len(add_ex['trace_results']), i\n",
    "#     # There is randomness in the order of expected node (from set()), thus sorting here \n",
    "#     orig_ex['trace_results'].sort(key=lambda d: len(d['dec_prompt']))\n",
    "#     add_ex['trace_results'].sort(key=lambda d: len(d['dec_prompt']))\n",
    "#     for j, (orig_d, add_d) in enumerate(zip(orig_ex['trace_results'], add_ex['trace_results'])):\n",
    "#         assert orig_d['is_good_sample'] == add_d['is_good_sample'], (i, j)\n",
    "#         if not orig_d['is_good_sample']:\n",
    "#             continue\n",
    "            \n",
    "#         # is good sample: add the new sections \n",
    "#         orig_d['trace_scores']['c->p'] = add_d['trace_scores']['c->p']\n",
    "        \n",
    "#         # put all at end in the dict \n",
    "#         _t = orig_d['trace_scores']['all']\n",
    "#         del orig_d['trace_scores']['all']\n",
    "#         orig_d['trace_scores']['all'] = _t\n",
    "        \n",
    "#     f.write(json.dumps(orig_ex, indent=None) + '\\n')\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USKG error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from play_pred()\n",
    "\n",
    "def pred_sql(mt, ex):\n",
    "    text_in = ex['text_in']\n",
    "    struct_in = ex['struct_in']\n",
    "\n",
    "    txt = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "    \n",
    "    tokenized_txt = mt.tokenizer_uskg([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    device = mt.model.device\n",
    "    pred = mt.tokenizer_uskg.batch_decode(\n",
    "      mt.model.generate(\n",
    "        torch.tensor(tokenized_txt.data['input_ids'], dtype=int, device=device),\n",
    "        torch.tensor(tokenized_txt.data['attention_mask'], dtype=int, device=device),\n",
    "        num_beams=1, \n",
    "        max_length=256\n",
    "        ), \n",
    "      skip_special_tokens=True \n",
    "    )\n",
    "    return pred[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from evaluator.evaluate_one()\n",
    "\n",
    "def evaluate_sql(evaluator, db_name, gold, predicted):\n",
    "    schema = evaluator.schemas[db_name]\n",
    "    g_sql = sp_eval.get_sql(schema, gold)\n",
    "    hardness = evaluator.eval_hardness(g_sql)\n",
    "    # self.scores[hardness][\"count\"] += 1\n",
    "    # self.scores[\"all\"][\"count\"] += 1\n",
    "\n",
    "    parse_error = False\n",
    "    try:\n",
    "        p_sql = sp_eval.get_sql(schema, predicted)\n",
    "    except:\n",
    "        # If p_sql is not valid, then we will use an empty sql to evaluate with the correct sql\n",
    "        p_sql = {\n",
    "            \"except\": None,\n",
    "            \"from\": {\"conds\": [], \"table_units\": []},\n",
    "            \"groupBy\": [],\n",
    "            \"having\": [],\n",
    "            \"intersect\": None,\n",
    "            \"limit\": None,\n",
    "            \"orderBy\": [],\n",
    "            \"select\": [False, []],\n",
    "            \"union\": None,\n",
    "            \"where\": [],\n",
    "        }\n",
    "\n",
    "        # TODO fix\n",
    "        parse_error = True\n",
    "\n",
    "    # rebuild sql for value evaluation\n",
    "    kmap = evaluator.kmaps[db_name]\n",
    "    g_valid_col_units = sp_eval.build_valid_col_units(g_sql[\"from\"][\"table_units\"], schema)\n",
    "    g_sql = sp_eval.rebuild_sql_val(g_sql)\n",
    "    g_sql = sp_eval.rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n",
    "    p_valid_col_units = sp_eval.build_valid_col_units(p_sql[\"from\"][\"table_units\"], schema)\n",
    "    p_sql = sp_eval.rebuild_sql_val(p_sql)\n",
    "    p_sql = sp_eval.rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n",
    "    \n",
    "    exec_score = None\n",
    "    partial_scores = None\n",
    "    exact_score = None\n",
    "    if evaluator.etype in [\"all\", \"exec\"]:\n",
    "        exec_score = sp_eval.eval_exec_match(\n",
    "            evaluator.db_paths[db_name], predicted, gold, p_sql, g_sql\n",
    "        )\n",
    "        exec_score = int(exec_score)\n",
    "    if evaluator.etype in [\"all\", \"match\"]:\n",
    "        partial_scores = evaluator.eval_partial_match(p_sql, g_sql)\n",
    "        exact_score = evaluator.eval_exact_match(p_sql, g_sql, partial_scores)\n",
    "        # update_scores_match(self.scores, exact_score, hardness, partial_scores, PARTIAL_TYPES)\n",
    "\n",
    "    return {\n",
    "        \"predicted\": predicted,\n",
    "        \"gold\": gold,\n",
    "        \"predicted_parse_error\": parse_error,\n",
    "        \"hardness\": hardness,\n",
    "        \"exact\": exact_score,\n",
    "        \"partial\": partial_scores,\n",
    "        \"exec\": exec_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'\n",
    "\n",
    "def execute_sql(db, sql_str):\n",
    "    db_path = os.path.join(db_dir, db, f'{db}.sqlite')\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(sql_str)\n",
    "        res = cursor.fetchall()\n",
    "    except:\n",
    "        res = 'ERROR'\n",
    "    conn.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph', 'serialized_schema', 'struct_in', 'text_in', 'seq_out'])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = processed_spider_dev[503]\n",
    "ex.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('battle_death',\n",
       " \"How many battles did not lose any ship with tonnage '225'?\",\n",
       " \"select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage = '225' );\",\n",
       " 'select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage > 225 )')"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred_sql(mt_uskg, ex)\n",
    "ex['db_id'], ex['text_in'], ex['seq_out'], pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "txt = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "txt_toks = mt_uskg.tokenizer_uskg.tokenize(txt)\n",
    "len(txt_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage = '225' );\",\n",
       " [(7,)])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_res = execute_sql(ex['db_id'], ex['seq_out'])\n",
    "ex['seq_out'], exec_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage > 225 )',\n",
       " [(3,)])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_res = execute_sql(ex['db_id'], pred)\n",
    "pred, exec_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8,)]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_pred = \"\"\"\n",
    "SELECT COUNT(DISTINCT battle.id) AS num_battles\n",
    "FROM battle\n",
    "LEFT JOIN ship ON battle.id = ship.lost_in_battle\n",
    "WHERE (ship.tonnage != '225' OR ship.tonnage IS NULL)\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(ex['db_id'], chatgpt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = evaluate_sql(evaluator, db_name=spider_ex['db_id'], gold=spider_ex['seq_out'], predicted=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab76cae127742adbc790384e85093b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Eval all \n",
    "# TODO: rerun with tokenizer_uskg decoding \n",
    "\n",
    "eval_sql_results = []\n",
    "\n",
    "for ex_id, ex in enumerate(tqdm(processed_spider_dev)):\n",
    "    pred = pred_sql(mt_uskg, ex)\n",
    "    eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "    eval_sql_results.append(eval_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6692456479690522, 0.6808510638297872)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "avg_exact, avg_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(eval_sql_results):\n",
    "    if d['exact'] and d['exec']:\n",
    "        continue\n",
    "    err_msg = ('A' if not d['exact'] else '') + ('X' if not d['exec'] else '')\n",
    "    ex = processed_spider_dev[i]\n",
    "    print(f'ID = {i}: {err_msg}  ({ex[\"db_id\"]}) {ex[\"text_in\"]}')\n",
    "    print(f'Pred: {d[\"predicted\"]}')\n",
    "    print(f'Gold: {d[\"gold\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator.schemas['dog_kennels'].schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acc by db_id \n",
    "eval_sql_results_by_db_id = defaultdict(list)\n",
    "\n",
    "for i, d in enumerate(eval_sql_results):\n",
    "    d['ex_id'] = i\n",
    "    ex = processed_spider_dev[i]\n",
    "    db_id = ex['db_id']\n",
    "    eval_sql_results_by_db_id[db_id].append(d)\n",
    "\n",
    "len(eval_sql_results_by_db_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concert_singer\t0.8889\t0.8889\n",
      "pets_1\t0.5714\t0.7381\n",
      "car_1\t0.3478\t0.3913\n",
      "flight_2\t0.7000\t0.7500\n",
      "employee_hire_evaluation\t0.9474\t0.9737\n",
      "cre_Doc_Template_Mgt\t0.8333\t0.9048\n",
      "course_teach\t0.8667\t0.9333\n",
      "museum_visit\t0.7222\t0.8333\n",
      "wta_1\t0.6774\t0.6129\n",
      "battle_death\t0.5000\t0.5000\n",
      "student_transcripts_tracking\t0.6667\t0.6795\n",
      "tvshow\t0.7258\t0.6613\n",
      "poker_player\t0.8750\t0.8750\n",
      "voter_1\t0.6000\t0.6667\n",
      "world_1\t0.5083\t0.4833\n",
      "orchestra\t0.8000\t0.8750\n",
      "network_1\t0.6250\t0.4643\n",
      "dog_kennels\t0.5854\t0.5976\n",
      "singer\t0.8667\t0.8667\n",
      "real_estate_properties\t0.5000\t0.5000\n"
     ]
    }
   ],
   "source": [
    "for db_id, results in eval_sql_results_by_db_id.items():\n",
    "    _avg_exact = np.mean([d['exact'] for d in results])\n",
    "    _avg_exec = np.mean([d['exec'] for d in results])\n",
    "    print(f'{db_id}\\t{_avg_exact:.4f}\\t{_avg_exec:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model intermediate inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k, v in mt_uskg.model.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.t5.configuration_t5.T5Config"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t5_config = mt_uskg.model.config\n",
    "type(_t5_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_t5_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Attention(\n",
       "  (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "  (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_module = uskg.models.prompt.modeling_t5.T5Attention(_t5_config)\n",
    "sa_module.eval()\n",
    "sa_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_module.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 3\n",
    "seq_len = 10\n",
    "dim = 1024\n",
    "fake_sa_input = torch.zeros(bsz, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attn_output, present_key_value_state, position_bias, attn_weights = sa_module(fake_sa_input, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 1024])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_key_value_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 10, 10])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 10, 10])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_bias.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_analysis_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the accelerate of the car make amc hornet sportabout (sw)?',\n",
       " '| car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker ( amc ) , fullname , country | model_list : modelid , maker , model ( amc ) | car_names : makeid , model ( amc ) , make ( amc hornet , amc hornet sportabout (sw) ) | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " \"select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t2.make = 'amc hornet sportabout (sw)';\")"
      ]
     },
     "execution_count": 862,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_id = 111\n",
    "a_ex_id = 0\n",
    "\n",
    "ex = processed_spider_dev[ex_id]\n",
    "ex['text_in'], \\\n",
    "ex['struct_in'], \\\n",
    "ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp test\n",
    "# ex['seq_out'] = 'select year from cars_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex_list = ctu.create_analysis_sample_dicts(\n",
    "                mt_uskg, ex,\n",
    "                subject_type='column',\n",
    "                remove_struct_duplicate_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph', 'serialized_schema', 'struct_in', 'text_in', 'seq_out', 'enc_sentence', 'enc_tokenized', 'text_range', 'struct_range', 'struct_node_ranges_dict', 'dec_prompt', 'expect', 'expect_type', 'remove_struct_duplicate_nodes', 'parsed_struct_in', 'col2table', 'token_ranges_dict', 'node_name_ranges', 'expect_input_ranges', 'alias2table', 'self_ranges', 'context_ranges', 'category'])"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex_list[a_ex_id].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1': 'cars_data', 't2': 'car_names'}"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex_list[a_ex_id]['alias2table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(d['dec_prompt'], d['expect'], d['node_name_ranges'], d['expect_input_ranges'], '------',\\\n",
    "  d['self_ranges'], d['context_ranges'],\\\n",
    "  d['category'], '------' * 2) for d in a_ex_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(a_ex_list[a_ex_id])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ctu.add_clean_prediction(mt_uskg, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse_sql_alias2table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1': 'table_name', 't2': 'other_table', 't3': 'ttt'}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sql = 'SELECT t2.aaa , t3.ccc FROM table_name as t1 JOIN other_table as t2 on table_name.a_a = other_table.b_a JOIN ttt as t3 on other_table.asth = ttt.asth'\n",
    "ctu.parse_sql_alias2table(_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select t2.aaa, distinct(t3.ccc), count(*) from table_name as t1 join other_table as t2 on table_name.a_a = other_table.b_a join ttt as t3 on other_table.asth = ttt.asth where t2.col like',\n",
       " 'select t2.aaa, distinct(t3.ccc), count(*) from table_name as t1 join other_table as t2 on table_name.a_a = other_table.b_a join ttt as t3 on other_table.asth = ttt.asth where t2.col like %hey']"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sql = 'SELECT t2.aaa, DISTINCT(t3.ccc), COUNT(*) FROM table_name as t1 JOIN other_table as t2 on table_name.a_a = other_table.b_a JOIN ttt as t3 on other_table.asth = ttt.asth WHERE t2.col like %hey%'.lower()\n",
    "prompts = ctu.make_syntax_dec_prompt(_sql, '%', is_punct=True)\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "('.', ['select', 't1', '.', 'accelerate', 'from', 'cars_data', 'as', 't1', 'join', 'car_names', 'as', 't2', 'on', 't1', '.', 'id', '=', 't2', '.', 'makeid', 'where', 't2', '.', 'make', '=', \"'\", 'amc', 'hornet', 'sportabout', '(', 'sw', ')', \"'\", ';'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [875]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a_ex_list_syntax \u001b[38;5;241m=\u001b[39m \u001b[43mctu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_syntax_analysis_sample_dicts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmt_uskg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/rome/notebooks/experiments/causal_trace_uskg.py:2424\u001b[0m, in \u001b[0;36mcreate_syntax_analysis_sample_dicts\u001b[0;34m(mt, ex)\u001b[0m\n\u001b[1;32m   2422\u001b[0m _phrase \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_phrase_cache)\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _phrase:\n\u001b[0;32m-> 2424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _phrase \u001b[38;5;129;01min\u001b[39;00m SQL_SYNTAX_PHRASES \u001b[38;5;241m+\u001b[39m SQL_SYNTAX_PUNCTS, (_phrase, sql_tokens)\n\u001b[1;32m   2425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _phrase \u001b[38;5;129;01min\u001b[39;00m SQL_SYNTAX_PHRASES:\n\u001b[1;32m   2426\u001b[0m         syntax_phrases\u001b[38;5;241m.\u001b[39madd(_phrase)\n",
      "\u001b[0;31mAssertionError\u001b[0m: ('.', ['select', 't1', '.', 'accelerate', 'from', 'cars_data', 'as', 't1', 'join', 'car_names', 'as', 't2', 'on', 't1', '.', 'id', '=', 't2', '.', 'makeid', 'where', 't2', '.', 'make', '=', \"'\", 'amc', 'hornet', 'sportabout', '(', 'sw', ')', \"'\", ';'])"
     ]
    }
   ],
   "source": [
    "a_ex_list_syntax = ctu.create_syntax_analysis_sample_dicts(\n",
    "                mt_uskg, ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### context_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the name of the different car makers who produced a car in 1970?',\n",
       " '| car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " 'select year from cars_data',\n",
       " 'year')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex = dict(a_ex_list[0])\n",
    "a_ex['text_in'], a_ex['struct_in'], a_ex['seq_out'], a_ex['expect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex['alias2table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_range = a_ex['text_range']\n",
    "struct_range = a_ex['struct_range']\n",
    "\n",
    "# For full context tokens, use [0, L] and [R, -1]\n",
    "# L: node left max end index ; R: node right min start index\n",
    "\n",
    "token_ranges_dict = a_ex['token_ranges_dict']\n",
    "_all_node_range_lists = list(token_ranges_dict['col_name_ranges'].values()) + list(token_ranges_dict['table_name_ranges'].values()) + list(token_ranges_dict['db_id_ranges'].values())\n",
    "_all_node_ranges = [rg\n",
    "                    for rg_list in _all_node_range_lists\n",
    "                    for rg in rg_list]\n",
    "_all_left_endpoint = [s for s, e in _all_node_ranges] + [struct_range[1]]\n",
    "_all_right_endpoint = [e for s, e in _all_node_ranges] + [struct_range[0]]\n",
    "# TODO: pull this part out to the shared function (e.g. create_analysis_sample_dicts)\n",
    "# TODO: test for columns on ends\n",
    "\n",
    "expect_input_ranges = a_ex['expect_input_ranges']    # list of ranges of node-of-interest (code allows dup)\n",
    "# tok_indices = [i for s, e in expect_input_ranges for i in range(s, e)]\n",
    "# expect_input_indices = [i for s, e in expect_input_ranges for i in range(s, e)]\n",
    "enc_sentence = a_ex['enc_sentence']\n",
    "dec_prompt = a_ex['dec_prompt']\n",
    "# node = a_ex['expect']\n",
    "\n",
    "context_range_endpoints = [struct_range[0]]\n",
    "self_range_endpoints = []       # This is different from `expect_input_ranges`: this includes boundary toks\n",
    "for tok_s, tok_e in expect_input_ranges:\n",
    "    _l = max([e for e in _all_right_endpoint if e <= tok_s])\n",
    "    _r = min([s for s in _all_left_endpoint if s >= tok_e])\n",
    "    context_range_endpoints.extend([_l, _r])\n",
    "    self_range_endpoints.extend([_l, _r])\n",
    "context_range_endpoints.append(struct_range[1])\n",
    "\n",
    "self_ranges = [(self_range_endpoints[i], self_range_endpoints[i+1])\n",
    "                for i in range(0, len(self_range_endpoints), 2)]\n",
    "self_tok_indices = [i for s, e in self_ranges for i in range(s, e)]\n",
    "\n",
    "context_ranges = [(context_range_endpoints[i], context_range_endpoints[i+1])\n",
    "                    for i in range(0, len(context_range_endpoints), 2)]\n",
    "context_ranges = [(s, e) for s, e in context_ranges if e > s]\n",
    "context_tok_indices = corrupt_tok_indices = [i for s, e in context_ranges for i in range(s, e)]\n",
    "\n",
    "text_tok_indices = list(range(*text_range))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(137, 140)], [(24, 137)])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_ranges, context_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tok_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", year\n"
     ]
    }
   ],
   "source": [
    "for s, e in self_ranges:\n",
    "    _piece = tokenizer.decode(a_ex['enc_tokenized']['input_ids'][s : e])\n",
    "    print(_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| car_1 | continents : contid, continent | countries : countryid, countryname, continent | car_makers : id, maker, fullname, country | model_list : modelid, maker, model | car_names : makeid, model, make | cars_data : id, mpg, cylinders, edispl, horsepower, weight, accelerate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, e in context_ranges:\n",
    "    _piece = tokenizer.decode(a_ex['enc_tokenized']['input_ids'][s : e])\n",
    "    print(_piece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_uskg_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex = dict(a_ex_list[a_ex_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sentence = a_ex['enc_sentence']\n",
    "dec_prompt = a_ex['dec_prompt']\n",
    "expect = a_ex['expect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the name of the different car makers who produced a car in 1970?; structed knowledge: | car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " 'select distinct t1.maker from car_makers as t1 join model_list as t2 on t1.id = t2.maker join car_names as t3 on t2.model = t3.model join cars_data as t4 on t3.',\n",
       " 'makeid')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sentence, dec_prompt, expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence] * 11,\n",
    "    [dec_prompt] * 11,\n",
    "    answer=expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = ctu.run_model_forward_uskg(mt_uskg.model, **inp, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, torch.Size([11, 16, 141, 151]))"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers, (bsz, n_heads, seq_len, seq_len + prev_len)\n",
    "len(_out.encoder_attentions), _out.encoder_attentions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, torch.Size([11, 16, 2, 151]))"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers, (bsz, n_heads, prompt_len, seq_len + prev_len)\n",
    "len(_out.cross_attentions), _out.cross_attentions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, torch.Size([11, 16, 2, 12]))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers, (bsz, n_heads, prompt_len, prompt_len + prev_len)\n",
    "len(_out.decoder_attentions), _out.decoder_attentions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask'])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex = dict(a_ex_list[a_ex_id])\n",
    "a_ex = ctu.add_clean_prediction(mt_uskg, a_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enc_sentence': 'Which city has the most frequent destination airport?; structed knowledge: | flight_2 | airlines : uid , airline , abbreviation , country | airports : city , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport',\n",
       " 'seq_out': 'select t1.city from airports as t1 join flights as t2 on t1.airportcode = t2.destairport group by t1.city order by count(*) desc limit 1',\n",
       " 'dec_prompt': 'select t1.city from airports as t1 join flights as t2 on t1.airportcode = t2.destairport group by t1.',\n",
       " 'expect': 'city',\n",
       " 'expect_type': 'column',\n",
       " 'db_id': 'flight_2',\n",
       " 'expect_input_ranges': [(45, 46)],\n",
       " 'expect_table': 'airports',\n",
       " 'answer': 'city',\n",
       " 'base_score': 0.9983423948287964,\n",
       " 'answers_t': [6726],\n",
       " 'correct_prediction': True,\n",
       " 'category': {'sql_hardness': 'extra',\n",
       "  'node_role': 'group by',\n",
       "  'text_match': 'exact'}}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ctu.make_basic_result_dict(a_ex)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sentence = a_ex['enc_sentence']\n",
    "dec_prompt = a_ex['dec_prompt']\n",
    "expect = a_ex['expect']\n",
    "answer = result['answer']\n",
    "answers_t = result['answers_t']\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence] * 11,\n",
    "    [dec_prompt] * 11,\n",
    "    answer=expect)\n",
    "\n",
    "text_range = a_ex['text_range']\n",
    "struct_range = a_ex['struct_range']\n",
    "\n",
    "self_ranges = a_ex['self_ranges']\n",
    "context_ranges = a_ex['context_ranges']\n",
    "\n",
    "self_tok_indices = [i for s, e in self_ranges for i in range(s, e)]\n",
    "context_tok_indices = corrupt_tok_indices = [i for s, e in context_ranges for i in range(s, e)]\n",
    "text_tok_indices = list(range(*text_range))\n",
    "struct_tok_indices = list(range(*struct_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "#                     for tnum in range(*struct_range)],\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6726], 'city', 0.8450507521629333)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_t, answer, _score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8450507521629333"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_to_corrupt = [(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", 0, \"embed\"))\n",
    "                for tnum in text_tok_indices]\n",
    "\n",
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    states_to_corrupt=states_to_corrupt,\n",
    "#     tokens_to_mix=corrupt_tok_indices,\n",
    "#     tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ").item()\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9952, device='cuda:0')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pair of identical input to test correctness \n",
    "\n",
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "#                     for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_patch=[],\n",
    "    states_to_patch_1st_pass=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 12))\n",
    "                    for tnum in text_tok_indices],\n",
    "    states_to_unpatch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 23))\n",
    "                    for tnum in struct_tok_indices],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    tokens_to_mix_1st_pass=context_tok_indices,\n",
    "    replace=True,\n",
    ").item()\n",
    "\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9952, device='cuda:0')"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "#                     for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_patch=[],\n",
    "    states_to_patch_1st_pass=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 12))\n",
    "                    for tnum in text_tok_indices],\n",
    "    states_to_unpatch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 23))\n",
    "                    for tnum in struct_tok_indices],\n",
    "    answers_t=answers_t,\n",
    "    states_to_corrupt=[(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", 0, \"embed\"))\n",
    "                    for tnum in text_tok_indices],\n",
    "    states_to_corrupt_1st_pass=[(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", 0, \"embed\"))\n",
    "                    for tnum in context_tok_indices],\n",
    "    replace=True,\n",
    ").item()\n",
    "\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7253002524375916"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test corrupting attention \n",
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "#                     for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    states_to_corrupt=[(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", l, \"self_attn\"))\n",
    "                    for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers)],\n",
    "    replace=True,\n",
    ").item()\n",
    "\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n, w in mt_uskg.model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_probs = ctu.run_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "#                     for tnum in range(*struct_range)],\n",
    "    states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "                    for tnum in self_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_unpatch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "                    for tnum in self_tok_indices],\n",
    "    answer_len=len(answers_t),\n",
    "    tokens_to_mix=corrupt_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32102])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.], device='cuda:0'),\n",
       "indices=tensor([7634], device='cuda:0'))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(vocab_probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs[0, 7634]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2642e-25, 1.2223e-15, 7.3942e-18,  ..., 9.1578e-20, 2.6884e-39,\n",
       "         2.8131e-39]], device='cuda:0')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trace - partial edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uskg.models.prompt.modeling_t5 import T5Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_t5_config = copy.deepcopy(mt_uskg.model.config)\n",
    "_t5_config.d_model = 10\n",
    "_t5_config.d_kv = 4\n",
    "_t5_config.num_heads = 3\n",
    "_t5_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Attention(\n",
       "  (q): Linear(in_features=10, out_features=12, bias=False)\n",
       "  (k): Linear(in_features=10, out_features=12, bias=False)\n",
       "  (v): Linear(in_features=10, out_features=12, bias=False)\n",
       "  (o): Linear(in_features=12, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_attn_module = T5Attention(config=_t5_config)\n",
    "test_attn_module.eval()\n",
    "test_attn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('q.weight', torch.Size([12, 10])),\n",
       " ('k.weight', torch.Size([12, 10])),\n",
       " ('v.weight', torch.Size([12, 10])),\n",
       " ('o.weight', torch.Size([10, 12]))]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.size()) for k, v in test_attn_module.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'q', 'k', 'v', 'o']"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k, v in test_attn_module.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7, 10]),\n",
       " [('prev_key', torch.Size([1, 3, 2, 4])),\n",
       "  ('prev_value', torch.Size([1, 3, 2, 4])),\n",
       "  ('prev_key_padding_mask', torch.Size([1, 2]))])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_bs = 1\n",
    "_test_seqlen = 7\n",
    "_test_prevlen = 2\n",
    "\n",
    "_test_h = torch.randn(_test_bs, _test_seqlen, _t5_config.d_model)\n",
    "# _test_h[:, 0] = 999.0\n",
    "_test_prefix = {\n",
    "    'prev_key': torch.randn(_test_bs, _t5_config.num_heads, _test_prevlen, _t5_config.d_kv),\n",
    "    'prev_value': torch.randn(_test_bs, _t5_config.num_heads, _test_prevlen, _t5_config.d_kv),\n",
    "    'prev_key_padding_mask': torch.zeros(_test_bs, _test_prevlen).bool()\n",
    "}\n",
    "_test_h.size(), [(k, v.size()) for k, v in _test_prefix.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8561, -0.0552,  0.3845,  1.1441, -1.2765, -0.6869, -0.9646,\n",
       "          -1.0255, -2.3659, -1.7153],\n",
       "         [ 1.0685, -0.1617,  0.8310, -1.7257,  0.3674,  1.7559,  0.5763,\n",
       "          -0.9344,  0.9016,  0.7490],\n",
       "         [-1.1887, -1.0820, -0.5925,  0.7623, -0.6538, -0.0067,  0.5618,\n",
       "           1.3310,  1.2580, -0.6973],\n",
       "         [ 0.2807,  0.0763, -0.3539,  0.9494, -0.1557, -0.7645, -0.2103,\n",
       "          -1.0175, -0.3029, -0.0376],\n",
       "         [ 0.0984,  0.5610, -2.3323,  1.3421, -1.0381, -1.8568, -0.7754,\n",
       "          -1.6037,  0.2501, -1.4155],\n",
       "         [-1.4474, -0.4784,  0.0972, -0.3393,  1.2340,  0.7611, -0.4786,\n",
       "           0.0506, -0.1188,  2.7051],\n",
       "         [ 2.0642, -0.0186, -0.8283,  1.0852,  0.9819, -0.4044,  0.9831,\n",
       "          -0.2723,  0.2037,  1.6401]]])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev_key': tensor([[[[-2.2578,  0.2019,  0.3287,  0.3906],\n",
       "           [-1.2143, -0.7039, -1.0298,  0.1425]],\n",
       " \n",
       "          [[ 0.3967,  1.5168,  0.0967,  1.4454],\n",
       "           [ 0.1648,  0.2483,  1.5992,  1.2469]],\n",
       " \n",
       "          [[ 1.3875,  0.4460, -0.2676, -1.2290],\n",
       "           [ 2.0209,  1.1736,  0.8446,  0.8827]]]]),\n",
       " 'prev_value': tensor([[[[ 0.8351,  1.2052,  1.4187, -0.5358],\n",
       "           [-1.4274,  0.2792,  2.0149,  1.3695]],\n",
       " \n",
       "          [[-0.0379,  1.8999, -0.4236, -0.9176],\n",
       "           [ 1.5794,  1.1735, -0.2925,  2.2855]],\n",
       " \n",
       "          [[ 1.1935,  0.9343, -0.5582,  0.8163],\n",
       "           [-1.3765,  0.4046,  1.0941,  0.4058]]]]),\n",
       " 'prev_key_padding_mask': tensor([[False, False]])}"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_mask = torch.zeros(1, 1, 1, _test_seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = test_attn_module.forward(\n",
    "    _test_h,\n",
    "    mask=_test_mask,\n",
    "    prefix=_test_prefix,\n",
    "    output_attentions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3544, 0.3264, 0.2939, 0.3878, 0.4695, 0.2997, 0.1775],\n",
       "         [0.4028, 0.1655, 0.2949, 0.5148, 0.4162, 0.4189, 0.4683],\n",
       "         [0.5839, 0.4639, 0.1393, 0.4925, 0.6272, 0.1917, 0.4634]]])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out[3].sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_mask_attention(att, mix_mask):\n",
    "    # token 4,5,6 (in real_seq) not attending to 2 (in full_seq, i.e. 0 in real_seq)\n",
    "    # att: (bs, n_head, real_seq, full_seq)\n",
    "#     att[:, :, 4:, 2] = 0.0\n",
    "\n",
    "    _zero = torch.tensor(0, dtype=att.dtype)\n",
    "    att = torch.where(mix_mask, _zero, att)\n",
    "\n",
    "    print(att)\n",
    "    return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ True, False,  True, False,  True, False,  True, False,  True],\n",
       "           [False, False, False, False, False, False, False, False, False],\n",
       "           [ True, False,  True, False,  True, False,  True, False,  True],\n",
       "           [False, False, False, False, False, False, False, False, False],\n",
       "           [ True, False,  True, False,  True, False,  True, False,  True],\n",
       "           [False, False, False, False, False, False, False, False, False],\n",
       "           [ True, False,  True, False,  True, False,  True, False,  True]]]]),\n",
       " tensor([[[[ True,  True, False, False, False,  True,  True,  True,  True],\n",
       "           [ True,  True, False, False, False,  True,  True,  True,  True],\n",
       "           [ True,  True, False, False, False,  True,  True,  True,  True],\n",
       "           [ True,  True,  True,  True,  True, False, False, False, False],\n",
       "           [ True,  True,  True,  True,  True, False, False, False, False],\n",
       "           [ True,  True,  True,  True,  True, False, False, False, False],\n",
       "           [ True,  True,  True,  True,  True, False, False, False, False]]]]))"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_mask = torch.zeros(1, 1, _test_seqlen, _test_seqlen + _test_prevlen, dtype=bool)\n",
    "mix_mask[:, :, ::2, ::2] = 1\n",
    "\n",
    "mix_mask_2 = torch.zeros(1, 1, _test_seqlen, _test_seqlen + _test_prevlen, dtype=bool)\n",
    "mix_mask_2[:, :, :, :_test_prevlen] = 1\n",
    "mix_mask_2[:, :, 3:, _test_prevlen : _test_prevlen+3] = 1\n",
    "mix_mask_2[:, :, :3, _test_prevlen+3 :] = 1\n",
    "\n",
    "mix_mask, mix_mask_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_mask | mix_mask_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attn_module.ext_attention_weights_fn = lambda att : _test_mask_attention(att, mix_mask_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7fc33d6065e0>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test_attn_module.ext_attention_weights_fn)\n",
    "print(test_attn_module.ext_attention_logits_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_out = test_attn_module.forward(\n",
    "    _test_h,\n",
    "    mask=_test_mask,\n",
    "    prefix=_test_prefix,\n",
    "    output_attentions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex = dict(a_ex_list[a_ex_id])\n",
    "a_ex = ctu.add_clean_prediction(mt_uskg, a_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enc_sentence': 'What is the accelerate of the car make amc hornet sportabout (sw)?; structed knowledge: | car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker ( amc ) , fullname , country | model_list : modelid , maker , model ( amc ) | car_names : makeid , model ( amc ) , make ( amc hornet , amc hornet sportabout (sw) ) | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " 'seq_out': \"select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t2.make = 'amc hornet sportabout (sw)';\",\n",
       " 'dec_prompt': 'select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t2.',\n",
       " 'expect': 'make',\n",
       " 'expect_type': 'column',\n",
       " 'db_id': 'car_1',\n",
       " 'expect_input_ranges': [(121, 145)],\n",
       " 'expect_table': 'car_names',\n",
       " 'answer': 'make',\n",
       " 'base_score': 0.9998800754547119,\n",
       " 'answers_t': [19509],\n",
       " 'correct_prediction': True,\n",
       " 'category': {'sql_hardness': 'medium',\n",
       "  'node_role': 'where',\n",
       "  'text_match': 'exact'}}"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ctu.make_basic_result_dict(a_ex)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sentence = a_ex['enc_sentence']\n",
    "dec_prompt = a_ex['dec_prompt']\n",
    "expect = a_ex['expect']\n",
    "answer = result['answer']\n",
    "answers_t = result['answers_t']\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence] * 2,\n",
    "    [dec_prompt] * 2,\n",
    "    answer=expect)\n",
    "\n",
    "text_range = a_ex['text_range']\n",
    "struct_range = a_ex['struct_range']\n",
    "\n",
    "self_ranges = a_ex['self_ranges']\n",
    "context_ranges = a_ex['context_ranges']\n",
    "\n",
    "self_tok_indices = [i for s, e in self_ranges for i in range(s, e)]\n",
    "context_tok_indices = corrupt_tok_indices = [i for s, e in context_ranges for i in range(s, e)]\n",
    "text_tok_indices = list(range(*text_range))\n",
    "struct_tok_indices = list(range(*struct_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "#                     for tnum in range(*struct_range)],\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=context_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([19509], 'make', 0.9987825751304626)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_t, answer, _score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "_score = ctu.trace_attention_manip_uskg_multi_token(\n",
    "# _probs = ctu.run_attention_manip_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     answer_len=len(answers_t),\n",
    "    answers_t=answers_t,\n",
    "#     states_to_patch=[],\n",
    "    layers_to_mix=[ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in range(24)],\n",
    "    src_tokens_to_mix=text_tok_indices + struct_tok_indices, # src doesn't have prefix \n",
    "#     src_tokens_to_mix=[-1],\n",
    "    tgt_tokens_to_mix=list(range(10)) + [i + 10 for i in text_tok_indices + struct_tok_indices],  # tgt has prefix \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.6447], device='cuda:0'),\n",
       "indices=tensor([4350], device='cuda:0'))"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_probs.max(dim=-1)\n",
    "# _score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name'"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.decode([4350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctu.layername_uskg(mt_uskg.model, 'encoder', 0, 'self_attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nested_json_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[0.4967141530112327, -0.13826430117118466],\n",
       "   [0.6476885381006925, 1.5230298564080254]],\n",
       "  [[-0.23415337472333597, -0.23413695694918055],\n",
       "   [1.5792128155073915, 0.7674347291529088]]],\n",
       " [[[-0.4694743859349521, 0.5425600435859647],\n",
       "   [-0.46341769281246226, -0.46572975357025687]],\n",
       "  [[0.24196227156603412, -1.913280244657798],\n",
       "   [-1.7249178325130328, -0.5622875292409727]]]]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(2,2,2,2).tolist()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctu.nested_list_processing(a, func=lambda x: np.format_float_positional(x, precision=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [[[[0.4967141530112327, -0.13826430117118466],\n",
       "    [0.6476885381006925, 1.5230298564080254]],\n",
       "   [[-0.23415337472333597, -0.23413695694918055],\n",
       "    [1.5792128155073915, 0.7674347291529088]]],\n",
       "  [[[-0.4694743859349521, 0.5425600435859647],\n",
       "    [-0.46341769281246226, -0.46572975357025687]],\n",
       "   [[0.24196227156603412, -1.913280244657798],\n",
       "    [-1.7249178325130328, -0.5622875292409727]]]],\n",
       " 'a_list': {'a0': [[[0.4967141530112327, -0.13826430117118466],\n",
       "    [0.6476885381006925, 1.5230298564080254]],\n",
       "   [[-0.23415337472333597, -0.23413695694918055],\n",
       "    [1.5792128155073915, 0.7674347291529088]]],\n",
       "  'a1_list': {'a10': [[-0.4694743859349521, 0.5425600435859647],\n",
       "    [-0.46341769281246226, -0.46572975357025687]],\n",
       "   'a11': [[0.24196227156603412, -1.913280244657798],\n",
       "    [-1.7249178325130328, -0.5622875292409727]]}}}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = {\n",
    "    'a': a,\n",
    "    'a_list': {\n",
    "        'a0': a[0],\n",
    "        'a1_list': {\n",
    "            'a10': a[1][0],\n",
    "            'a11': a[1][1],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [[[['0.5', '-0.14'], ['0.65', '1.52']],\n",
       "   [['-0.23', '-0.23'], ['1.58', '0.77']]],\n",
       "  [[['-0.47', '0.54'], ['-0.46', '-0.47']],\n",
       "   [['0.24', '-1.91'], ['-1.72', '-0.56']]]],\n",
       " 'a_list': {'a0': [[['0.5', '-0.14'], ['0.65', '1.52']],\n",
       "   [['-0.23', '-0.23'], ['1.58', '0.77']]],\n",
       "  'a1_list': {'a10': [['-0.47', '0.54'], ['-0.46', '-0.47']],\n",
       "   'a11': [['0.24', '-1.91'], ['-1.72', '-0.56']]}}}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.nested_json_processing(b, func=lambda x: np.format_float_positional(x, precision=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = processed_spider_dev[97]\n",
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "dec_prompt = \"select t1.model from\"\n",
    "expect = \"car_names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    enc_sentences=[enc_sentence]*11,\n",
    "    dec_prompts=[dec_prompt]*11,\n",
    "    answer=expect\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', '_', 'name', 's']"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_toks = decode_tokens(mt_uskg.tokenizer, mt_uskg.tokenizer.encode(expect, add_special_tokens=False))\n",
    "ans_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> select t1.model from car_name'"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.decode(inp['decoder_input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['select', '', 't', '1.', 'model', 'from'], 6)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.tokenize(dec_prompt), len(mt_uskg.tokenizer.tokenize(dec_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = ctu.trace_with_patch_uskg_multi_token(\n",
    "#     mt_uskg.model,\n",
    "#     inp=inp,\n",
    "# #     states_to_patch=[(4, ctu.layername_uskg(mt_uskg.model, 'decoder', 3))],\n",
    "#     states_to_patch=[],\n",
    "# #     answers_t=mt_uskg.tokenizer.encode(expect, add_special_tokens=False),\n",
    "# #     tokens_to_mix=(0, len(inp['input_ids'][0])-1),\n",
    "#     tokens_to_mix=None,\n",
    "#     replace=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.5643, device='cuda:0'),\n",
       " tensor(1.0000, device='cuda:0'),\n",
       " tensor(0.9996, device='cuda:0'),\n",
       " tensor(1.0000, device='cuda:0')]"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 443,  834, 4350,    7], device='cuda:0')"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_len = len(mt_uskg.tokenizer.tokenize(expect))\n",
    "pred_out = ctu.predict_from_input_uskg_multi_token(mt_uskg.model, inp, pred_len=answer_len)\n",
    "pred, p = pred_out\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model', 'car'), ('_', '_'), ('name', 'name'), ('s', 's')]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_toks = decode_tokens(mt_uskg.tokenizer, pred[0])\n",
    "# list(zip(pred_toks, ans_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('car_1',\n",
       " 'Find the model of the car whose weight is below the average weight.',\n",
       " '| car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " 'select t1.model from car_names as t1 join cars_data as t2 on t1.makeid = t2.id where t2.weight < (select avg(weight) from cars_data)')"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['db_id'], ex['question'], ex['struct_in'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=[(4, ctu.layername_uskg(mt_uskg.model, 'decoder', 3))],\n",
    "    states_to_unpatch=[(4, ctu.layername_uskg(mt_uskg.model, 'decoder', 4, 'cross_attn'))],\n",
    "    states_to_patch_1st_pass=[(4, ctu.layername_uskg(mt_uskg.model, 'decoder', l)) for l in range(mt_uskg.num_enc_layers)],\n",
    "    answers_t=pred[0],\n",
    "    tokens_to_mix=(10, 20, 30, 40),\n",
    "    tokens_to_mix_1st_pass=(5, 15, 25),\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.5877, device='cuda:0'),\n",
       " tensor(1., device='cuda:0'),\n",
       " tensor(0.9988, device='cuda:0'),\n",
       " tensor(1.0000, device='cuda:0')]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inp = ctu.make_inputs_t5(mt_uskg.tokenizer, [text_in] * 11, [dec_prompt] * 11)\n",
    "# inp = ctu.make_inputs_t5(mt_uskg.tokenizer, [enc_sentence] * 11, [dec_prompt] * 11, answer=expect, device='cpu')\n",
    "# # answer_t, base_score = [d[0] for d in ctu.predict_from_input_uskg(mt_uskg.model, inp)]\n",
    "# # base_score = base_score.item()\n",
    "# # [answer] = ctu.decode_tokens(mt_uskg.tokenizer, [answer_t])\n",
    "# answer_len = 1\n",
    "# if expect is not None:\n",
    "#     answer_len = len(mt_uskg.tokenizer.tokenize(expect))\n",
    "# with torch.no_grad():\n",
    "#     answers_t, base_score = [d[0] for d in ctu.predict_from_input_uskg_multi_token(mt_uskg.model, inp, pred_len=answer_len)]\n",
    "# # base_score = base_score.min().item()\n",
    "# # [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
    "# answer = ctu.decode_sentences(mt_uskg.tokenizer, answers_t)\n",
    "\n",
    "# expect, answers_t, answer, base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # e_range = (129, 132)\n",
    "# # e_range = [8, 13, 131]\n",
    "# e_range = list(range(7, 14))\n",
    "\n",
    "# r = ctu.trace_with_patch_uskg(\n",
    "#     mt_uskg.model,\n",
    "#     inp=inp,\n",
    "#     states_to_patch=[], \n",
    "#     answers_t=answer_t, \n",
    "#     tokens_to_mix=e_range,\n",
    "#     tokens_to_mix_individual_indices=True,\n",
    "#     replace=True,\n",
    "# )\n",
    "\n",
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = mt_uskg.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['mar', 'y', ':', 'has', '', 'a', 'little', 'lamb', 'b', 'b'],\n",
       " ['mar', 'y', ':', 'has', '', 'a', 'little', 'lamb', 'b', 'b'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"mary: has a little  lambbb\"\n",
    "s_ = \"mary: has a little lambbb\"\n",
    "tokenizer.tokenize(s), tokenizer.tokenize(s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3157, 63, 10, 65, 3, 9, 385, 17871, 115, 115, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(s)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'mar'),\n",
       " (1, 'y'),\n",
       " (2, ':'),\n",
       " (3, 'has'),\n",
       " (4, ''),\n",
       " (5, 'a'),\n",
       " (6, 'little'),\n",
       " (7, 'lamb'),\n",
       " (8, 'b'),\n",
       " (9, 'b'),\n",
       " (10, '</s>')]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(t.tokens()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenSpan(start=7, end=10)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_to_tokens(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lambbb'"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.decode_sentences(tokenizer, t['input_ids'][7:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"(a(a)a)\".rindex(\")\"), \"(a(a)a)\".index(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = \"\"\"| concert_singer | singer : singer_id , name ( First Last ) , country ( France , Germany , United States ) , \\\n",
    "song_name , song_release_year , age , is_male\"\"\"\n",
    "\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test struct_in parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "_struct_in = \"\"\"| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , \\\n",
    "average | singer : singer_id , name ( First Last ) , country ( France , Germany , United States ) , \\\n",
    "song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , \\\n",
    "stadium_id , year ( 2008 , 2012 , 2022 ) | singer_in_concert : concert_id , singer_id\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 'stadium'),\n",
       "  [[(5, 'stadium_id'), []],\n",
       "   [(7, 'location'), []],\n",
       "   [(9, 'name'), []],\n",
       "   [(11, 'capacity'), []],\n",
       "   [(13, 'highest'), []],\n",
       "   [(15, 'lowest'), []],\n",
       "   [(17, 'average'), []]]),\n",
       " ((19, 'singer'),\n",
       "  [[(21, 'singer_id'), []],\n",
       "   [(23, 'name'), [(25, 'First Last')]],\n",
       "   [(29, 'country'), [(31, 'France'), (33, 'Germany'), (35, 'United States')]],\n",
       "   [(39, 'song_name'), []],\n",
       "   [(41, 'song_release_year'), []],\n",
       "   [(43, 'age'), []],\n",
       "   [(45, 'is_male'), []]]),\n",
       " ((47, 'concert'),\n",
       "  [[(49, 'concert_id'), []],\n",
       "   [(51, 'concert_name'), []],\n",
       "   [(53, 'theme'), []],\n",
       "   [(55, 'stadium_id'), []],\n",
       "   [(57, 'year'), [(59, '2008'), (61, '2012'), (63, '2022')]]]),\n",
       " ((66, 'singer_in_concert'),\n",
       "  [[(68, 'concert_id'), []], [(70, 'singer_id'), []]])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.parse_struct_in(_struct_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "_text_in = text_in\n",
    "\n",
    "enc_sentence = f\"{_text_in}; structed knowledge: {_struct_in}\"\n",
    "enc_tokenized = mt_uskg.tokenizer(enc_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ranges_dict = ctu.find_struct_name_ranges(mt_uskg.tokenizer, enc_tokenized['input_ids'], _struct_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_ranges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d_key, d in token_ranges_dict.items():\n",
    "    for name, ranges in d.items():\n",
    "        for s, e in ranges:\n",
    "            recs_name = ctu.decode_sentences(mt_uskg.tokenizer, enc_tokenized['input_ids'][s:e])\n",
    "            print(f'{d_key}\\t{name}\\t{recs_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: cars \tlemma: car\n",
      "word: data \tlemma: datum\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('cars data')\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"id\": 1,\n",
       "  \"text\": \"cars\",\n",
       "  \"lemma\": \"car\",\n",
       "  \"upos\": \"NOUN\",\n",
       "  \"xpos\": \"NNS\",\n",
       "  \"feats\": \"Number=Plur\",\n",
       "  \"start_char\": 0,\n",
       "  \"end_char\": 4\n",
       "}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('cars data')\n",
    "doc.sentences[0].words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer-fast / uskg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = processed_spider_dev[416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  1738,     3,     9,   208,   122,   599,  5525,   834,   858,\n",
       "           834, 26416,    61,    45,  7071,   213,   539,   834,  1201, 32100,\n",
       "          2464,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: add special token (<, <=) to t5-tokenizer \n",
    "# could use uskg tokenizer, but it's not TokenizerFast \n",
    "\n",
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "txt = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "\n",
    "tokenized_txt = mt_uskg.tokenizer([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "\n",
    "device = mt_uskg.model.device\n",
    "_output = mt_uskg.model.generate(\n",
    "    torch.tensor(tokenized_txt.data['input_ids'], dtype=int, device=device),\n",
    "    torch.tensor(tokenized_txt.data['attention_mask'], dtype=int, device=device),\n",
    "    num_beams=1, \n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select avg(num_of_staff) from museum where open_year 2009']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.batch_decode(\n",
    "    _output,\n",
    "    skip_special_tokens=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select avg(num_of_staff) from museum where open_year  < 2009']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer_uskg.batch_decode(\n",
    "    _output,\n",
    "    skip_special_tokens=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 't', '3.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.tokenize('t3.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input for chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = processed_spider_dev[503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many battles did not lose any ship with tonnage '225'?; structed knowledge: | battle_death | battle : id , name , date , bulgarian_commander , latin_commander , result | ship : lost_in_battle , id , name , tonnage , ship_type , location , disposition_of_ship | death : caused_by_ship_id , id , note , killed , injured => select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage = '225' );\n"
     ]
    }
   ],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "\n",
    "print(f\"{enc_sentence} => {ex['seq_out']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-time patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For exp2: ['trace_results'][.]: ['%col%_score'] -> ['%node%_score']\n",
    "\n",
    "# res_dir = '/home/yshao/Projects/rome/results/exp2_text_struct_interaction/'\n",
    "# in_path = os.path.join(res_dir, 'exp=2_train_column-tmp.jsonl')\n",
    "# out_path = os.path.join(res_dir, 'exp=2_train_column.jsonl')\n",
    "\n",
    "# with open(in_path, 'r') as f:\n",
    "#     all_results = [json.loads(l) for l in f]\n",
    "\n",
    "# for ex_d in all_results:\n",
    "#     for d in ex_d['trace_results']:\n",
    "#         if not d['is_good_sample']:\n",
    "#             continue\n",
    "#         d['r_node_score'] = d['r_col_score']\n",
    "#         d['r_struct_no_node_score'] = d['r_struct_no_col_score']\n",
    "#         d['r_node_corrupt_all_score'] = d['r_col_corrupt_all_score']\n",
    "#         del d['r_col_score']\n",
    "#         del d['r_struct_no_col_score']\n",
    "#         del d['r_col_corrupt_all_score']\n",
    "\n",
    "# all_results.sort(key=lambda d: d['ex_id'])\n",
    "        \n",
    "# with open(out_path, 'w') as f:\n",
    "#     for d in all_results:\n",
    "#         f.write(json.dumps(d, indent=None) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RE test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_target = ' a b a c aa da a '\n",
    "subject = 'bb'\n",
    "m = re.search(fr'\\W({subject})\\W', dec_target)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<callable_iterator at 0x7f9d8fe45b20>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = re.finditer(fr'\\W({subject})\\W', dec_target)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = list(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exp4 plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uskg_enc_attention(d, savepdf=None):\n",
    "    ## Assume 16 heads, 24 layers (T5 large config)\n",
    "    \n",
    "    ## encoder self attention \n",
    "    inspect_layers = [0, 6, 12, 18, 23]\n",
    "    att_dict = d['attentions']\n",
    "    \n",
    "    cand_len = len(att_dict['enc_cand_tokens'])\n",
    "    head_len = len(att_dict['enc_head_tokens'])\n",
    "\n",
    "    fig_w = 22\n",
    "    fig_h = (0.11*cand_len + 1) * head_len\n",
    "    fig, ax_list = plt.subplots(\n",
    "        nrows=head_len,\n",
    "        ncols=len(inspect_layers),\n",
    "        squeeze=False,\n",
    "        figsize=(fig_w, fig_h))\n",
    "\n",
    "    att_mat = ctu.nested_list_processing(att_dict['enc_attn'], func=float)\n",
    "    att_mat = np.array(att_mat)\n",
    "    \n",
    "    for expect_i in range(len(att_dict['enc_head_tokens'])):\n",
    "        for l_id, layer in enumerate(inspect_layers):\n",
    "            val_mat = att_mat[layer, :, expect_i, :]  # layer, all heads, expect tok i -> all toks \n",
    "            val_mat = val_mat.transpose()    # (cand_toks, n_heads)\n",
    "            x_labels = range(val_mat.shape[1])\n",
    "            y_labels = att_dict['enc_cand_tokens']\n",
    "            title_toks = att_dict['enc_head_tokens'][:expect_i] + [f\"*{att_dict['enc_head_tokens'][expect_i]}*\"]\n",
    "            title = f\"L{layer}  Head token: {' '.join(title_toks)}\\n\"\n",
    "            \n",
    "            ax = ax_list[expect_i, l_id]\n",
    "            _draw_single_plot_2(ax,\n",
    "                                val_mat=val_mat, \n",
    "                                x_labels=x_labels, \n",
    "                                y_labels=y_labels,\n",
    "                                title=title)\n",
    "            \n",
    "    fig.tight_layout()\n",
    "    if savepdf:\n",
    "        plt.savefig(savepdf, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uskg_cross_attention(d, savepdf=None):\n",
    "    ## Assume 16 heads, 24 layers (T5 large config)\n",
    "    \n",
    "    ## decoder cross attention \n",
    "    inspect_layers = [0, 6, 12, 18, 23]\n",
    "    att_dict = d['attentions']\n",
    "    cand_len = len(att_dict['cross_cand_tokens'])\n",
    "    head_len = len(att_dict['cross_head_tokens'])\n",
    "    prompt_len = len(att_dict['dec_cand_tokens'])\n",
    "\n",
    "    fig_w = 22\n",
    "    fig_h = (0.11*cand_len + 1) * head_len\n",
    "    fig, ax_list = plt.subplots(\n",
    "        nrows=head_len,\n",
    "        ncols=len(inspect_layers),\n",
    "        squeeze=False,\n",
    "        figsize=(fig_w, fig_h))\n",
    "\n",
    "    att_mat = ctu.nested_list_processing(att_dict['cross_attn'], func=float)\n",
    "    att_mat = np.array(att_mat)\n",
    "    \n",
    "    for expect_i in range(head_len):\n",
    "        for l_id, layer in enumerate(inspect_layers):\n",
    "            val_mat = att_mat[layer, :, expect_i, :]  # layer, all heads, expect tok i -> all toks \n",
    "            val_mat = val_mat.transpose()    # (cand_toks, n_heads)\n",
    "            x_labels = range(val_mat.shape[1])\n",
    "            y_labels = att_dict['cross_cand_tokens']\n",
    "            # a small hack to use gold tokens from dec_prompt (dec_cand_tokens) for previous steps and predicted tokens at this step \n",
    "            # dec_prompt ends with the first (head_len-1) tokens of the target node \n",
    "            title_toks = att_dict['dec_cand_tokens'][prompt_len - (head_len-1) : prompt_len - (head_len-1) + expect_i] + [f\"*{att_dict['cross_head_tokens'][expect_i]}*\"]\n",
    "            title = f\"L{layer}  Head token: {' '.join(title_toks)}\\n\"\n",
    "            \n",
    "            ax = ax_list[expect_i, l_id]\n",
    "            _draw_single_plot_2(ax,\n",
    "                                val_mat=val_mat, \n",
    "                                x_labels=x_labels, \n",
    "                                y_labels=y_labels,\n",
    "                                title=title)\n",
    "            \n",
    "    fig.tight_layout()\n",
    "    if savepdf:\n",
    "        plt.savefig(savepdf, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d['attentions']['dec_cand_tokens'][-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepdf_path = '/home/yshao/Projects/rome/results/figs/exp4_inspect_attention/tmp-6-cross.pdf'\n",
    "plot_uskg_cross_attention(d, savepdf=savepdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uskg_dec_attention(d, savepdf=None):\n",
    "    ## Assume 16 heads, 24 layers (T5 large config)\n",
    "    \n",
    "    ## decoder self attention \n",
    "    inspect_layers = [0, 6, 12, 18, 23]\n",
    "    att_dict = d['attentions']\n",
    "    prompt_len = cand_len = len(att_dict['dec_cand_tokens'])\n",
    "    head_len = len(att_dict['dec_head_tokens'])\n",
    "\n",
    "    fig_w = 22\n",
    "    fig_h = (0.11*cand_len + 1) * head_len\n",
    "    fig, ax_list = plt.subplots(\n",
    "        nrows=head_len,\n",
    "        ncols=len(inspect_layers),\n",
    "        squeeze=False,\n",
    "        figsize=(fig_w, fig_h))\n",
    "\n",
    "    att_mat = ctu.nested_list_processing(att_dict['dec_attn'], func=float)\n",
    "    att_mat = np.array(att_mat)\n",
    "    \n",
    "    for expect_i in range(head_len):\n",
    "        for l_id, layer in enumerate(inspect_layers):\n",
    "            val_mat = att_mat[layer, :, expect_i, :]  # layer, all heads, expect tok i -> all toks \n",
    "            val_mat = val_mat.transpose()    # (cand_toks, n_heads)\n",
    "            x_labels = range(val_mat.shape[1])\n",
    "            y_labels = att_dict['dec_cand_tokens']\n",
    "            # a small hack to use gold tokens from dec_prompt (dec_cand_tokens) for previous steps and predicted tokens at this step \n",
    "            # dec_prompt ends with the first (head_len-1) tokens of the target node \n",
    "            title_toks = att_dict['dec_cand_tokens'][prompt_len - (head_len-1) : prompt_len - (head_len-1) + expect_i] + [f\"*{att_dict['dec_head_tokens'][expect_i]}*\"]\n",
    "            title = f\"L{layer}  Head token: {' '.join(title_toks)}\\n\"\n",
    "            \n",
    "            ax = ax_list[expect_i, l_id]\n",
    "            _draw_single_plot_2(ax,\n",
    "                                val_mat=val_mat, \n",
    "                                x_labels=x_labels, \n",
    "                                y_labels=y_labels,\n",
    "                                title=title)\n",
    "            \n",
    "    fig.tight_layout()\n",
    "    if savepdf:\n",
    "        plt.savefig(savepdf, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepdf_path = '/home/yshao/Projects/rome/results/figs/exp4_inspect_attention/tmp-6-dec.pdf'\n",
    "plot_uskg_dec_attention(d, savepdf=savepdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 16, 1, 141)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(d['attentions']['enc_attn']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 141)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_att_mat = ctu.nested_list_processing(d['attentions']['enc_attn'], float)\n",
    "_att_mat = np.array(_att_mat)[-1, :, 0, :]  # layer 0, all heads, expect tok 0 -> all toks \n",
    "\n",
    "_att_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_len = len(d['attentions']['enc_tgt_tokens'])\n",
    "fig_h = cand_len / 10\n",
    "\n",
    "fig = plt.figure(figsize=(4, fig_h))\n",
    "ax = fig.gca()\n",
    "\n",
    "val_mat = _att_mat.transpose()\n",
    "x_labels = range(val_mat.shape[1])\n",
    "y_labels = d['attentions']['enc_tgt_tokens']\n",
    "title_toks = d['attentions']['enc_src_tokens'][:-1] + [f\"*{d['attentions']['enc_src_tokens'][-1]}*\"]\n",
    "title = ' '.join(title_toks)\n",
    "\n",
    "_draw_single_plot_2(ax,\n",
    "                    val_mat=val_mat, \n",
    "                    x_labels=x_labels, \n",
    "                    y_labels=y_labels,\n",
    "                    title=title,\n",
    "                   )\n",
    "fig.tight_layout()\n",
    "plt.savefig('/home/yshao/Projects/rome/results/figs/exp4_inspect_attention/tmp-1.pdf',\n",
    "            bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_list = plt.subplots(nrows=4, ncols=2)\n",
    "ax_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exp5.2 attention trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(2,3,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "_m = torch.zeros_like(t).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False]],\n",
       "\n",
       "         [[ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True]],\n",
       "\n",
       "         [[ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True]]],\n",
       "\n",
       "\n",
       "        [[[False, False],\n",
       "          [False, False],\n",
       "          [False, False],\n",
       "          [False, False]],\n",
       "\n",
       "         [[ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True]],\n",
       "\n",
       "         [[ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True],\n",
       "          [ True,  True]]]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_m[:, 1:, None, :] = True\n",
    "_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "## K/V: (bs, seq_len, m_dim) --[k/v]--> (bs, seq_len, n_head * dim) --[shape]--> (bs, n_head, seq_len, dim)\n",
    "## can corrupt v output on seq_len dimensions with certain seq_indices, giving attention ignoring these indices \n",
    "## can then restore the attention final output for other_seq_indices; no\n",
    "\n",
    "replace=True  # True to replace with instead of add noise\n",
    "noise_fn = lambda x: 0 * x\n",
    "\n",
    "patch_spec = []\n",
    "unpatch_spec = []\n",
    "corrupt_spec = []\n",
    "\n",
    "toks_to_mix = []\n",
    "\n",
    "def patch_rep(x, layer):\n",
    "\n",
    "    # if first_pass or (layer not in patch_spec and layer not in unpatch_spec):\n",
    "    if (layer not in patch_spec) and (layer not in unpatch_spec) and (layer not in corrupt_spec):\n",
    "        return x\n",
    "\n",
    "    h = untuple(x)\n",
    "    if layer in corrupt_spec:\n",
    "        toks_to_mix = corrupt_spec[layer]\n",
    "        if toks_to_mix:\n",
    "            mix_len = len(toks_to_mix)\n",
    "\n",
    "            noise_data = noise_fn(\n",
    "                torch.from_numpy(prng(h.shape[0] - 1, mix_len, h.shape[2]))\n",
    "            ).to(device=h.device, dtype=h.dtype)\n",
    "\n",
    "            if replace:\n",
    "                h[1:, toks_to_mix] = noise_data\n",
    "            else:\n",
    "                h[1:, toks_to_mix] += noise_data\n",
    "\n",
    "    # If this layer is in the patch_spec, restore the uncorrupted hidden state\n",
    "    # for selected tokens.\n",
    "    toks_to_patch = patch_spec.get(layer, [])\n",
    "    toks_to_unpatch = unpatch_spec.get(layer, [])\n",
    "    # if toks_to_patch:\n",
    "    #     print(f'* 2nd pass, layer: {layer}, restoring: {toks_to_patch}')\n",
    "    # if toks_to_unpatch:\n",
    "    #     print(f'* 2nd pass, layer: {layer}, unpatching: {toks_to_unpatch}')\n",
    "\n",
    "    for t in toks_to_patch:\n",
    "        h[1:, t] = h[0, t]\n",
    "    for t in toks_to_unpatch:\n",
    "        NotImplemented\n",
    "        # h[1:, t] = untuple(first_pass_trace[layer].output)[1:, t]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "706px",
    "left": "30px",
    "top": "220px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

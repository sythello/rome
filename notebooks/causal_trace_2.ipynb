{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing\n",
    "\n",
    "A demonstration of the double-intervention causal tracing method.\n",
    "\n",
    "The strategy used by causal tracing is to understand important\n",
    "states within a transfomer by doing two interventions simultaneously:\n",
    "\n",
    "1. Corrupt a subset of the input.  In our paper, we corrupt the subject tokens\n",
    "   to frustrate the ability of the transformer to accurately complete factual\n",
    "   prompts about the subject.\n",
    "2. Restore a subset of the internal hidden states.  In our paper, we scan\n",
    "   hidden states at all layers and all tokens, searching for individual states\n",
    "   that carry the necessary information for the transformer to recover its\n",
    "   capability to complete the factual prompt.\n",
    "\n",
    "The traces of decisive states can be shown on a heatmap.  This notebook\n",
    "demonstrates the code for conducting causal traces and creating these heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `experiments.causal_trace` module contains a set of functions for running causal traces.\n",
    "\n",
    "In this notebook, we reproduce, demonstrate and discuss the interesting functions.\n",
    "\n",
    "We begin by importing several utility functions that deal with tokens and transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json\n",
    "import string\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from util import nethook\n",
    "from util.globals import DATA_DIR\n",
    "from experiments.causal_trace import (\n",
    "    ModelAndTokenizer,\n",
    "    layername,\n",
    "    guess_subject,\n",
    "    plot_trace_heatmap,\n",
    ")\n",
    "from experiments.causal_trace import (\n",
    "    make_inputs,\n",
    "    decode_tokens,\n",
    "    find_token_range,\n",
    "    predict_token,\n",
    "    predict_from_input,\n",
    "    collect_embedding_std,\n",
    ")\n",
    "from dsets import KnownsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f9f6c5c94f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# from uskg.models.unified.prefixtuning import Model\n",
    "from uskg.models.unified import finetune, prefixtuning\n",
    "from uskg.utils.configue import Configure\n",
    "from uskg.utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "from uskg.seq2seq_construction import spider as s2s_spider\n",
    "from uskg.third_party.spider.preprocess.get_tables import dump_db_json_schema\n",
    "from uskg.third_party.spider import evaluation as sp_eval\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# import stanza\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "\n",
    "from experiments import causal_trace_uskg as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer_uskg: hkunlp/from_all_T5_large_prefix_spider_with_cell_value2\n",
      "Using tokenizer_fast: t5-large\n",
      "prefix-tuning sequence length is 10.\n"
     ]
    }
   ],
   "source": [
    "mt_uskg = ctu.ModelAndTokenizer_USKG('t5-large-prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('constructor', 'seq2seq_construction.spider'),\n",
       " ('schema_serialization_with_db_content', True),\n",
       " ('target_with_db_id', False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mt_uskg.task_args.seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.model.pretrain_model.encoder.embed_tokens is mt_uskg.model.pretrain_model.shared, \\\n",
    "mt_uskg.model.pretrain_model.decoder.embed_tokens is mt_uskg.model.pretrain_model.shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [k for k,v in mt_uskg.model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    enc_sentences=[\"Translate to German: My name is Wolfgang and I live in Berlin\"],\n",
    "    dec_prompts=[\"Mein Name ist Wolfgang\"],\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ctu.run_model_forward_uskg(mt_uskg.model, **inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state']),\n",
       " torch.Size([1, 5, 32102]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys(), out['logits'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32102,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = out[\"logits\"][0, -1].detach().cpu().numpy()\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, -1.8642352),\n",
       " (6, -9.727753),\n",
       " (5, -10.966707),\n",
       " (27, -11.037394),\n",
       " (213, -12.864212)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5 = sorted(list(enumerate(logits)), key=lambda p: -p[1])[:5]\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', ',', '.', 'I', 'where']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mt_uskg.tokenizer.decode([p[0]]) for p in top_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spider dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_train_path = '/home/yshao/Projects/SDR-analysis/data/spider/train+ratsql_graph.json'\n",
    "spider_dev_path = '/home/yshao/Projects/SDR-analysis/data/spider/dev+ratsql_graph.json'\n",
    "spider_db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_spider_dev = ctu.load_raw_dataset(\n",
    "    data_filepath = spider_dev_path,\n",
    "    db_path=spider_db_dir,\n",
    "#     schema_cache=SCHEMA_CACHE\n",
    ")\n",
    "len(raw_spider_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_spider_dev[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.task_args.dataset.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_spider_dev = s2s_spider.DevDataset(\n",
    "    args=mt_uskg.task_args,\n",
    "    raw_datasets=raw_spider_dev,\n",
    "    cache_root='../cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the average, minimum, and maximum age of all singers from France?',\n",
       " '| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country ( France ) , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id',\n",
       " \"select avg(age), min(age), max(age) from singer where country = 'France'\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_spider_dev[4]['text_in'], \\\n",
    "processed_spider_dev[4]['struct_in'], \\\n",
    "processed_spider_dev[4]['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _occ_punct = set()\n",
    "\n",
    "for _id in range(len(processed_spider_dev)):\n",
    "    ex = processed_spider_dev[_id]\n",
    "#     _occ_punct.update(set(string.punctuation) & set(ex['seq_out']))\n",
    "    if '_(' in ex['struct_in']:\n",
    "        print(_id, ex['question'])\n",
    "        print(ex['struct_in'])\n",
    "        print(ex['seq_out'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train set\n",
    "\n",
    "raw_spider_train = ctu.load_raw_dataset(\n",
    "    data_filepath = spider_train_path,\n",
    "    db_path=spider_db_dir,\n",
    ")\n",
    "processed_spider_train = s2s_spider.TrainDataset(\n",
    "    args=mt_uskg.task_args,\n",
    "    raw_datasets=raw_spider_train,\n",
    "    cache_root='../cache')\n",
    "len(processed_spider_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"SELECT T2.name FROM membership_register_branch AS T1 JOIN branch AS T2 ON T1.branch_id  =  T2.branch_id JOIN member AS T3 ON T1.member_id  =  T3.member_id WHERE T3.Hometown  =  'Louisville ,  Kentucky' INTERSECT SELECT T2.name FROM membership_register_branch AS T1 JOIN branch AS T2 ON T1.branch_id  =  T2.branch_id JOIN member AS T3 ON T1.member_id  =  T3.member_id WHERE T3.Hometown  =  'Hiram ,  Georgia'\",\n",
       " 'question': 'What are the names of the branches that have some members with a hometown in Louisville, Kentucky and also those from Hiram, Goergia?',\n",
       " 'db_id': 'shop_membership',\n",
       " 'db_path': '/home/yshao/Projects/language/language/xsp/data/spider/database',\n",
       " 'db_table_names': ['member',\n",
       "  'branch',\n",
       "  'membership_register_branch',\n",
       "  'purchase'],\n",
       " 'db_column_names': {'table_id': [-1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3],\n",
       "  'column_name': ['*',\n",
       "   'Member_ID',\n",
       "   'Card_Number',\n",
       "   'Name',\n",
       "   'Hometown',\n",
       "   'Level',\n",
       "   'Branch_ID',\n",
       "   'Name',\n",
       "   'Open_year',\n",
       "   'Address_road',\n",
       "   'City',\n",
       "   'membership_amount',\n",
       "   'Member_ID',\n",
       "   'Branch_ID',\n",
       "   'Register_Year',\n",
       "   'Member_ID',\n",
       "   'Branch_ID',\n",
       "   'Year',\n",
       "   'Total_pounds']},\n",
       " 'db_column_types': ['text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number'],\n",
       " 'db_primary_keys': {'column_id': [1, 6, 12, 15]},\n",
       " 'db_foreign_keys': {'column_id': [13, 12, 16, 15],\n",
       "  'other_column_id': [6, 1, 6, 1]},\n",
       " 'rat_sql_graph': {'nodes': ['what',\n",
       "   'be',\n",
       "   'the',\n",
       "   'name',\n",
       "   'of',\n",
       "   'the',\n",
       "   'branch',\n",
       "   'that',\n",
       "   'have',\n",
       "   'some',\n",
       "   'member',\n",
       "   'with',\n",
       "   'a',\n",
       "   'hometown',\n",
       "   'in',\n",
       "   'louisville',\n",
       "   ',',\n",
       "   'kentucky',\n",
       "   'and',\n",
       "   'also',\n",
       "   'those',\n",
       "   'from',\n",
       "   'hiram',\n",
       "   ',',\n",
       "   'goergia',\n",
       "   '?',\n",
       "   '<C>NONE::*',\n",
       "   '<C>member::member_id',\n",
       "   '<C>member::card_number',\n",
       "   '<C>member::name',\n",
       "   '<C>member::hometown',\n",
       "   '<C>member::level',\n",
       "   '<C>branch::branch_id',\n",
       "   '<C>branch::name',\n",
       "   '<C>branch::open_year',\n",
       "   '<C>branch::address_road',\n",
       "   '<C>branch::city',\n",
       "   '<C>branch::membership_amount',\n",
       "   '<C>membership_register_branch::member_id',\n",
       "   '<C>membership_register_branch::branch_id',\n",
       "   '<C>membership_register_branch::register_year',\n",
       "   '<C>purchase::member_id',\n",
       "   '<C>purchase::branch_id',\n",
       "   '<C>purchase::year',\n",
       "   '<C>purchase::total_pound',\n",
       "   '<T>member',\n",
       "   '<T>branch',\n",
       "   '<T>membership_register_branch',\n",
       "   '<T>purchase'],\n",
       "  'q_nodes_orig': ['what',\n",
       "   'are',\n",
       "   'the',\n",
       "   'names',\n",
       "   'of',\n",
       "   'the',\n",
       "   'branches',\n",
       "   'that',\n",
       "   'have',\n",
       "   'some',\n",
       "   'members',\n",
       "   'with',\n",
       "   'a',\n",
       "   'hometown',\n",
       "   'in',\n",
       "   'louisville',\n",
       "   ',',\n",
       "   'kentucky',\n",
       "   'and',\n",
       "   'also',\n",
       "   'those',\n",
       "   'from',\n",
       "   'hiram',\n",
       "   ',',\n",
       "   'goergia',\n",
       "   '?'],\n",
       "  'relations': '[[2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 37, 5, 5, 5, 37, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 41, 5, 5, 5, 5, 5, 5, 41, 5, 5, 41, 5, 5, 6, 39, 6, 6], [0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 41, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 41, 5, 5, 41, 5, 5, 5, 39, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 37, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 49, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 14, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 21, 21, 21, 21], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 42, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 14, 11, 11, 11, 11, 8, 8, 8, 8, 8, 8, 10, 8, 8, 10, 8, 8, 8, 19, 17, 17, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 11, 14, 11, 11, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 20, 17, 17, 17], [7, 7, 7, 38, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 11, 11, 14, 11, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 20, 17, 17, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 38, 7, 7, 7, 50, 7, 7, 7, 7, 7, 7, 7, 7, 8, 11, 11, 11, 14, 11, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 20, 17, 17, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 11, 11, 11, 11, 14, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 20, 17, 17, 17], [7, 7, 7, 7, 7, 7, 42, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 14, 11, 11, 11, 11, 11, 8, 10, 8, 8, 10, 8, 8, 17, 19, 17, 17], [7, 7, 7, 38, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 11, 14, 11, 11, 11, 11, 8, 8, 8, 8, 8, 8, 8, 17, 20, 17, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 11, 11, 14, 11, 11, 11, 8, 8, 8, 8, 8, 8, 8, 17, 20, 17, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 11, 11, 11, 14, 11, 11, 8, 8, 8, 8, 8, 8, 8, 17, 20, 17, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 11, 11, 11, 11, 14, 11, 8, 8, 8, 8, 8, 8, 8, 17, 20, 17, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 11, 11, 11, 11, 11, 14, 8, 8, 8, 8, 8, 8, 8, 17, 20, 17, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 42, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 14, 11, 11, 8, 8, 8, 8, 17, 17, 19, 17], [7, 7, 7, 7, 7, 7, 42, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8, 11, 14, 11, 8, 8, 8, 8, 17, 17, 20, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 11, 11, 14, 8, 8, 8, 8, 17, 17, 20, 17], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 42, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 14, 11, 11, 11, 17, 17, 17, 19], [7, 7, 7, 7, 7, 7, 42, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 9, 8, 8, 8, 8, 8, 8, 8, 8, 11, 14, 11, 11, 17, 17, 17, 20], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 11, 11, 14, 11, 17, 17, 17, 20], [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 11, 11, 11, 14, 17, 17, 17, 20], [22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 40, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 26, 24, 25, 25, 25, 25, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 34, 28, 30, 30], [22, 22, 22, 22, 22, 22, 40, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 26, 23, 23, 23, 23, 23, 24, 25, 25, 25, 25, 25, 23, 23, 23, 23, 23, 23, 23, 28, 34, 30, 30], [22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 26, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 25, 25, 23, 23, 23, 23, 29, 29, 34, 28], [22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 26, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 25, 25, 25, 29, 29, 28, 34]]'},\n",
       " 'serialized_schema': ' | shop_membership | member : member_id , card_number , name , hometown ( Louisville, Kentucky ) , level | branch : branch_id , name , open_year , address_road , city , membership_amount | membership_register_branch : member_id , branch_id , register_year | purchase : member_id , branch_id , year , total_pounds',\n",
       " 'struct_in': '| shop_membership | member : member_id , card_number , name , hometown ( Louisville, Kentucky ) , level | branch : branch_id , name , open_year , address_road , city , membership_amount | membership_register_branch : member_id , branch_id , register_year | purchase : member_id , branch_id , year , total_pounds',\n",
       " 'text_in': 'What are the names of the branches that have some members with a hometown in Louisville, Kentucky and also those from Hiram, Goergia?',\n",
       " 'seq_out': \"select t2.name from membership_register_branch as t1 join branch as t2 on t1.branch_id = t2.branch_id join member as t3 on t1.member_id = t3.member_id where t3.hometown = 'Louisville, Kentucky' intersect select t2.name from membership_register_branch as t1 join branch as t2 on t1.branch_id = t2.branch_id join member as t3 on t1.member_id = t3.member_id where t3.hometown = 'Hiram, Georgia'\"}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_spider_train[5441]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis sample 1 (ID = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many singers do we have?', 'select count(*) from singer')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = processed_spider_dev[0]\n",
    "ex['question'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "dec_prompt = 'select count(*) from'\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    enc_sentences=[enc_sentence],\n",
    "    dec_prompts=[dec_prompt],\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many singers do we have?; structed knowledge: | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id</s>',\n",
       " '<pad> select count(*) from')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.decode(inp['input_ids'][0]), mt_uskg.tokenizer.decode(inp['decoder_input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ctu.run_model_forward_uskg(mt_uskg.model, **inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state']),\n",
       " torch.Size([1, 7, 32102]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys(), out['logits'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32102,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = out[\"logits\"][0, -1].detach().cpu().numpy()\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7634, -5.629301),\n",
       " (6721, -15.1248665),\n",
       " (10159, -17.77869),\n",
       " (2377, -18.263933),\n",
       " (8782, -18.631098)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5 = sorted(list(enumerate(logits)), key=lambda p: -p[1])[:5]\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['singer', 'vocal', 'sing', 'artist', 'singing']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mt_uskg.tokenizer.decode([p[0]]) for p in top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e_range = ctu.find_token_range(mt_uskg.tokenizer, inp[\"input_ids\"][0], 'singer')\n",
    "# e_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 8), (15, 125))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_range, struct_range = ctu.find_text_struct_in_range(mt_uskg.tokenizer, inp[\"input_ids\"][0])\n",
    "text_range, struct_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many singers do we have?; structed knowledge: | concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id</s>',\n",
       " 'How many singers do we have?',\n",
       " '| concert_singer | stadium : stadium_id, location, name, capacity, highest, lowest, average | singer : singer_id, name, country, song_name, song_release_year, age, is_male | concert : concert_id, concert_name, theme, stadium_id, year | singer_in_concert : concert_id, singer_id')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb, te = text_range\n",
    "sb, se = struct_range\n",
    "mt_uskg.tokenizer.decode(inp['input_ids'][0]), \\\n",
    "mt_uskg.tokenizer.decode(inp['input_ids'][0][tb:te]), \\\n",
    "mt_uskg.tokenizer.decode(inp['input_ids'][0][sb:se])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_hidden_flow_uskg(): corrupted input: *How *many *singer *s *do *we *have *? *; * *struct *e *d *knowledge *: *| *concert *_ *s *inger *| *stadium * *: *stadium *_ *i *d * *, *location * *, *name * *, *capacity * *, *highest * *, *lowest * *, *average *| *singer * *: *singer *_ *i *d * *, *name * *, *country * *, *song *_ *name * *, *song *_ *release *_ *year * *, *age * *, *is *_ *male *| *concert * *: *concert *_ *i *d * *, *concert *_ *name * *, *theme * *, *stadium *_ *i *d * *, *year *| *singer *_ *in *_ *conce *r *t * *: *concert *_ *i *d * *, *singer *_ *i *d *</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a6ab59e50d4835b614db67be994427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.encoder:   0%|          | 0/1512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3493e4be9d4a49a9a8120a648d6df427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.decoder:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = ctu.calculate_hidden_flow_uskg(\n",
    "    mt_uskg,\n",
    "    enc_sentence=enc_sentence,\n",
    "    dec_prompt=dec_prompt,\n",
    "    subject='singer',\n",
    "    replace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['scores', 'low_score', 'high_score', 'input_ids', 'input_tokens', 'dec_input_ids', 'dec_input_tokens', 'subject_range', 'answer', 'window', 'correct_prediction', 'kind']),\n",
       " True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys(), result['correct_prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2487089356436627e-06"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['low_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctu.plot_trace_heatmap_t5(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis sample 2 (ID = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What are  the different countries with singers above age 20?',\n",
       " 'select distinct country from singer where age > 20')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = 9\n",
    "ex = processed_spider_dev[_id]\n",
    "ex['question'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "dec_prompt = \"select distinct country from singer where\"\n",
    "expect = \"age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_hidden_flow_uskg(): corrupted input: *What *are *the *different *countries *with *singer *s *above *age *20 *? *; * *struct *e *d *knowledge *: *| *concert *_ *s *inger *| *stadium * *: *stadium *_ *i *d * *, *location * *, *name * *, *capacity * *, *highest * *, *lowest * *, *average *| *singer * *: *singer *_ *i *d * *, *name * *, *country * *, *song *_ *name * *, *song *_ *release *_ *year * *, *age * *, *is *_ *male *| *concert * *: *concert *_ *i *d * *, *concert *_ *name * *, *theme * *, *stadium *_ *i *d * *, *year *| *singer *_ *in *_ *conce *r *t * *: *concert *_ *i *d * *, *singer *_ *i *d *</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac04e3d082042f2a990247ac1f0c95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.encoder:   0%|          | 0/1560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a50b4d88314031b157eb615ed72989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.decoder:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = ctu.calculate_hidden_flow_uskg(\n",
    "    mt_uskg,\n",
    "    enc_sentence=enc_sentence,\n",
    "    dec_prompt=dec_prompt,\n",
    "    subject='singer',\n",
    "    replace=True,\n",
    "    expect=expect,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['scores', 'low_score', 'high_score', 'input_ids', 'input_tokens', 'dec_input_ids', 'dec_input_tokens', 'subject_range', 'answer', 'window', 'correct_prediction', 'kind']),\n",
       " True,\n",
       " tensor(1.0000, device='cuda:0'),\n",
       " 0.0005113474908284843)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys(), result['correct_prediction'], result['high_score'], result['low_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctu.plot_trace_heatmap_t5(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis sample 3 (ID = 97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Find the model of the car whose weight is below the average weight.',\n",
       " 'select t1.model from car_names as t1 join cars_data as t2 on t1.makeid = t2.id where t2.weight < (select avg(weight) from cars_data)')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = 97\n",
    "ex = processed_spider_dev[_id]\n",
    "ex['question'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "dec_prompt = \"select t1.model from car_names as t1 join cars_\"\n",
    "expect = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', -10.943039),\n",
       " ('stat', -21.073414),\n",
       " ('daten', -22.921432),\n",
       " ('re', -23.442766),\n",
       " ('performance', -23.719862)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_in = ex['text_in']\n",
    "# struct_in = ex['struct_in']\n",
    "\n",
    "# enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "# dec_prompt = \"select t1.model from car_names as t1 join cars_\"\n",
    "# expect = \"data\"\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    enc_sentences=[enc_sentence],\n",
    "    dec_prompts=[dec_prompt],\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "out = ctu.run_model_forward_uskg(mt_uskg.model, **inp)\n",
    "logits = out[\"logits\"][0, -1].detach().cpu().numpy()\n",
    "\n",
    "top_5 = sorted(list(enumerate(logits)), key=lambda p: -p[1])[:5]\n",
    "\n",
    "[(mt_uskg.tokenizer.decode([p[0]]), p[1]) for p in top_5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_hidden_flow_uskg(): corrupted input: *Find *the *model *of *the *car * *whose *weight *is *below *the *average *weight *. *; * *struct *e *d *knowledge *: *| *car *_ *1 *| *continent *s * *: *cont *i *d * *, *continent *| *countries * *: *country *i *d * *, *country *name * *, *continent *| *car *_ *makers * *: * *i *d * *, *maker * *, *full *name * *, *country *| *model *_ *list * *: *model *i *d * *, *maker * *, *model *| *car *_ *name *s * *: *make *i *d * *, *model * *, *make *| *cars *_ *data * *: * *i *d * *, * *mp *g * *, * *cylinder *s * *, * *e *disp *l * *, *horsepower * *, *weight * *, *accelerate * *, *year *</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328d97f4b69940a0a2f6058f1a370b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.encoder:   0%|          | 0/1668 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376e361260d747ea9dd41d81a85207ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.decoder:   0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = ctu.calculate_hidden_flow_uskg(\n",
    "    mt_uskg,\n",
    "    enc_sentence=enc_sentence,\n",
    "    dec_prompt=dec_prompt,\n",
    "    subject=None,\n",
    "    replace=True,\n",
    "    expect=expect,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['scores', 'low_score', 'high_score', 'input_ids', 'input_tokens', 'dec_input_ids', 'dec_input_tokens', 'subject_range', 'answer', 'window', 'correct_prediction', 'kind']),\n",
       " True,\n",
       " tensor(0.9999, device='cuda:0'),\n",
       " 0.0008115010568872094)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys(), result['correct_prediction'], result['high_score'], result['low_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctu.plot_trace_heatmap_t5(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "dec_prompt = \"select t1.model from car_names\"\n",
    "expect = \"as\"  # 't1' -> '_', 't', '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_hidden_flow_uskg(): corrupted input: *Find *the *model *of *the *car * *whose *weight *is *below *the *average *weight *. *; * *struct *e *d *knowledge *: *| *car *_ *1 *| *continent *s * *: *cont *i *d * *, *continent *| *countries * *: *country *i *d * *, *country *name * *, *continent *| *car *_ *makers * *: * *i *d * *, *maker * *, *full *name * *, *country *| *model *_ *list * *: *model *i *d * *, *maker * *, *model *| *car *_ *name *s * *: *make *i *d * *, *model * *, *make *| *cars *_ *data * *: * *i *d * *, * *mp *g * *, * *cylinder *s * *, * *e *disp *l * *, *horsepower * *, *weight * *, *accelerate * *, *year *</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa85cbc173a4f46b93d0c489d3ddcd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.encoder:   0%|          | 0/1668 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ca6ec568f14fcd80b325cfe0cdc393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trace_important_states_uskg.decoder:   0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = ctu.calculate_hidden_flow_uskg(\n",
    "    mt_uskg,\n",
    "    enc_sentence=enc_sentence,\n",
    "    dec_prompt=dec_prompt,\n",
    "    subject=None,\n",
    "    replace=True,\n",
    "    expect=expect,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['scores', 'low_score', 'high_score', 'input_ids', 'input_tokens', 'dec_input_ids', 'dec_input_tokens', 'subject_range', 'answer', 'window', 'correct_prediction', 'kind']),\n",
       " True,\n",
       " tensor(1.0000, device='cuda:0'),\n",
       " 0.9942196011543274)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys(), result['correct_prediction'], result['high_score'], result['low_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctu.plot_trace_heatmap_t5(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = '/home/yshao/Projects/language/language/xsp/data/spider/tables.json'\n",
    "db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmaps = sp_eval.build_foreign_key_map_from_json(table_path)\n",
    "evaluator = sp_eval.Evaluator(db_dir=db_dir, kmaps=kmaps, etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0, 0, 'hard')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "_sql_str = 'select t1.birth_date from people as t1 join poker_player as t2 on t1.people_id = t2.people_id order by t2.earnings asc limit 1'\n",
    "db_name = 'poker_player'\n",
    "schema = evaluator.schemas[db_name]\n",
    "_sql = sp_eval.get_sql(schema, _sql_str)\n",
    "sp_eval.count_component1(_sql), sp_eval.count_component2(_sql), sp_eval.count_others(_sql), \\\n",
    "evaluator.eval_hardness(_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _detect_column_role(dec_prompt):\n",
    "    role_keyword_pattern = r'\\W(select|where|join|group by|having|order by)\\W'\n",
    "    all_kws = re.findall(role_keyword_pattern, ' ' + dec_prompt + ' ')\n",
    "    assert len(all_kws) > 0, dec_prompt\n",
    "    col_role_kw = all_kws[-1]\n",
    "    return col_role_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp0: Study the influence of corrupting a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the average, minimum, and maximum age for all French singers?',\n",
       " \"select avg(age), min(age), max(age) from singer where country = 'France'\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = 5\n",
    "ex = processed_spider_dev[_id]\n",
    "ex['question'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('select avg(age), min(age), max(age) from singer where', 'country')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "\n",
    "# expect = 'singer'\n",
    "# expect = 'age'\n",
    "expect = 'country'\n",
    "dec_prompt = ctu.make_dec_prompt(ex['seq_out'], expect)\n",
    "\n",
    "ex['enc_sentence'] = enc_sentence\n",
    "ex['dec_prompt'] = dec_prompt\n",
    "ex['expect'] = expect\n",
    "dec_prompt, expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['country']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_toks = decode_tokens(mt_uskg.tokenizer, mt_uskg.tokenizer.encode(expect, add_special_tokens=False))\n",
    "ans_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999990463256836, 'country')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence],\n",
    "    [dec_prompt],\n",
    "    answer=expect,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "answer_len = len(mt_uskg.tokenizer.tokenize(expect))\n",
    "with torch.no_grad():\n",
    "    answers_t, base_score = [d[0] for d in ctu.predict_from_input_uskg_multi_token(mt_uskg.model, inp, pred_len=answer_len)]\n",
    "base_score = base_score.min().item()\n",
    "answer = ctu.decode_sentences(mt_uskg.tokenizer, answers_t)\n",
    "base_score, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([684], device='cuda:0')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0bf9cb15134342bb9b3c4d40ebd864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Corrupt effect: columns:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15951d5486094c779eaf34568678d9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Corrupt effect: tables:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_d = ctu.token_corruption_influence_uskg(\n",
    "    mt_uskg,\n",
    "#     enc_sentence=enc_sentence,\n",
    "#     dec_prompt=dec_prompt,\n",
    "#     expect=expect,\n",
    "    ex,\n",
    "    replace=True,\n",
    "    use_tqdm=True,\n",
    "    skips=('token',)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['enc_sentence', 'dec_prompt', 'expect', 'base_score', 'answers_t', 'answer', 'res_list'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'corrpt_type': 'column',\n",
       "  'corrpt_idx': (38, 39),\n",
       "  'corrpt_token': 'location',\n",
       "  'corrpt_score': 0.9999988675117493,\n",
       "  'corrpt_drop': 1.7881393432617188e-07},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (44, 45),\n",
       "  'corrpt_token': 'capacity',\n",
       "  'corrpt_score': 0.9999990463256836,\n",
       "  'corrpt_drop': 0.0},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (47, 48),\n",
       "  'corrpt_token': 'highest',\n",
       "  'corrpt_score': 0.9999989867210388,\n",
       "  'corrpt_drop': 5.960464477539063e-08},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (50, 51),\n",
       "  'corrpt_token': 'lowest',\n",
       "  'corrpt_score': 0.9999990463256836,\n",
       "  'corrpt_drop': 0.0},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (53, 54),\n",
       "  'corrpt_token': 'average',\n",
       "  'corrpt_score': 0.9999991655349731,\n",
       "  'corrpt_drop': -1.1920928955078125e-07},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (67, 68),\n",
       "  'corrpt_token': 'country',\n",
       "  'corrpt_score': 4.299651834571705e-07,\n",
       "  'corrpt_drop': 0.9999986163605001},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (70, 73),\n",
       "  'corrpt_token': 'song_name',\n",
       "  'corrpt_score': 0.999999463558197,\n",
       "  'corrpt_drop': -4.172325134277344e-07},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (75, 80),\n",
       "  'corrpt_token': 'song_release_year',\n",
       "  'corrpt_score': 0.9999995231628418,\n",
       "  'corrpt_drop': -4.76837158203125e-07},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (82, 83),\n",
       "  'corrpt_token': 'age',\n",
       "  'corrpt_score': 0.9999995231628418,\n",
       "  'corrpt_drop': -4.76837158203125e-07},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (85, 88),\n",
       "  'corrpt_token': 'is_male',\n",
       "  'corrpt_score': 0.999999463558197,\n",
       "  'corrpt_drop': -4.172325134277344e-07},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (98, 101),\n",
       "  'corrpt_token': 'concert_name',\n",
       "  'corrpt_score': 0.9999990463256836,\n",
       "  'corrpt_drop': 0.0},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (103, 104),\n",
       "  'corrpt_token': 'theme',\n",
       "  'corrpt_score': 0.9999989867210388,\n",
       "  'corrpt_drop': 5.960464477539063e-08},\n",
       " {'corrpt_type': 'column',\n",
       "  'corrpt_idx': (112, 113),\n",
       "  'corrpt_token': 'year',\n",
       "  'corrpt_score': 0.9999988675117493,\n",
       "  'corrpt_drop': 1.7881393432617188e-07},\n",
       " {'corrpt_type': 'table',\n",
       "  'corrpt_idx': (29, 30),\n",
       "  'corrpt_token': 'stadium',\n",
       "  'corrpt_score': 0.9999991655349731,\n",
       "  'corrpt_drop': -1.1920928955078125e-07},\n",
       " {'corrpt_type': 'table',\n",
       "  'corrpt_idx': (55, 56),\n",
       "  'corrpt_token': 'singer',\n",
       "  'corrpt_score': 0.9999991655349731,\n",
       "  'corrpt_drop': -1.1920928955078125e-07},\n",
       " {'corrpt_type': 'table',\n",
       "  'corrpt_idx': (89, 90),\n",
       "  'corrpt_token': 'concert',\n",
       "  'corrpt_score': 0.9999993443489075,\n",
       "  'corrpt_drop': -2.980232238769531e-07},\n",
       " {'corrpt_type': 'table',\n",
       "  'corrpt_idx': (114, 121),\n",
       "  'corrpt_token': 'singer_in_concert',\n",
       "  'corrpt_score': 0.9999988675117493,\n",
       "  'corrpt_drop': 1.7881393432617188e-07}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = result_d['res_list']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[d for d in result if d['corrpt_type'] != 'token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tHow\t0.0037964582443237305\n",
      "1\tmany\t0.0013628602027893066\n",
      "2\tbattle\t0.01413428783416748\n",
      "3\ts\t-0.0008431673049926758\n",
      "4\tdid\t0.052848875522613525\n",
      "5\tnot\t0.09322965145111084\n",
      "6\tlose\t0.002241969108581543\n",
      "7\tany\t0.056729793548583984\n",
      "8\tship\t-0.0024902820587158203\n",
      "9\twith\t0.04180067777633667\n",
      "10\tto\t0.006940901279449463\n",
      "11\tn\t0.07953697443008423\n",
      "12\tnage\t0.0018101334571838379\n",
      "13\t\t0.0008842945098876953\n",
      "14\t'\t0.0007312297821044922\n",
      "15\t225\t-0.002007007598876953\n",
      "16\t'\t0.000843048095703125\n",
      "17\t?\t0.001425027847290039\n",
      "18\t;\t-0.0008304119110107422\n",
      "19\t\t0.0033646225929260254\n",
      "20\tstruct\t0.005487203598022461\n",
      "21\te\t-0.0010203719139099121\n",
      "22\td\t0.0009642243385314941\n",
      "23\tknowledge\t-0.00048089027404785156\n",
      "24\t:\t-0.00015664100646972656\n",
      "25\t|\t0.0022062063217163086\n",
      "26\tbattle\t0.0007958412170410156\n",
      "27\t_\t0.000949561595916748\n",
      "28\tde\t0.002287924289703369\n",
      "29\ta\t0.0008778572082519531\n",
      "30\tth\t0.0004265308380126953\n",
      "31\t|\t0.0012375712394714355\n",
      "32\tbattle\t0.0064389705657958984\n",
      "33\t\t-0.0003082752227783203\n",
      "34\t:\t-0.0015889406204223633\n",
      "35\t\t-0.002041161060333252\n",
      "36\ti\t-0.002639591693878174\n",
      "37\td\t0.0030516386032104492\n",
      "38\t\t0.0008799433708190918\n",
      "39\t,\t0.001226961612701416\n",
      "40\tname\t0.0022575855255126953\n",
      "41\t\t0.0013141632080078125\n",
      "42\t,\t0.0004660487174987793\n",
      "43\tdate\t0.001199960708618164\n",
      "44\t\t0.0036588311195373535\n",
      "45\t,\t0.002676248550415039\n",
      "46\tbulg\t0.00016325712203979492\n",
      "47\tarian\t0.0017170906066894531\n",
      "48\t_\t0.00043189525604248047\n",
      "49\tcommand\t0.017362475395202637\n",
      "50\ter\t0.004083693027496338\n",
      "51\t\t0.003155231475830078\n",
      "52\t,\t0.0028000473976135254\n",
      "53\t\t0.0020836591720581055\n",
      "54\tlatin\t0.0017630457878112793\n",
      "55\t_\t0.0010582804679870605\n",
      "56\tcommand\t0.006787300109863281\n",
      "57\ter\t0.005815982818603516\n",
      "58\t\t0.005763590335845947\n",
      "59\t,\t0.01128298044204712\n",
      "60\tresult\t0.001990079879760742\n",
      "61\t|\t-0.0022734999656677246\n",
      "62\tship\t-0.0031125545501708984\n",
      "63\t\t-0.002832651138305664\n",
      "64\t:\t0.2955291271209717\n",
      "S*65*\tlost\t0.9968710248273283\n",
      "*66*\t_\t0.5390499830245972\n",
      "*67*\tin\t0.8797281607985497\n",
      "68\t_\t0.49318522214889526\n",
      "*69*\tb\t0.8122162520885468\n",
      "*70*\ta\t0.6109114289283752\n",
      "*71*E\tttle\t0.7297702133655548\n",
      "72\t\t-0.0015021562576293945\n",
      "73\t,\t0.023115992546081543\n",
      "74\t\t-0.003117501735687256\n",
      "75\ti\t-0.002889871597290039\n",
      "76\td\t-0.0015906691551208496\n",
      "77\t\t-0.00035506486892700195\n",
      "78\t,\t0.0039408206939697266\n",
      "79\tname\t-0.002143383026123047\n",
      "80\t\t0.0008987188339233398\n",
      "81\t,\t0.006338953971862793\n",
      "82\tto\t0.06164884567260742\n",
      "83\tn\t0.029359042644500732\n",
      "84\tnage\t0.0030454397201538086\n",
      "85\t\t0.005168020725250244\n",
      "86\t,\t0.001688838005065918\n",
      "87\tship\t0.008625507354736328\n",
      "88\t_\t0.005386054515838623\n",
      "89\ttype\t-0.0006664395332336426\n",
      "90\t\t0.009672999382019043\n",
      "91\t,\t0.0002510547637939453\n",
      "92\tlocation\t0.00198209285736084\n",
      "93\t\t0.0018159747123718262\n",
      "94\t,\t0.0002453327178955078\n",
      "95\tdisposition\t0.000460207462310791\n",
      "96\t_\t0.0017185211181640625\n",
      "97\tof\t0.0002980232238769531\n",
      "98\t_\t0.0056972503662109375\n",
      "99\tship\t0.0028567910194396973\n",
      "100\t|\t0.01932370662689209\n",
      "101\tdeath\t0.0008538961410522461\n",
      "102\t\t-0.001347661018371582\n",
      "103\t:\t0.002463400363922119\n",
      "104\tcaused\t-0.0013953447341918945\n",
      "105\t_\t0.0009922981262207031\n",
      "106\tby\t0.005261480808258057\n",
      "107\t_\t0.004453897476196289\n",
      "108\tship\t0.0012708306312561035\n",
      "109\t_\t0.002262592315673828\n",
      "110\ti\t0.00289839506149292\n",
      "111\td\t0.007022380828857422\n",
      "112\t\t-0.0002517104148864746\n",
      "113\t,\t-4.589557647705078e-05\n",
      "114\t\t-0.00299072265625\n",
      "115\ti\t-0.0028443336486816406\n",
      "116\td\t-0.0026192665100097656\n",
      "117\t\t-0.0006656646728515625\n",
      "118\t,\t0.0003933906555175781\n",
      "119\tnote\t-0.00017625093460083008\n",
      "120\t\t0.0006151795387268066\n",
      "121\t,\t5.9485435485839844e-05\n",
      "122\tkilled\t0.0003504753112792969\n",
      "123\t\t-0.0002913475036621094\n",
      "124\t,\t-0.0006744861602783203\n",
      "125\tinjured\t-0.0009760260581970215\n"
     ]
    }
   ],
   "source": [
    "# ID = 503\n",
    "\n",
    "_seq_len = len([d for d in result if d['corrpt_type'] == 'token'])\n",
    "_tags = [[False, False, False] for _ in range(_seq_len)]  # (is_span_start, is_span_end, is_unit)\n",
    "for i, d in enumerate(result):\n",
    "    if d['corrpt_drop'] > 0.5:\n",
    "        s, e = d['corrpt_idx']\n",
    "        if e - s > 1:\n",
    "            _tags[s][0] = True\n",
    "            _tags[e-1][1] = True\n",
    "        else:\n",
    "            _tags[s][2] = True\n",
    "\n",
    "l = []\n",
    "for i in range(_seq_len):\n",
    "    id_str = str(i)\n",
    "    if _tags[i][2]:\n",
    "        id_str = f'*{i}*'\n",
    "    if _tags[i][0]:\n",
    "        id_str = 'S' + id_str\n",
    "    if _tags[i][1]:\n",
    "        id_str = id_str + 'E'\n",
    "    \n",
    "    d = result[i]\n",
    "    l.append(f'{id_str}\\t{d[\"corrpt_token\"]}\\t{d[\"corrpt_drop\"]}')\n",
    "#         l.append(f'{i}\\t{d[\"corrpt_token\"]}\\t{d[\"corrpt_drop\"]}')\n",
    "print('\\n'.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp3 (moved to py script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '/home/yshao/Projects/rome/results/exp3_relational_nodes_mutual'\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "res_path = os.path.join(res_dir, 'exp=3.0_dev_column.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_results = []\n",
    "# subject_type = 'column'\n",
    "# remove_struct_duplicate_cols = True\n",
    "# skips = ('token',)\n",
    "\n",
    "# f = open(res_path, 'w')\n",
    "\n",
    "# for ex_id, ex in enumerate(tqdm(processed_spider_dev)):\n",
    "#     text_in = ex['text_in']\n",
    "#     struct_in = ex['struct_in']\n",
    "\n",
    "#     enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "#     ex['enc_sentence'] = enc_sentence\n",
    "    \n",
    "#     parsed_struct_in = ctu.parse_struct_in(struct_in)\n",
    "#     col2table = defaultdict(list)\n",
    "#     db_id_t, tables = parsed_struct_in\n",
    "#     for table_name_t, cols in tables:\n",
    "#         for col_name_t, vals in cols:\n",
    "#             _, table_name, _ = table_name_t\n",
    "#             _, col_name, _ = col_name_t\n",
    "#             col2table[col_name].append(table_name)\n",
    "\n",
    "#     token_ranges_dict = ctu.find_struct_name_ranges(mt_uskg.tokenizer, ex)\n",
    "#     if subject_type == 'column':\n",
    "#         node_name_ranges = token_ranges_dict['col_name_ranges']\n",
    "#     elif subject_type == 'table':\n",
    "#         node_name_ranges = token_ranges_dict['table_name_ranges']\n",
    "    \n",
    "#     sql_tokens = ctu.separate_punct(ex['seq_out']).split(' ')\n",
    "#     sql_nodes = set()\n",
    "#     for t in sql_tokens:\n",
    "#         if t in node_name_ranges:\n",
    "#             sql_nodes.add(t)\n",
    "    \n",
    "#     if subject_type == 'column':\n",
    "#         for t in list(sql_nodes):\n",
    "#             if len(col2table[t]) == 0:\n",
    "#                 raise ValueError(struct_in, t)\n",
    "#             elif (len(col2table[t]) > 1) and remove_struct_duplicate_cols:\n",
    "#                 sql_nodes.remove(t)\n",
    "\n",
    "#     for node in sql_nodes:\n",
    "#         _ex = dict(ex)\n",
    "        \n",
    "#         tok_ranges = node_name_ranges[node]\n",
    "        \n",
    "#         expect = node\n",
    "#         dec_prompt = ctu.make_dec_prompt(ex['seq_out'], expect)\n",
    "\n",
    "#         ex['dec_prompt'] = dec_prompt\n",
    "#         ex['expect'] = expect\n",
    "        \n",
    "#         # Check base performance \n",
    "#         inp = ctu.make_inputs_t5(\n",
    "#             mt_uskg.tokenizer,\n",
    "#             [enc_sentence],\n",
    "#             [dec_prompt],\n",
    "#             answer=expect,\n",
    "#             device='cuda'\n",
    "#         )\n",
    "\n",
    "#         answer_len = len(mt_uskg.tokenizer.tokenize(expect))\n",
    "#         with torch.no_grad():\n",
    "#             answers_t, base_score = [d[0] for d in ctu.predict_from_input_uskg_multi_token(mt_uskg.model, inp, pred_len=answer_len)]\n",
    "#         base_score = base_score.min().item()\n",
    "#         answer = ctu.decode_sentences(mt_uskg.tokenizer, answers_t)\n",
    "\n",
    "#         result_d = ctu.token_corruption_influence_uskg(\n",
    "#             mt_uskg,\n",
    "#             # enc_sentence=enc_sentence,\n",
    "#             # dec_prompt=dec_prompt,\n",
    "#             # expect=expect,\n",
    "#             ex,\n",
    "#             replace=True,\n",
    "#             use_tqdm=False,\n",
    "#         )\n",
    "        \n",
    "#         result_d['ex_id'] = ex_id\n",
    "#         all_results.append(result_d)\n",
    "#         f.write(json.dumps(result_d, indent=None) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1655"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(res_path, 'r') as f:\n",
    "    all_results = [json.loads(l) for l in f]\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['enc_sentence', 'dec_prompt', 'expect', 'base_score', 'answers_t', 'answer', 'res_list', 'ex_id', 'correct_prediction', 'expect_table']),\n",
       " dict_keys(['corrpt_type', 'corrpt_idx', 'corrpt_token', 'corrpt_score', 'corrpt_drop']))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[0].keys(), all_results[0]['res_list'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-time patches\n",
    "# # patch 1, for those d without \"correct_prediction\", set it True \n",
    "# # patch 2, for each d, add \"expect_table\" which is table of \"expect\" column \n",
    "\n",
    "# in_path = os.path.join(res_dir, 'exp=3.0_dev_column_old.jsonl')\n",
    "# out_path = os.path.join(res_dir, 'exp=3.0_dev_column.jsonl')\n",
    "\n",
    "# with open(in_path, 'r') as f:\n",
    "#     all_results = [json.loads(l) for l in f]\n",
    "\n",
    "# for d in all_results:\n",
    "#     spider_ex = processed_spider_dev[d['ex_id']]\n",
    "#     db_id = spider_ex['db_id']\n",
    "#     col2table = db_col2table_cache[db_id]\n",
    "#     d['expect_table'] = col2table[d['expect']][0]\n",
    "\n",
    "# with open(out_path, 'w') as f:\n",
    "#     for d in all_results:\n",
    "#         f.write(json.dumps(d, indent=None) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('self', 1178),\n",
       " ('self_table', 1178),\n",
       " ('other_col', 18891),\n",
       " ('other_table', 3972),\n",
       " ('other_col_max', 1178),\n",
       " ('other_table_max', 1178)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_col2table_cache = dict()   # db_id -> col2table\n",
    "\n",
    "scores_per_rel = {\n",
    "    k: [] \n",
    "    for k in ['self', 'self_table', 'other_col', 'other_table', 'other_col_max', 'other_table_max']}\n",
    "n_corr_pred = 0\n",
    "\n",
    "for d in all_results:\n",
    "    if not d['correct_prediction']:\n",
    "        continue\n",
    "        \n",
    "    n_corr_pred += 1\n",
    "    ex_id = d['ex_id']\n",
    "    spider_ex = processed_spider_dev[ex_id]\n",
    "    db_id = spider_ex['db_id']\n",
    "    \n",
    "    # TODO: make this a function: get_col2table(struct_in)\n",
    "    if db_col2table_cache.get(db_id) is None:\n",
    "        struct_in = spider_ex['struct_in']\n",
    "        parsed_struct_in = ctu.parse_struct_in(struct_in)\n",
    "        # parsed_struct_in_cache[db_id] = parsed_struct_in\n",
    "    \n",
    "        col2table = defaultdict(list)\n",
    "        db_id_t, tables = parsed_struct_in\n",
    "        for table_name_t, cols in tables:\n",
    "            for col_name_t, vals in cols:\n",
    "                _, table_name, _ = table_name_t\n",
    "                _, col_name, _ = col_name_t\n",
    "                col2table[col_name].append(table_name)\n",
    "        db_col2table_cache[db_id] = col2table\n",
    "    col2table = db_col2table_cache[db_id]\n",
    "    \n",
    "    col = d['expect']\n",
    "    tab = d['expect_table']\n",
    "    \n",
    "    d['self_drop'] = None\n",
    "    d['self_table_drop'] = None\n",
    "    d['other_col_drop'] = []\n",
    "    d['other_table_drop'] = []\n",
    "    for res in d['res_list']:\n",
    "        _drop = res['corrpt_drop']\n",
    "        if res['corrpt_type'] == 'column':\n",
    "            if res['corrpt_token'] == col:\n",
    "                d['self_drop'] = _drop\n",
    "            else:\n",
    "                d['other_col_drop'].append(_drop)\n",
    "        elif res['corrpt_type'] == 'table':\n",
    "            if res['corrpt_token'] in col2table[col]:\n",
    "                d['self_table_drop'] = _drop\n",
    "            else:\n",
    "                d['other_table_drop'].append(_drop)\n",
    "    d['other_col_max_drop'] = max(d['other_col_drop'])\n",
    "    d['other_table_max_drop'] = max(d['other_table_drop'])\n",
    "    \n",
    "    scores_per_rel['self'].append(d['self_drop'])\n",
    "    scores_per_rel['self_table'].append(d['self_table_drop'])\n",
    "    scores_per_rel['other_col'].extend(d['other_col_drop'])\n",
    "    scores_per_rel['other_table'].extend(d['other_table_drop'])\n",
    "    scores_per_rel['other_col_max'].append(d['other_col_max_drop'])\n",
    "    scores_per_rel['other_table_max'].append(d['other_table_max_drop'])\n",
    "    \n",
    "[(k, len(scores_per_rel[k])) for k in scores_per_rel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('self', 0.671617640890816),\n",
       " ('self_table', 0.06442814806196535),\n",
       " ('other_col', 0.003760848689351455),\n",
       " ('other_table', 0.009762111044622013),\n",
       " ('other_col_max', 0.04336059879598554),\n",
       " ('other_table_max', 0.03053987597881895)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, np.mean(scores_per_rel[k])) for k in scores_per_rel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_table_effective_ids = []\n",
    "\n",
    "for i, d in enumerate(all_results):\n",
    "    self_table_drop = d.get('self_table_drop', 0.0)\n",
    "    if self_table_drop > 0.5:\n",
    "        self_table_effective_ids.append(i)\n",
    "len(self_table_effective_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[155, 157, 180, 183, 202, 205, 210, 285, 355, 358, 369, 372, 373, 403, 406, 409, 412, 415, 455, 483, 607, 609, 650, 676, 678, 761, 950, 952, 953, 1016, 1044, 1138, 1140, 1144, 1147, 1164, 1167, 1187, 1189, 1191, 1193, 1194, 1195, 1196, 1198, 1201, 1203, 1205, 1207, 1210, 1213, 1221, 1224, 1228, 1236, 1239, 1242, 1254, 1255, 1258, 1259, 1263, 1267, 1271, 1275, 1279, 1293, 1296, 1299, 1303, 1309, 1313, 1323, 1325, 1456, 1485, 1521, 1523]\n"
     ]
    }
   ],
   "source": [
    "print(self_table_effective_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155 select t2. --> make\n",
      "157 select t2.make, t1.year from cars_data as t1 join car_names as t2 on t1.id = t2. --> makeid\n",
      "180 select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2. --> makeid\n",
      "183 select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2. --> makeid\n",
      "202 select min(weight) from cars_data where --> cylinders\n",
      "205 select min(weight) from cars_data where --> cylinders\n",
      "210 select t1.countryname, t1. --> countryid\n",
      "285 select count(*) from countries as t1 join car_makers as t2 on t1.countryid = t2. --> country\n",
      "355 select count(*) from flights as t1 join airports as t2 on t1. --> sourceairport\n",
      "358 select count(*) from flights as t1 join airports as t2 on t1. --> sourceairport\n",
      "369 select count(*) from flights as t1 join airports as t2 on t1.destairport = t2.airportcode join airports as t3 on t1. --> sourceairport\n",
      "372 select count(*) from flights as t1 join airports as t2 on t1.destairport = t2.airportcode join airports as t3 on t1.sourceairport = t3.airportcode where t2. --> city\n",
      "373 select count(*) from flights as t1 join airports as t2 on t1.destairport = t2.airportcode join airports as t3 on t1. --> sourceairport\n",
      "403 select t1.city from airports as t1 join flights as t2 on t1.airportcode = t2. --> sourceairport\n",
      "406 select t1.airportcode from airports as t1 join flights as t2 on t1.airportcode = t2.destairport or t1.airportcode = t2. --> sourceairport\n",
      "409 select t1.airportcode from airports as t1 join flights as t2 on t1.airportcode = t2.destairport or t1.airportcode = t2. --> sourceairport\n",
      "412 select t1.airportcode from airports as t1 join flights as t2 on t1.airportcode = t2.destairport or t1.airportcode = t2. --> sourceairport\n",
      "415 select t1.airportcode from airports as t1 join flights as t2 on t1.airportcode = t2.destairport or t1.airportcode = t2. --> sourceairport\n",
      "455 select t1.flightno from flights as t1 join airports as t2 on t1. --> sourceairport\n",
      "483 select airportname from airports where airportcode not in (select --> sourceairport\n",
      "607 select t3.name from course_arrange as t1 join --> course\n",
      "609 select t3.name from course_arrange as t1 join --> course\n",
      "650 select count(*) from visitor where --> id\n",
      "676 select winner_name from matches where --> year\n",
      "678 select count(*) from matches where --> year\n",
      "761 select count(distinct winner_name) from matches where --> tourney_name\n",
      "950 select title, directed_by from cartoon order by --> original_air_date\n",
      "952 select --> title\n",
      "953 select title, directed_by from cartoon order by --> original_air_date\n",
      "1016 select production_code, channel from cartoon order by --> original_air_date\n",
      "1044 select id from tv_channel group by --> country\n",
      "1138 select region from country as t1 join city as t2 on t1. --> code\n",
      "1140 select region from country as t1 join city as t2 on t1. --> code\n",
      "1144 select t2.language from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1147 select t2.language from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1164 select avg(lifeexpectancy) from country where --> continent\n",
      "1167 select avg(lifeexpectancy) from country where --> continent\n",
      "1187 select count(t2.language) from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1189 select count(t2.language) from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1191 select count(*) from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1193 select count(*) from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1194 select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1195 select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1196 select t1.continent from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1198 select t1.continent from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1201 select count(*) from (select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1203 select count(*) from (select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1205 select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1207 select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1210 select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1213 select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1221 select distinct t1.region from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1224 select distinct t1.region from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1228 select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1236 select t2.language from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1239 select t2.language from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1242 select t2.language from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1254 select avg( --> lifeexpectancy\n",
      "1255 select avg(lifeexpectancy) from country where name not in (select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1258 select avg( --> lifeexpectancy\n",
      "1259 select avg(lifeexpectancy) from country where name not in (select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1263 select sum(population) from country where name not in (select t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1267 select t2.language from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1271 select t2.language from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1275 select count(distinct t2.language) from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1279 select count(distinct t2.language) from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1293 select --> code\n",
      "1296 select --> code\n",
      "1299 select distinct t2.name from country as t1 join city as t2 on t2.countrycode = t1. --> code\n",
      "1303 select distinct t2.name from country as t1 join city as t2 on t2.countrycode = t1. --> code\n",
      "1309 select distinct t3.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1313 select distinct t3.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1323 select count(t2.language), t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1325 select count(t2.language), t1.name from country as t1 join countrylanguage as t2 on t1. --> code\n",
      "1456 select --> grade\n",
      "1485 select --> id\n",
      "1521 select avg( --> grade\n",
      "1523 select avg( --> grade\n"
     ]
    }
   ],
   "source": [
    "# Any type of samples make self-table effective? More with JOIN \n",
    "\n",
    "for i in self_table_effective_ids:\n",
    "    res_d = all_results[i]\n",
    "    print(i, res_d['dec_prompt'], '-->', res_d['expect'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = 157\n",
    "res_d = all_results[_id]\n",
    "result = res_d['res_list']\n",
    "# _seq_len = max([d['corrpt_idx'][1] - 1 for d in result])\n",
    "# _tags = [[False, False, False] for _ in range(_seq_len)]  # (is_span_start, is_span_end, is_unit)\n",
    "# for i, d in enumerate(result):\n",
    "#     if d['corrpt_drop'] > 0.5:\n",
    "#         s, e = d['corrpt_idx']\n",
    "#         if e - s > 1:\n",
    "#             _tags[s][0] = True\n",
    "#             _tags[e-1][1] = True\n",
    "#         else:\n",
    "#             _tags[s][2] = True\n",
    "\n",
    "# spider_ex = processed_spider_dev[res_d['ex_id']]\n",
    "print(res_d['enc_sentence'])\n",
    "print(res_d['dec_prompt'], '-->', res_d['expect'])\n",
    "print()\n",
    "\n",
    "l = []\n",
    "for d in result:\n",
    "    _prefix = ''\n",
    "    db_id = spider_ex['db_id']\n",
    "    \n",
    "    if d[\"corrpt_type\"] == 'column' and d[\"corrpt_token\"] == res_d['expect']:\n",
    "        _prefix = '**'\n",
    "    elif d[\"corrpt_type\"] == 'table' and d[\"corrpt_token\"] == res_d['expect_table']:\n",
    "        _prefix = '*>'\n",
    "    l.append(f'{_prefix}{d[\"corrpt_type\"]}\\t{d[\"corrpt_token\"]}\\t{d[\"corrpt_drop\"]}')\n",
    "#         l.append(f'{i}\\t{d[\"corrpt_token\"]}\\t{d[\"corrpt_drop\"]}')\n",
    "print('\\n'.join(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by different aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('medium', 565), ('easy', 219), ('hard', 216), ('extra', 178)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Hardness\n",
    "\n",
    "samples_by_hardness = defaultdict(list)\n",
    "\n",
    "for res_d in all_results:\n",
    "    if not res_d['correct_prediction']:\n",
    "        continue\n",
    "    hardness = spider_id2hardness[res_d['ex_id']]\n",
    "    samples_by_hardness[hardness].append(res_d)\n",
    "\n",
    "[(h, len(samples)) for h, samples in samples_by_hardness.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardness: easy (219)\n",
      "self\t0.6729\t\n",
      "self_table\t0.0200\t\n",
      "other_col\t0.0022\t\n",
      "other_table\t0.0024\t\n",
      "other_col_max\t0.0389\t\n",
      "other_table_max\t0.0063\t\n",
      "\n",
      "Hardness: medium (565)\n",
      "self\t0.6568\t\n",
      "self_table\t0.0510\t\n",
      "other_col\t0.0028\t\n",
      "other_table\t0.0060\t\n",
      "other_col_max\t0.0344\t\n",
      "other_table_max\t0.0185\t\n",
      "\n",
      "Hardness: hard (216)\n",
      "self\t0.6767\t\n",
      "self_table\t0.0705\t\n",
      "other_col\t0.0069\t\n",
      "other_table\t0.0177\t\n",
      "other_col_max\t0.0619\t\n",
      "other_table_max\t0.0551\t\n",
      "\n",
      "Hardness: extra (178)\n",
      "self\t0.7109\t\n",
      "self_table\t0.1544\t\n",
      "other_col\t0.0047\t\n",
      "other_table\t0.0192\t\n",
      "other_col_max\t0.0546\t\n",
      "other_table_max\t0.0688\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for h in ['easy', 'medium', 'hard', 'extra']:\n",
    "    samples = samples_by_hardness[h]\n",
    "    print(f'Hardness: {h} ({len(samples)})')\n",
    "    # print('[Example sql]', samples[0]['seq_out'])\n",
    "    \n",
    "    for rel in scores_per_rel.keys():\n",
    "        _rel_k = rel + '_drop'\n",
    "        rel_scores = [x for res_d in samples for x in ctu.ensure_list(res_d[_rel_k])]\n",
    "        avg_score = np.mean(rel_scores)\n",
    "        print(f'{rel}\\t{avg_score:.4f}\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('select', 669),\n",
       " ('where', 346),\n",
       " ('order by', 68),\n",
       " ('join', 90),\n",
       " ('group by', 1),\n",
       " ('having', 4)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Column role\n",
    "\n",
    "samples_by_column_role = defaultdict(list)\n",
    "\n",
    "for res_d in all_results:\n",
    "    if not res_d['correct_prediction']:\n",
    "        continue\n",
    "    col_role_kw = _detect_column_role(res_d['dec_prompt'])\n",
    "    samples_by_column_role[col_role_kw].append(res_d)\n",
    "\n",
    "[(r, len(samples)) for r, samples in samples_by_column_role.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column role: select (669)\n",
      "self\t0.6103\t\n",
      "self_table\t0.0210\t\n",
      "other_col\t0.0025\t\n",
      "other_table\t0.0036\t\n",
      "other_col_max\t0.0383\t\n",
      "other_table_max\t0.0131\t\n",
      "\n",
      "Column role: where (346)\n",
      "self\t0.7348\t\n",
      "self_table\t0.0284\t\n",
      "other_col\t0.0017\t\n",
      "other_table\t0.0022\t\n",
      "other_col_max\t0.0186\t\n",
      "other_table_max\t0.0074\t\n",
      "\n",
      "Column role: order by (68)\n",
      "self\t0.8414\t\n",
      "self_table\t0.0353\t\n",
      "other_col\t0.0011\t\n",
      "other_table\t0.0027\t\n",
      "other_col_max\t0.0106\t\n",
      "other_table_max\t0.0061\t\n",
      "\n",
      "Column role: join (90)\n",
      "self\t0.7382\t\n",
      "self_table\t0.5404\t\n",
      "other_col\t0.0241\t\n",
      "other_table\t0.0809\t\n",
      "other_col_max\t0.2031\t\n",
      "other_table_max\t0.2695\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r, samples in samples_by_column_role.items():\n",
    "    if len(samples) < 10:\n",
    "        continue\n",
    "    print(f'Column role: {r} ({len(samples)})')\n",
    "    # print('[Example sql]', samples[0]['seq_out'])\n",
    "    \n",
    "    for rel in scores_per_rel.keys():\n",
    "        _rel_k = rel + '_drop'\n",
    "        rel_scores = [x for res_d in samples for x in ctu.ensure_list(res_d[_rel_k])]\n",
    "        avg_score = np.mean(rel_scores)\n",
    "        print(f'{rel}\\t{avg_score:.4f}\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exact', 644), ('partial', 173), ('no-match', 361)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text match \n",
    "\n",
    "samples_by_text_match = {k: [] for k in ['exact', 'partial', 'no-match']}\n",
    "\n",
    "for res_d in all_results:\n",
    "    if not res_d['correct_prediction']:\n",
    "        continue\n",
    "    spider_ex = processed_spider_dev[res_d['ex_id']]\n",
    "    text_match = ctu.check_text_match(spider_ex, col=res_d['expect'], tab=res_d['expect_table'])\n",
    "    samples_by_text_match[text_match].append(res_d)\n",
    "\n",
    "[(m, len(samples)) for m, samples in samples_by_text_match.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-match: exact (644)\n",
      "self\t0.5317\t\n",
      "self_table\t0.0215\t\n",
      "other_col\t0.0020\t\n",
      "other_table\t0.0034\t\n",
      "other_col_max\t0.0223\t\n",
      "other_table_max\t0.0106\t\n",
      "\n",
      "Text-match: partial (173)\n",
      "self\t0.7947\t\n",
      "self_table\t0.0020\t\n",
      "other_col\t0.0020\t\n",
      "other_table\t0.0022\t\n",
      "other_col_max\t0.0457\t\n",
      "other_table_max\t0.0112\t\n",
      "\n",
      "Text-match: no-match (361)\n",
      "self\t0.8622\t\n",
      "self_table\t0.1710\t\n",
      "other_col\t0.0079\t\n",
      "other_table\t0.0258\t\n",
      "other_col_max\t0.0798\t\n",
      "other_table_max\t0.0754\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m, samples in samples_by_text_match.items():\n",
    "    print(f'Text-match: {m} ({len(samples)})')\n",
    "    # print('[Example sql]', samples[0]['seq_out'])\n",
    "    \n",
    "    for rel in scores_per_rel.keys():\n",
    "        _rel_k = rel + '_drop'\n",
    "        rel_scores = [x for res_d in samples for x in ctu.ensure_list(res_d[_rel_k])]\n",
    "        avg_score = np.mean(rel_scores)\n",
    "        print(f'{rel}\\t{avg_score:.4f}\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-1: column/table corruption\n",
    "- In py script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'dev_column'\n",
    "\n",
    "res_json_path = f'/home/yshao/Projects/rome/results/struct_node_restore/{exp_name}.jsonl'\n",
    "with open(res_json_path, 'r') as f:\n",
    "#     res_dicts = [json.loads(l) for l in f if l]\n",
    "    all_str = f.read()\n",
    "all_str = all_str.replace('{\"ex_id\":', '\\n{\"ex_id\":').strip()\n",
    "res_dicts = [json.loads(l) for l in all_str.split('\\n')]\n",
    "len(res_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([r for d in res_dicts for r in d['trace_results']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_trace_results = []\n",
    "for d in res_dicts:\n",
    "    for r in d['trace_results']:\n",
    "        if r['correct_prediction'] and r['is_good_sample']:\n",
    "            r['ex_id'] = d['ex_id']\n",
    "            good_trace_results.append(r)\n",
    "len(good_trace_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['ex_id', 'trace_results']),\n",
       " dict_keys(['low_score', 'high_score', 'input_ids', 'input_tokens', 'dec_input_ids', 'dec_input_tokens', 'subject_range', 'subject_range_individual_indices', 'answer', 'window', 'correct_prediction', 'kind', 'scores', 'is_good_sample', 'target_node', 'ex_id']),\n",
       " list)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys(), good_trace_results[0].keys(), type(good_trace_results[0]['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating all plots\n",
    "fig_save_dir = f'/home/yshao/Projects/rome/results/struct_node_restore/figs/{exp_name}'\n",
    "\n",
    "for i, r in enumerate(tqdm(good_trace_results)):\n",
    "    result = dict(r)\n",
    "\n",
    "    enc_s, dec_s = result['scores']\n",
    "    enc_s = np.array(enc_s)\n",
    "    dec_s = np.array(dec_s)\n",
    "    result['scores'] = [enc_s, dec_s]\n",
    "\n",
    "    ex_id = r['ex_id']\n",
    "    ctu.plot_trace_heatmap_t5(result, savepdf=os.path.join(fig_save_dir, f'{i}-ex_id={ex_id}.pdf'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding special samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57-ex_id=69\n",
      "60-ex_id=70\n",
      "83-ex_id=139\n",
      "87-ex_id=143\n",
      "88-ex_id=144\n",
      "89-ex_id=145\n",
      "91-ex_id=153\n"
     ]
    }
   ],
   "source": [
    "for i, r in enumerate(good_trace_results):\n",
    "    _, dec_s = r['scores']\n",
    "    ## dec_s: (n_toks, n_layers)\n",
    "    non_last_token_s = np.array(dec_s)[:-1]\n",
    "    if (non_last_token_s > 0.5).any():\n",
    "        print(f'{i}-ex_id={r[\"ex_id\"]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-1.1: severing decoder cross-attention\n",
    "- TODO: merge to py script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single sample (ID = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = 2\n",
    "ex = processed_spider_dev[_id]\n",
    "ex['question'], ex['struct_in'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "enc_tokenized = mt_uskg.tokenizer(enc_sentence)\n",
    "\n",
    "token_ranges_dict = ctu.find_struct_name_ranges(mt_uskg.tokenizer, enc_tokenized['input_ids'], struct_in)\n",
    "token_ranges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'name'\n",
    "tok_ranges = token_ranges_dict['col_name_ranges'][col]\n",
    "tok_indices = [i for s, e in tok_ranges for i in range(s, e)]\n",
    "\n",
    "dec_prompt = make_dec_prompt(ex['seq_out'], col)\n",
    "\n",
    "dec_prompt, tok_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ctu.calculate_hidden_flow_uskg(\n",
    "    mt_uskg,\n",
    "    enc_sentence=enc_sentence,\n",
    "    dec_prompt=dec_prompt,\n",
    "    expect=col,\n",
    "    e_range=tok_indices,\n",
    "    enc_token_range=[],    # no analysis\n",
    "    dec_token_range=None,  # full analysis\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    "    sever_kind='self_attn'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no sever\n",
    "ctu.plot_trace_heatmap_t5(result)\n",
    "print([f\"{s:.2f}\" for s in result['scores'][1][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sever cross_attn\n",
    "ctu.plot_trace_heatmap_t5(result)\n",
    "print([f\"{s:.2f}\" for s in result['scores'][1][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sever mlp; a bit better than no severing??\n",
    "ctu.plot_trace_heatmap_t5(result)\n",
    "print([f\"{s:.2f}\" for s in result['scores'][1][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sever self_attn\n",
    "ctu.plot_trace_heatmap_t5(result)\n",
    "print([f\"{s:.2f}\" for s in result['scores'][1][-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result['scores'][1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-2: dirty text recovery\n",
    "- Corrupt the text, restore different parts of encoder final output\n",
    "- Idea is to check the existence of contextual understanding (incorporating text info into struct representation)\n",
    "- (In py script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single sample\n",
    "- ID = 2, col = 'name': (name exact match with text)\n",
    "    - none: wrong; text: correct; struct: correct; col: correct\n",
    "- ID = 5, col = 'country': (no exact match, but value match with text (French) )\n",
    "    - none: wrong; text: correct; struct: correct; col: correct\n",
    "- ID = 145, col = 'year': (no exact match, but value match with text (1980) )\n",
    "    - none: wrong; text: correct; struct: correct; col: correct\n",
    "- ID = 7, col = 'song_release_year': (multi-token, and with confusing columns in table (song_name) )\n",
    "    - none: wrong; text: wrong; struct: correct; col: wrong\n",
    "    - hypothesis: need clean struct to avoid confusion\n",
    "- ID = 185, col = 'airportcode': (multi-token, with confusing col (airportname), but have match with text (airport code) )\n",
    "    - none: wrong; text: correct; struct: wrong (2nd token); col: wrong\n",
    "    - differs from hypothesis... clean struct should know sql info about the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('concert_singer',\n",
       " 'Show the stadium name and capacity with most number of concerts in year 2014 or after.',\n",
       " '| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id',\n",
       " 'select t2.name, t2.capacity from concert as t1 join stadium as t2 on t1.stadium_id = t2.stadium_id where t1.year >= 2014 group by t2.stadium_id order by count(*) desc limit 1')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = 24\n",
    "ex = processed_spider_dev[_id]\n",
    "ex['db_id'], ex['question'], ex['struct_in'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 17), (24, 134))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "enc_tokenized = mt_uskg.tokenizer(enc_sentence)\n",
    "\n",
    "text_range, struct_range = ctu.find_text_struct_in_range(mt_uskg.tokenizer, enc_tokenized['input_ids'])\n",
    "text_range, struct_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name', 'nationality'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ranges_dict = ctu.find_struct_name_ranges(mt_uskg.tokenizer, enc_tokenized['input_ids'], struct_in)\n",
    "\n",
    "col_name_ranges = token_ranges_dict['col_name_ranges']\n",
    "\n",
    "sql_tokens = ctu.separate_punct(ex['seq_out']).split(' ')\n",
    "\n",
    "sql_cols = set()\n",
    "for t in sql_tokens:\n",
    "    if t in col_name_ranges:\n",
    "        sql_cols.add(t)\n",
    "sql_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 'orchestra', 'orchestra'),\n",
       " [((3, 'conductor', 'conductor'),\n",
       "   [[(5, 'conductor_id', 'conductor_id'), []],\n",
       "    [(7, 'name', 'name'), []],\n",
       "    [(9, 'age', 'age'), []],\n",
       "    [(11, 'nationality', 'nationality ( USA )'), [(13, 'USA', 'USA')]],\n",
       "    [(16, 'year_of_work', 'year_of_work'), []]]),\n",
       "  ((18, 'orchestra', 'orchestra'),\n",
       "   [[(20, 'orchestra_id', 'orchestra_id'), []],\n",
       "    [(22, 'orchestra', 'orchestra'), []],\n",
       "    [(24, 'conductor_id', 'conductor_id'), []],\n",
       "    [(26, 'record_company', 'record_company'), []],\n",
       "    [(28, 'year_of_founded', 'year_of_founded'), []],\n",
       "    [(30, 'major_record_format', 'major_record_format'), []]]),\n",
       "  ((32, 'performance', 'performance'),\n",
       "   [[(34, 'performance_id', 'performance_id'), []],\n",
       "    [(36, 'orchestra_id', 'orchestra_id'), []],\n",
       "    [(38, 'type', 'type'), []],\n",
       "    [(40, 'date', 'date'), []],\n",
       "    [(42, 'official_ratings_(millions)', 'official_ratings_(millions)'), []],\n",
       "    [(44, 'weekly_rank', 'weekly_rank'), []],\n",
       "    [(46, 'share', 'share'), []]]),\n",
       "  ((48, 'show', 'show'),\n",
       "   [[(50, 'show_id', 'show_id'), []],\n",
       "    [(52, 'performance_id', 'performance_id'), []],\n",
       "    [(54, 'if_first_show', 'if_first_show'), []],\n",
       "    [(56, 'result', 'result'), []],\n",
       "    [(58, 'attendance', 'attendance'), []]])])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_struct_in = ctu.parse_struct_in(struct_in)\n",
    "parsed_struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('select name from conductor where', 'nationality')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = 'nationality'\n",
    "# tok_ranges = token_ranges_dict['col_name_ranges'][col]\n",
    "# tok_indices = [i for s, e in tok_ranges for i in range(s, e)]\n",
    "dec_prompt = ctu.make_dec_prompt(ex['seq_out'], col)\n",
    "expect = col\n",
    "dec_prompt, expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('nationality', [(45, 51)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_toks = token_ranges_dict['col_name_ranges'][col]\n",
    "col, col_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = ctu.calculate_hidden_flow_uskg(\n",
    "#     mt_uskg,\n",
    "#     enc_sentence=enc_sentence,\n",
    "#     dec_prompt=dec_prompt,\n",
    "#     expect=col,\n",
    "#     e_range=text_range,\n",
    "#     enc_token_range=[],    # no analysis\n",
    "#     dec_token_range=None,  # full analysis\n",
    "#     tokens_to_mix_individual_indices=False,\n",
    "#     replace=True,\n",
    "#     sever_kind=None,\n",
    "# )\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence] * 11,\n",
    "    [dec_prompt] * 11,\n",
    "    answer=expect)\n",
    "\n",
    "encoder_text_last_layer_states = [\n",
    "    (tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "    for tnum in range(*text_range)\n",
    "]\n",
    "\n",
    "encoder_struct_last_layer_states = [\n",
    "    (tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "    for tnum in range(*struct_range)\n",
    "]\n",
    "\n",
    "encoder_col_last_layer_states = [\n",
    "    (tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "    for tnum in col_toks\n",
    "]\n",
    "\n",
    "answer_len = len(mt_uskg.tokenizer.tokenize(expect))\n",
    "answers_t, base_score = [d[0] for d in ctu.predict_from_input_uskg_multi_token(mt_uskg.model, inp, pred_len=answer_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.1734, device='cuda:0'), tensor(1.0000, device='cuda:0')],\n",
       " tensor(0.1734, device='cuda:0'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Corrupting text: expect wrong pred \n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(1.0000, device='cuda:0'), tensor(0.9999, device='cuda:0')],\n",
       " tensor(0.9999, device='cuda:0'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Restoring text encoding for decoder, but struct encoding are with dirty text encoding \n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=encoder_text_last_layer_states,\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(1.0000, device='cuda:0'), tensor(1., device='cuda:0')],\n",
       " tensor(1.0000, device='cuda:0'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Restoring clean struct encoding but dirty text encoding for decoder\n",
    "## Prediction being correct means encoder final output has \"contextual\" or \"semantic\" understanding of struct_in \n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=encoder_struct_last_layer_states,\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(1.0000, device='cuda:0'), tensor(1.0000, device='cuda:0')],\n",
       " tensor(1.0000, device='cuda:0'))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Restoring clean col_name encoding but dirty text encoding for decoder (stricter than above)\n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=encoder_col_last_layer_states,\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(1.0000, device='cuda:0'), tensor(1.0000, device='cuda:0')],\n",
       " tensor(1.0000, device='cuda:0'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For exp-2.1: mutual corruption\n",
    "# First pass: corrupt text (no restore)\n",
    "# Second pass: corrupt struct, no restore, reset struct output to first pass\n",
    "\n",
    "all_ans_probs = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=encoder_struct_last_layer_states,\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix_1st_pass=text_range,\n",
    "    tokens_to_mix=struct_range,\n",
    "    tokens_to_mix_individual_indices=False,\n",
    "    replace=True,\n",
    ")\n",
    "all_ans_probs, min(all_ans_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_path = '/home/yshao/Projects/rome/results/exp2_text_struct_interaction/exp=2_train_column.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5439, 5440, 6999)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples[5439]['ex_id'], all_samples[5440]['ex_id'], all_samples[-1]['ex_id'], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5\n",
    "\n",
    "base_scores = []\n",
    "low_scores = []\n",
    "\n",
    "## len = n_good_samples\n",
    "restore_scores_dict = {\n",
    "    'text': [],\n",
    "    'struct': [],\n",
    "    'node': [],\n",
    "    'struct_no_node': [],\n",
    "    'node_corrupt_all': [],\n",
    "    # 2.0.1: cancelled\n",
    "    # 'ctname': [],      # col name + table name (col belongs to) \n",
    "    # 'catname': [],     # col name + all table names \n",
    "    # 'full_table': [],  # full table (col belongs to) \n",
    "    # 'all_col': [],     # all col names (regardless of table)\n",
    "}\n",
    "\n",
    "## len = total_samples\n",
    "mutual_scores_dict = {\n",
    "    f'{text}-{struct}': []\n",
    "    for text in ['clean_t', 'dc_t', 'dirty_t']\n",
    "    for struct in ['clean_s', 'dc_s', 'dirty_s']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10390, (6177, 6177), 2568, 1645)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['text_in', 'struct_in', 'seq_out', 'dec_prompt', 'expect', 'table', 'expect_input_range', 'mutual_scores', 'answer', 'answers_t', 'correct_prediction', 'base_score', 'low_score', 'is_good_sample', 'r_text_score', 'r_struct_score', 'r_node_score', 'r_struct_no_node_score', 'r_node_corrupt_all_score', 'ex_id']),\n",
       " dict_keys(['clean_t-clean_s', 'clean_t-dc_s', 'clean_t-dirty_s', 'dc_t-clean_s', 'dc_t-dc_s', 'dc_t-dirty_s', 'dirty_t-clean_s', 'dirty_t-dc_s', 'dirty_t-dirty_s']))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys(), d['mutual_scores'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    base_scores.append(d['base_score'])\n",
    "    low_scores.append(d['low_score'])\n",
    "    for k in restore_scores_dict.keys():\n",
    "        restore_scores_dict[k].append(d[f'r_{k}_score'])\n",
    "    for k in mutual_scores_dict.keys():\n",
    "        mutual_scores_dict[k].append(d['mutual_scores'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6177, 6177, 6177)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_scores), len(restore_scores_dict['text']), len(mutual_scores_dict['dc_t-dc_s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\t0.6394\n",
      "struct\t0.8891\n",
      "node\t0.7598\n",
      "struct_no_node\t0.1105\n",
      "node_corrupt_all\t0.7477\n"
     ]
    }
   ],
   "source": [
    "# results for exp2\n",
    "for k, scores in restore_scores_dict.items():\n",
    "    avg = np.mean(scores)\n",
    "    print(f'{k}\\t{avg:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score\tavg_gain\tperc_recover\n",
      "base_scores\t0.9613\t1.0000\n",
      "restore_text_scores\t0.6008\t0.6291\n",
      "restore_struct_scores\t0.8504\t0.9021\n",
      "restore_node_scores\t0.7211\t0.7764\n",
      "restore_struct_no_node_scores\t0.0718\t0.0792\n",
      "restore_node_corrupt_all_scores\t0.7091\t0.7505\n"
     ]
    }
   ],
   "source": [
    "print(f'Score\\tavg_gain\\tperc_recover')\n",
    "for score_label, high_scores in [\n",
    "    ('base_scores', base_scores),\n",
    "    ('restore_text_scores', restore_scores_dict['text']),\n",
    "    ('restore_struct_scores', restore_scores_dict['struct']),\n",
    "    ('restore_node_scores', restore_scores_dict['node']),\n",
    "    ('restore_struct_no_node_scores', restore_scores_dict['struct_no_node']),\n",
    "    ('restore_node_corrupt_all_scores', restore_scores_dict['node_corrupt_all']),\n",
    "]:\n",
    "    avg_gain = np.mean([h - l for h, l in zip(high_scores, low_scores)])\n",
    "    perc_recover = np.mean([h - l > 0.5 for h, l in zip(high_scores, low_scores)])\n",
    "    print(f'{score_label}\\t{avg_gain:.4f}\\t{perc_recover:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        clean_t dc_t    dirty_t \n",
      "clean_s 1.0000  0.9896  0.8891  \n",
      "dc_s    0.6394  0.4793  0.0387  \n",
      "dirty_s 0.3766  0.3141  0.0037  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results for exp2.1.1 \n",
    "msg = ' '*8\n",
    "for k_t in ['clean_t', 'dc_t', 'dirty_t']:\n",
    "    msg += f'{k_t:8s}'\n",
    "msg += '\\n'\n",
    "for k_s in ['clean_s', 'dc_s', 'dirty_s']:\n",
    "    msg += f'{k_s:8s}'\n",
    "    for k_t in ['clean_t', 'dc_t', 'dirty_t']:\n",
    "        k = f'{k_t}-{k_s}'\n",
    "        scores = mutual_scores_dict[k]\n",
    "        avg = np.mean(scores)\n",
    "        msg += f'{avg:.4f}  '\n",
    "    msg += '\\n'\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('easy', 248), ('medium', 446), ('hard', 174), ('extra', 166)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build an index for hardness \n",
    "\n",
    "spider_indices_per_hardness = defaultdict(list)\n",
    "spider_id2hardness = dict()\n",
    "\n",
    "for i, ex in enumerate(processed_spider_dev):\n",
    "    db_id = ex['db_id']\n",
    "    sql_str = ex['seq_out']\n",
    "    sql = sp_eval.get_sql(evaluator.schemas[db_id], sql_str)\n",
    "    hardness = evaluator.eval_hardness(sql)\n",
    "    spider_indices_per_hardness[hardness].append(i)\n",
    "    spider_id2hardness[i] = hardness\n",
    "\n",
    "[(h, len(indices)) for h, indices in spider_indices_per_hardness.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 25, 41, 42, 57, 58, 59, 60, 61, 62]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_indices_per_hardness['extra'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp2 + exp2.1.1 \n",
    "# res_path = '/home/yshao/Projects/rome/results/exp2_text_struct_interaction/exp=2_dev_column.jsonl'\n",
    "\n",
    "# with open(res_path, 'r') as f:\n",
    "#     all_samples = [json.loads(l) for l in f]\n",
    "# len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        if d['is_good_sample']:\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_by_hardness = defaultdict(list)\n",
    "\n",
    "# for ex in all_samples:\n",
    "#     for d in ex['trace_results']:\n",
    "#         if not d['is_good_sample']:\n",
    "#             continue\n",
    "\n",
    "for d in good_samples:\n",
    "#     sql_str = d['seq_out']\n",
    "#     db_id = d['struct_in'].split('|')[1].strip()    # TODO: add db_id to result during experiment main run \n",
    "#     sql = sp_eval.get_sql(evaluator.schemas[db_id], sql_str)\n",
    "#     hardness = evaluator.eval_hardness(sql)\n",
    "    hardness = spider_id2hardness[d['ex_id']]\n",
    "    samples_by_hardness[hardness].append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('medium', 480), ('easy', 181), ('hard', 186), ('extra', 136)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(h, len(samples)) for h, samples in samples_by_hardness.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardness: easy (181)\n",
      "[Example sql] select distinct country from singer where age > 20\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9572\t1.0000\n",
      "r_text_score\t0.6232\t0.6740\n",
      "r_struct_score\t0.8541\t0.9061\n",
      "r_col_score\t0.7442\t0.8011\n",
      "r_struct_no_col_score\t0.0945\t0.1105\n",
      "r_col_corrupt_all_score\t0.7708\t0.8177\n",
      "\n",
      "Hardness: medium (480)\n",
      "[Example sql] select name, country, age from singer order by age desc\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9462\t1.0000\n",
      "r_text_score\t0.5521\t0.5938\n",
      "r_struct_score\t0.8132\t0.8688\n",
      "r_col_score\t0.6674\t0.7188\n",
      "r_struct_no_col_score\t0.0577\t0.0583\n",
      "r_col_corrupt_all_score\t0.6774\t0.7208\n",
      "\n",
      "Hardness: hard (186)\n",
      "[Example sql] select song_name from singer where age > (select avg(age) from singer)\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9462\t1.0000\n",
      "r_text_score\t0.5662\t0.5914\n",
      "r_struct_score\t0.7303\t0.7688\n",
      "r_col_score\t0.6793\t0.7204\n",
      "r_struct_no_col_score\t0.0947\t0.1183\n",
      "r_col_corrupt_all_score\t0.7154\t0.7419\n",
      "\n",
      "Hardness: extra (136)\n",
      "[Example sql] select t2.name, t2.location from concert as t1 join stadium as t2 on t1.stadium_id = t2.stadium_id where t1.year = 2014 intersect select t2.name, t2.location from concert as t1 join stadium as t2 on t1.stadium_id = t2.stadium_id where t1.year = 2015\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9346\t1.0000\n",
      "r_text_score\t0.4163\t0.3971\n",
      "r_struct_score\t0.8308\t0.8750\n",
      "r_col_score\t0.6615\t0.7279\n",
      "r_struct_no_col_score\t0.1152\t0.1324\n",
      "r_col_corrupt_all_score\t0.7424\t0.7941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for h in ['easy', 'medium', 'hard', 'extra']:\n",
    "    samples = samples_by_hardness[h]\n",
    "    print(f'Hardness: {h} ({len(samples)})')\n",
    "    print('[Example sql]', samples[0]['seq_out'])\n",
    "    print(f'Score\\tavg_gain\\tperc_recover')\n",
    "    \n",
    "    low_scores = [d['low_score'] for d in samples]\n",
    "    for score_label in [\n",
    "        'base_score',\n",
    "        'r_text_score',\n",
    "        'r_struct_score',\n",
    "        'r_col_score',\n",
    "        'r_struct_no_col_score',\n",
    "        'r_col_corrupt_all_score'\n",
    "    ]:\n",
    "        high_scores = [d[score_label] for d in samples]\n",
    "        avg_gain = np.mean([h - l for h, l in zip(high_scores, low_scores)])\n",
    "        perc_recover = np.mean([h - l > 0.5 for h, l in zip(high_scores, low_scores)])\n",
    "        print(f'{score_label}\\t{avg_gain:.4f}\\t{perc_recover:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by column role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_keyword_pattern = r'\\W(select|where|join|group by|having|order by)\\W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select t2.name, t2.location from concert as t1 join stadium as t2 on t1.stadium_id = t2.stadium_id where t1.year = 2014 intersect select t2.name, t2.location from concert as t1 join stadium as t2 on t1.stadium_id = t2.stadium_id where t1.year = 2015'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sql_str = samples_by_hardness['extra'][0]['seq_out']\n",
    "_sql_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select', 'join', 'where', 'select', 'join', 'where']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(role_keyword_pattern, ' ' + _sql_str + ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('select', 632),\n",
       " ('where', 231),\n",
       " ('order by', 57),\n",
       " ('join', 60),\n",
       " ('group by', 1),\n",
       " ('having', 2)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_by_column_role = defaultdict(list)\n",
    "\n",
    "for d in good_samples:\n",
    "#     all_kws = re.findall(role_keyword_pattern, ' ' + d['dec_prompt'] + ' ')\n",
    "#     assert len(all_kws) > 0, d['dec_prompt']\n",
    "#     col_role_kw = all_kws[-1]\n",
    "    col_role_kw = _detect_column_role(d['dec_prompt'])\n",
    "    samples_by_column_role[col_role_kw].append(d)\n",
    "\n",
    "[(r, len(samples)) for r, samples in samples_by_column_role.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column role: select (632)\n",
      "[Example prompt] select name, country,\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9639\t1.0000\n",
      "r_text_score\t0.6161\t0.6519\n",
      "r_struct_score\t0.7748\t0.8070\n",
      "r_col_score\t0.6741\t0.7184\n",
      "r_struct_no_col_score\t0.0500\t0.0475\n",
      "r_col_corrupt_all_score\t0.6990\t0.7278\n",
      "\n",
      "Column role: where (231)\n",
      "[Example prompt] select avg(age), min(age), max(age) from singer where\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9186\t1.0000\n",
      "r_text_score\t0.5183\t0.5584\n",
      "r_struct_score\t0.8727\t0.9654\n",
      "r_col_score\t0.8211\t0.9091\n",
      "r_struct_no_col_score\t0.0841\t0.1212\n",
      "r_col_corrupt_all_score\t0.7374\t0.8139\n",
      "\n",
      "Column role: order by (57)\n",
      "[Example prompt] select song_name, song_release_year from singer order by\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9447\t1.0000\n",
      "r_text_score\t0.4849\t0.5263\n",
      "r_struct_score\t0.9077\t0.9649\n",
      "r_col_score\t0.8048\t0.8596\n",
      "r_struct_no_col_score\t-0.0102\t0.0000\n",
      "r_col_corrupt_all_score\t0.5500\t0.5789\n",
      "\n",
      "Column role: join (60)\n",
      "[Example prompt] select t2.countryname from car_makers as t1 join countries as t2 on t1.country = t2.\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.8718\t1.0000\n",
      "r_text_score\t0.0407\t0.0000\n",
      "r_struct_score\t0.8127\t0.8833\n",
      "r_col_score\t0.1594\t0.1667\n",
      "r_struct_no_col_score\t0.4611\t0.5000\n",
      "r_col_corrupt_all_score\t0.8877\t0.9500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in samples_by_column_role.keys():\n",
    "    samples = samples_by_column_role[r]\n",
    "    if len(samples) < 10:\n",
    "        continue\n",
    "    print(f'Column role: {r} ({len(samples)})')\n",
    "    print('[Example prompt]', samples[0]['dec_prompt'])\n",
    "    print(f'Score\\tavg_gain\\tperc_recover')\n",
    "    \n",
    "    low_scores = [d['low_score'] for d in samples]\n",
    "    for score_label in [\n",
    "        'base_score',\n",
    "        'r_text_score',\n",
    "        'r_struct_score',\n",
    "        'r_col_score',\n",
    "        'r_struct_no_col_score',\n",
    "        'r_col_corrupt_all_score'\n",
    "    ]:\n",
    "        high_scores = [d[score_label] for d in samples]\n",
    "        avg_gain = np.mean([h - l for h, l in zip(high_scores, low_scores)])\n",
    "        perc_recover = np.mean([h - l > 0.5 for h, l in zip(high_scores, low_scores)])\n",
    "        print(f'{score_label}\\t{avg_gain:.4f}\\t{perc_recover:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_path = '/home/yshao/Projects/rome/results/exp2_text_struct_interaction/exp=2_dev_column.jsonl'\n",
    "# with open(res_path, 'r') as f:\n",
    "#     all_samples = [json.loads(l) for l in f]\n",
    "# len(all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split by text match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('shop',\n",
       " 'number_products',\n",
       " 'Sort all the shops by number products in descending order, and return the name, location and district of each shop.',\n",
       " 'exact')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = good_samples[244]\n",
    "spider_ex = processed_spider_dev[d['ex_id']]\n",
    "col = d['expect']\n",
    "tab = d['table']\n",
    "d['table'], d['expect'], d['text_in'], ctu.check_text_match(spider_ex, col, tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exact', 553), ('partial', 159), ('no-match', 271)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_by_text_match = {k: [] for k in ['exact', 'partial', 'no-match']}\n",
    "\n",
    "for d in good_samples:\n",
    "    spider_ex = processed_spider_dev[d['ex_id']]\n",
    "    text_match = _check_text_match(spider_ex, d)\n",
    "    samples_by_text_match[text_match].append(d)\n",
    "\n",
    "[(m, len(samples)) for m, samples in samples_by_text_match.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text match: exact (553)\n",
      "[Example] Show name, country, age for all singers ordered by age from the oldest to the youngest.\n",
      "[Example] select name, country, *age*\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9501\t1.0000\n",
      "r_text_score\t0.7393\t0.7830\n",
      "r_struct_score\t0.7989\t0.8517\n",
      "r_col_score\t0.6998\t0.7468\n",
      "r_struct_no_col_score\t0.0679\t0.0723\n",
      "r_col_corrupt_all_score\t0.7193\t0.7595\n",
      "\n",
      "Text match: partial (159)\n",
      "[Example] Show the name and the release year of the song by the youngest singer.\n",
      "[Example] select song_name, *song_release_year*\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9513\t1.0000\n",
      "r_text_score\t0.2488\t0.2767\n",
      "r_struct_score\t0.7862\t0.8239\n",
      "r_col_score\t0.6924\t0.7484\n",
      "r_struct_no_col_score\t0.0420\t0.0692\n",
      "r_col_corrupt_all_score\t0.5945\t0.6415\n",
      "\n",
      "Text match: no-match (271)\n",
      "[Example] What is the average, minimum, and maximum age of all singers from France?\n",
      "[Example] select avg(age), min(age), max(age) from singer where *country*\n",
      "Score\tavg_gain\tperc_recover\n",
      "base_score\t0.9366\t1.0000\n",
      "r_text_score\t0.3370\t0.3469\n",
      "r_struct_score\t0.8375\t0.8893\n",
      "r_col_score\t0.6432\t0.7048\n",
      "r_struct_no_col_score\t0.1251\t0.1365\n",
      "r_col_corrupt_all_score\t0.7618\t0.8044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in ['exact', 'partial', 'no-match']:\n",
    "    samples = samples_by_text_match[m]\n",
    "    print(f'Text match: {m} ({len(samples)})')\n",
    "    print('[Example]', samples[0]['text_in'])\n",
    "    print(f\"[Example] {samples[0]['dec_prompt']} *{samples[0]['expect']}*\")\n",
    "    print(f'Score\\tavg_gain\\tperc_recover')\n",
    "    \n",
    "    low_scores = [d['low_score'] for d in samples]\n",
    "    for score_label in [\n",
    "        'base_score',\n",
    "        'r_text_score',\n",
    "        'r_struct_score',\n",
    "        'r_col_score',\n",
    "        'r_struct_no_col_score',\n",
    "        'r_col_corrupt_all_score'\n",
    "#         'm-dc_t-dc_s-score',\n",
    "    ]:\n",
    "        high_scores = [d[score_label] for d in samples]\n",
    "        avg_gain = np.mean([h - l for h, l in zip(high_scores, low_scores)])\n",
    "        perc_recover = np.mean([h - l > 0.5 for h, l in zip(high_scores, low_scores)])\n",
    "        print(f'{score_label}\\t{avg_gain:.4f}\\t{perc_recover:.4f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = samples_by_text_match['no-match'][50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What are the names of all European countries with at least 3 manufacturers? The database is as following: | car_1 | continents : contid , continent ( europe ) | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year => select t1.countryname from countries as t1 join continents as t2 on t1.continent = t2.contid join car_makers as t3 on t1.countryid = t3.country where t2.continent = 'europe' group by t1.countryname having count(*) >= 3;\""
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in} The database is as following: {struct_in}\"\n",
    "\n",
    "f\"{enc_sentence} => {ex['seq_out']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['ex_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-2.2: dirty text struct restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 983)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_json_path = f'/home/yshao/Projects/rome/results/exp2.2_dirty_text_struct_restore/exp=2.2_dev_column.jsonl'\n",
    "\n",
    "with open(res_json_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "\n",
    "good_trace_results = []\n",
    "for ex in all_samples:\n",
    "    for r in ex['trace_results']:\n",
    "        if r['is_good_sample']:\n",
    "            r['ex_id'] = ex['ex_id']\n",
    "            good_trace_results.append(r)\n",
    "    \n",
    "len(all_samples), len(good_trace_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_trace_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generating all plots\n",
    "# fig_save_dir = f'/home/yshao/Projects/rome/results/figs/exp2.2_dirty_text_struct_restore/exp=2.2_dev_column'\n",
    "\n",
    "# for i, r in enumerate(tqdm(good_trace_results)):\n",
    "#     result = dict(r)\n",
    "\n",
    "#     enc_s, dec_s = result['scores']\n",
    "    \n",
    "#     # enc_s = np.array(enc_s)\n",
    "#     enc_layers_vec = np.array(enc_s[0])\n",
    "#     assert enc_layers_vec.shape == (mt_uskg.num_enc_layers,)\n",
    "#     enc_token_range, = result['enc_token_range']\n",
    "    \n",
    "#     enc_s = np.zeros((len(result['input_tokens']), mt_uskg.num_enc_layers))\n",
    "#     enc_s[enc_token_range] = enc_layers_vec\n",
    "    \n",
    "#     dec_s = np.zeros((len(result['dec_input_tokens']), mt_uskg.num_dec_layers))\n",
    "#     result['scores'] = [enc_s, dec_s]\n",
    "\n",
    "#     ex_id = r['ex_id']\n",
    "#     ctu.plot_trace_heatmap_t5(result, savepdf=os.path.join(fig_save_dir, f'{i}-ex_id={ex_id}.pdf'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check avg per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_per_layer = [[] for _ in range(mt_uskg.num_enc_layers)]\n",
    "\n",
    "for r in good_trace_results:\n",
    "    enc_s, dec_s = r['scores']\n",
    "    for l, s in enumerate(enc_s[0]):\n",
    "        scores_per_layer[l].append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_by_layer = [np.mean(scores) for scores in scores_per_layer]\n",
    "avg_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Layer\\tAvg. Score')\n",
    "for l, avg in enumerate(avg_by_layer):\n",
    "    print(f'{l:<5d}\\t{avg:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(avg_by_layer, label='All')\n",
    "ax.set_xlabel('Layer to restore')\n",
    "ax.set_ylabel('Avg. P(correct answer)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  by hardness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {h : [[score], ...]}\n",
    "scores_by_hardness = {hardness : [] for hardness in ['easy', 'medium', 'hard', 'extra']}   \n",
    "\n",
    "for r in good_trace_results:\n",
    "    enc_s, dec_s = r['scores']\n",
    "    scores, = enc_s\n",
    "    hardness = spider_id2hardness[r['ex_id']]\n",
    "    scores_by_hardness[hardness].append(scores)\n",
    "\n",
    "[(h, len(scores)) for h, scores in scores_by_hardness.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h, sample_scores in scores_by_hardness.items():\n",
    "    print(h)\n",
    "    scores_by_layer = zip(*sample_scores)\n",
    "    avg_by_layer = [np.mean(scores) for scores in scores_by_layer]\n",
    "    print([f'{avg:.2f}' for avg in avg_by_layer])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(avg_by_layer, label='All')\n",
    "for h, sample_scores in scores_by_hardness.items():\n",
    "    h_scores_by_layer = zip(*sample_scores)\n",
    "    h_avg_by_layer = [np.mean(scores) for scores in h_scores_by_layer]\n",
    "    ax.plot(h_avg_by_layer, label=h)\n",
    "ax.set_xlabel('Layer to restore')\n",
    "ax.set_ylabel('Avg. P(correct answer)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  by column role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {h : [[score], ...]}\n",
    "scores_by_column_role = defaultdict(list)\n",
    "\n",
    "for r in good_trace_results:\n",
    "    enc_s, dec_s = r['scores']\n",
    "    scores, = enc_s\n",
    "    role = _detect_column_role(' '.join(r['dec_input_tokens']))\n",
    "    scores_by_column_role[role].append(scores)\n",
    "\n",
    "[(r, len(scores)) for r, scores in scores_by_column_role.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, sample_scores in scores_by_column_role.items():\n",
    "    if len(sample_scores) < 10:\n",
    "        continue\n",
    "    print(r)\n",
    "    scores_by_layer = zip(*sample_scores)\n",
    "    avg_by_layer = [np.mean(scores) for scores in scores_by_layer]\n",
    "    print([f'{avg:.2f}' for avg in avg_by_layer])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(avg_by_layer, label='All')\n",
    "for r, sample_scores in scores_by_column_role.items():\n",
    "    if len(sample_scores) < 10:\n",
    "        continue\n",
    "    r_scores_by_layer = zip(*sample_scores)\n",
    "    r_avg_by_layer = [np.mean(scores) for scores in r_scores_by_layer]\n",
    "    ax.plot(r_avg_by_layer, label=r)\n",
    "ax.set_xlabel('Layer to restore')\n",
    "ax.set_ylabel('Avg. P(correct answer)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### by text matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_by_text_matching = {k: [] for k in ['exact', 'partial', 'no-match']}\n",
    "\n",
    "for r in good_trace_results:\n",
    "    spider_ex = processed_spider_dev[r['ex_id']]\n",
    "    col = r['target_node']\n",
    "    tab = r['target_node_table']\n",
    "    m = ctu.check_text_match(spider_ex, col, tab)\n",
    "\n",
    "    enc_s, dec_s = r['scores']\n",
    "    scores, = enc_s\n",
    "    scores_by_text_matching[m].append(scores)\n",
    "\n",
    "[(m, len(samples)) for m, samples in scores_by_text_matching.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, sample_scores in scores_by_text_matching.items():\n",
    "    if len(sample_scores) < 10:\n",
    "        continue\n",
    "    print(r)\n",
    "    scores_by_layer = zip(*sample_scores)\n",
    "    avg_by_layer = [np.mean(scores) for scores in scores_by_layer]\n",
    "    print([f'{avg:.2f}' for avg in avg_by_layer])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(avg_by_layer, label='All')\n",
    "for m, sample_scores in scores_by_text_matching.items():\n",
    "    if len(sample_scores) < 10:\n",
    "        continue\n",
    "    r_scores_by_layer = zip(*sample_scores)\n",
    "    r_avg_by_layer = [np.mean(scores) for scores in r_scores_by_layer]\n",
    "    ax.plot(r_avg_by_layer, label=m)\n",
    "ax.set_xlabel('Layer to restore')\n",
    "ax.set_ylabel('Avg. P(correct answer)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USKG error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from play_pred()\n",
    "\n",
    "def pred_sql(mt, ex):\n",
    "    text_in = ex['text_in']\n",
    "    struct_in = ex['struct_in']\n",
    "\n",
    "    txt = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "    \n",
    "    tokenized_txt = mt.tokenizer_uskg([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    device = mt.model.device\n",
    "    pred = mt.tokenizer_uskg.batch_decode(\n",
    "      mt.model.generate(\n",
    "        torch.tensor(tokenized_txt.data['input_ids'], dtype=int, device=device),\n",
    "        torch.tensor(tokenized_txt.data['attention_mask'], dtype=int, device=device),\n",
    "        num_beams=1, \n",
    "        max_length=256\n",
    "        ), \n",
    "      skip_special_tokens=True \n",
    "    )\n",
    "    return pred[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from evaluator.evaluate_one()\n",
    "\n",
    "def evaluate_sql(evaluator, db_name, gold, predicted):\n",
    "    schema = evaluator.schemas[db_name]\n",
    "    g_sql = sp_eval.get_sql(schema, gold)\n",
    "    hardness = evaluator.eval_hardness(g_sql)\n",
    "    # self.scores[hardness][\"count\"] += 1\n",
    "    # self.scores[\"all\"][\"count\"] += 1\n",
    "\n",
    "    parse_error = False\n",
    "    try:\n",
    "        p_sql = sp_eval.get_sql(schema, predicted)\n",
    "    except:\n",
    "        # If p_sql is not valid, then we will use an empty sql to evaluate with the correct sql\n",
    "        p_sql = {\n",
    "            \"except\": None,\n",
    "            \"from\": {\"conds\": [], \"table_units\": []},\n",
    "            \"groupBy\": [],\n",
    "            \"having\": [],\n",
    "            \"intersect\": None,\n",
    "            \"limit\": None,\n",
    "            \"orderBy\": [],\n",
    "            \"select\": [False, []],\n",
    "            \"union\": None,\n",
    "            \"where\": [],\n",
    "        }\n",
    "\n",
    "        # TODO fix\n",
    "        parse_error = True\n",
    "\n",
    "    # rebuild sql for value evaluation\n",
    "    kmap = evaluator.kmaps[db_name]\n",
    "    g_valid_col_units = sp_eval.build_valid_col_units(g_sql[\"from\"][\"table_units\"], schema)\n",
    "    g_sql = sp_eval.rebuild_sql_val(g_sql)\n",
    "    g_sql = sp_eval.rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n",
    "    p_valid_col_units = sp_eval.build_valid_col_units(p_sql[\"from\"][\"table_units\"], schema)\n",
    "    p_sql = sp_eval.rebuild_sql_val(p_sql)\n",
    "    p_sql = sp_eval.rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n",
    "    \n",
    "    exec_score = None\n",
    "    partial_scores = None\n",
    "    exact_score = None\n",
    "    if evaluator.etype in [\"all\", \"exec\"]:\n",
    "        exec_score = sp_eval.eval_exec_match(\n",
    "            evaluator.db_paths[db_name], predicted, gold, p_sql, g_sql\n",
    "        )\n",
    "        exec_score = int(exec_score)\n",
    "    if evaluator.etype in [\"all\", \"match\"]:\n",
    "        partial_scores = evaluator.eval_partial_match(p_sql, g_sql)\n",
    "        exact_score = evaluator.eval_exact_match(p_sql, g_sql, partial_scores)\n",
    "        # update_scores_match(self.scores, exact_score, hardness, partial_scores, PARTIAL_TYPES)\n",
    "\n",
    "    return {\n",
    "        \"predicted\": predicted,\n",
    "        \"gold\": gold,\n",
    "        \"predicted_parse_error\": parse_error,\n",
    "        \"hardness\": hardness,\n",
    "        \"exact\": exact_score,\n",
    "        \"partial\": partial_scores,\n",
    "        \"exec\": exec_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'\n",
    "\n",
    "def execute_sql(db, sql_str):\n",
    "    db_path = os.path.join(db_dir, db, f'{db}.sqlite')\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(sql_str)\n",
    "        res = cursor.fetchall()\n",
    "    except:\n",
    "        res = 'ERROR'\n",
    "    conn.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph', 'serialized_schema', 'struct_in', 'text_in', 'seq_out'])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = processed_spider_dev[503]\n",
    "ex.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('battle_death',\n",
       " \"How many battles did not lose any ship with tonnage '225'?\",\n",
       " \"select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage = '225' );\",\n",
       " 'select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage > 225 )')"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred_sql(mt_uskg, ex)\n",
    "ex['db_id'], ex['text_in'], ex['seq_out'], pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "txt = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "txt_toks = mt_uskg.tokenizer_uskg.tokenize(txt)\n",
    "len(txt_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage = '225' );\",\n",
       " [(7,)])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_res = execute_sql(ex['db_id'], ex['seq_out'])\n",
    "ex['seq_out'], exec_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage > 225 )',\n",
       " [(3,)])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_res = execute_sql(ex['db_id'], pred)\n",
    "pred, exec_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8,)]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_pred = \"\"\"\n",
    "SELECT COUNT(DISTINCT battle.id) AS num_battles\n",
    "FROM battle\n",
    "LEFT JOIN ship ON battle.id = ship.lost_in_battle\n",
    "WHERE (ship.tonnage != '225' OR ship.tonnage IS NULL)\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(ex['db_id'], chatgpt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_res = evaluate_sql(evaluator, db_name=spider_ex['db_id'], gold=spider_ex['seq_out'], predicted=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted': 'select avg(num_of_staff) from museum where open_year  < 2009',\n",
       " 'gold': 'select avg(num_of_staff) from museum where open_year < 2009',\n",
       " 'predicted_parse_error': False,\n",
       " 'hardness': 'easy',\n",
       " 'exact': True,\n",
       " 'partial': {'select': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 1},\n",
       "  'select(no AGG)': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 1},\n",
       "  'where': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 1, 'pred_total': 1},\n",
       "  'where(no OP)': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 1},\n",
       "  'group(no Having)': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 0,\n",
       "   'pred_total': 0},\n",
       "  'group': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 0, 'pred_total': 0},\n",
       "  'order': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 0, 'pred_total': 0},\n",
       "  'and/or': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 1, 'pred_total': 1},\n",
       "  'IUEN': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 0, 'pred_total': 0},\n",
       "  'keywords': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 1}},\n",
       " 'exec': 1}"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab76cae127742adbc790384e85093b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Eval all \n",
    "# TODO: rerun with tokenizer_uskg decoding \n",
    "\n",
    "eval_sql_results = []\n",
    "\n",
    "for ex_id, ex in enumerate(tqdm(processed_spider_dev)):\n",
    "    pred = pred_sql(mt_uskg, ex)\n",
    "    eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "    eval_sql_results.append(eval_res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6692456479690522, 0.6808510638297872)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "avg_exact, avg_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(eval_sql_results):\n",
    "    if d['exact'] and d['exec']:\n",
    "        continue\n",
    "    err_msg = ('A' if not d['exact'] else '') + ('X' if not d['exec'] else '')\n",
    "    ex = processed_spider_dev[i]\n",
    "    print(f'ID = {i}: {err_msg}  ({ex[\"db_id\"]}) {ex[\"text_in\"]}')\n",
    "    print(f'Pred: {d[\"predicted\"]}')\n",
    "    print(f'Gold: {d[\"gold\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator.schemas['dog_kennels'].schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acc by db_id \n",
    "eval_sql_results_by_db_id = defaultdict(list)\n",
    "\n",
    "for i, d in enumerate(eval_sql_results):\n",
    "    d['ex_id'] = i\n",
    "    ex = processed_spider_dev[i]\n",
    "    db_id = ex['db_id']\n",
    "    eval_sql_results_by_db_id[db_id].append(d)\n",
    "\n",
    "len(eval_sql_results_by_db_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concert_singer\t0.8889\t0.8889\n",
      "pets_1\t0.5714\t0.7381\n",
      "car_1\t0.3478\t0.3913\n",
      "flight_2\t0.7000\t0.7500\n",
      "employee_hire_evaluation\t0.9474\t0.9737\n",
      "cre_Doc_Template_Mgt\t0.8333\t0.9048\n",
      "course_teach\t0.8667\t0.9333\n",
      "museum_visit\t0.7222\t0.8333\n",
      "wta_1\t0.6774\t0.6129\n",
      "battle_death\t0.5000\t0.5000\n",
      "student_transcripts_tracking\t0.6667\t0.6795\n",
      "tvshow\t0.7258\t0.6613\n",
      "poker_player\t0.8750\t0.8750\n",
      "voter_1\t0.6000\t0.6667\n",
      "world_1\t0.5083\t0.4833\n",
      "orchestra\t0.8000\t0.8750\n",
      "network_1\t0.6250\t0.4643\n",
      "dog_kennels\t0.5854\t0.5976\n",
      "singer\t0.8667\t0.8667\n",
      "real_estate_properties\t0.5000\t0.5000\n"
     ]
    }
   ],
   "source": [
    "for db_id, results in eval_sql_results_by_db_id.items():\n",
    "    _avg_exact = np.mean([d['exact'] for d in results])\n",
    "    _avg_exec = np.mean([d['exec'] for d in results])\n",
    "    print(f'{db_id}\\t{_avg_exact:.4f}\\t{_avg_exec:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = processed_spider_dev[97]\n",
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "dec_prompt = \"select t1.model from\"\n",
    "expect = \"car_names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    enc_sentences=[enc_sentence]*11,\n",
    "    dec_prompts=[dec_prompt]*11,\n",
    "    answer=expect\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', '_', 'name', 's']"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_toks = decode_tokens(mt_uskg.tokenizer, mt_uskg.tokenizer.encode(expect, add_special_tokens=False))\n",
    "ans_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> select t1.model from car_name'"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.decode(inp['decoder_input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['select', '', 't', '1.', 'model', 'from'], 6)"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.tokenize(dec_prompt), len(mt_uskg.tokenizer.tokenize(dec_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = ctu.trace_with_patch_uskg_multi_token(\n",
    "#     mt_uskg.model,\n",
    "#     inp=inp,\n",
    "# #     states_to_patch=[(4, ctu.layername_uskg(mt_uskg.model, 'decoder', 3))],\n",
    "#     states_to_patch=[],\n",
    "# #     answers_t=mt_uskg.tokenizer.encode(expect, add_special_tokens=False),\n",
    "# #     tokens_to_mix=(0, len(inp['input_ids'][0])-1),\n",
    "#     tokens_to_mix=None,\n",
    "#     replace=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.5643, device='cuda:0'),\n",
       " tensor(1.0000, device='cuda:0'),\n",
       " tensor(0.9996, device='cuda:0'),\n",
       " tensor(1.0000, device='cuda:0')]"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 443,  834, 4350,    7], device='cuda:0')"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_len = len(mt_uskg.tokenizer.tokenize(expect))\n",
    "pred_out = ctu.predict_from_input_uskg_multi_token(mt_uskg.model, inp, pred_len=answer_len)\n",
    "pred, p = pred_out\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model', 'car'), ('_', '_'), ('name', 'name'), ('s', 's')]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_toks = decode_tokens(mt_uskg.tokenizer, pred[0])\n",
    "# list(zip(pred_toks, ans_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('car_1',\n",
       " 'Find the model of the car whose weight is below the average weight.',\n",
       " '| car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " 'select t1.model from car_names as t1 join cars_data as t2 on t1.makeid = t2.id where t2.weight < (select avg(weight) from cars_data)')"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['db_id'], ex['question'], ex['struct_in'], ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = ctu.trace_with_repatch_uskg_multi_token(\n",
    "    mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=[(4, ctu.layername_uskg(mt_uskg.model, 'decoder', 3))],\n",
    "    states_to_unpatch=[(4, ctu.layername_uskg(mt_uskg.model, 'decoder', 4, 'cross_attn'))],\n",
    "    states_to_patch_1st_pass=[(4, ctu.layername_uskg(mt_uskg.model, 'decoder', l)) for l in range(mt_uskg.num_enc_layers)],\n",
    "    answers_t=pred[0],\n",
    "    tokens_to_mix=(10, 20, 30, 40),\n",
    "    tokens_to_mix_1st_pass=(5, 15, 25),\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.5877, device='cuda:0'),\n",
       " tensor(1., device='cuda:0'),\n",
       " tensor(0.9988, device='cuda:0'),\n",
       " tensor(1.0000, device='cuda:0')]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inp = ctu.make_inputs_t5(mt_uskg.tokenizer, [text_in] * 11, [dec_prompt] * 11)\n",
    "# inp = ctu.make_inputs_t5(mt_uskg.tokenizer, [enc_sentence] * 11, [dec_prompt] * 11, answer=expect, device='cpu')\n",
    "# # answer_t, base_score = [d[0] for d in ctu.predict_from_input_uskg(mt_uskg.model, inp)]\n",
    "# # base_score = base_score.item()\n",
    "# # [answer] = ctu.decode_tokens(mt_uskg.tokenizer, [answer_t])\n",
    "# answer_len = 1\n",
    "# if expect is not None:\n",
    "#     answer_len = len(mt_uskg.tokenizer.tokenize(expect))\n",
    "# with torch.no_grad():\n",
    "#     answers_t, base_score = [d[0] for d in ctu.predict_from_input_uskg_multi_token(mt_uskg.model, inp, pred_len=answer_len)]\n",
    "# # base_score = base_score.min().item()\n",
    "# # [answer] = decode_tokens(mt.tokenizer, [answer_t])\n",
    "# answer = ctu.decode_sentences(mt_uskg.tokenizer, answers_t)\n",
    "\n",
    "# expect, answers_t, answer, base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # e_range = (129, 132)\n",
    "# # e_range = [8, 13, 131]\n",
    "# e_range = list(range(7, 14))\n",
    "\n",
    "# r = ctu.trace_with_patch_uskg(\n",
    "#     mt_uskg.model,\n",
    "#     inp=inp,\n",
    "#     states_to_patch=[], \n",
    "#     answers_t=answer_t, \n",
    "#     tokens_to_mix=e_range,\n",
    "#     tokens_to_mix_individual_indices=True,\n",
    "#     replace=True,\n",
    "# )\n",
    "\n",
    "# r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = mt_uskg.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['mar', 'y', ':', 'has', '', 'a', 'little', 'lamb', 'b', 'b'],\n",
       " ['mar', 'y', ':', 'has', '', 'a', 'little', 'lamb', 'b', 'b'])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"mary: has a little  lambbb\"\n",
    "s_ = \"mary: has a little lambbb\"\n",
    "tokenizer.tokenize(s), tokenizer.tokenize(s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3157, 63, 10, 65, 3, 9, 385, 17871, 115, 115, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(s)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'mar'),\n",
       " (1, 'y'),\n",
       " (2, ':'),\n",
       " (3, 'has'),\n",
       " (4, ''),\n",
       " (5, 'a'),\n",
       " (6, 'little'),\n",
       " (7, 'lamb'),\n",
       " (8, 'b'),\n",
       " (9, 'b'),\n",
       " (10, '</s>')]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(t.tokens()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenSpan(start=7, end=10)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_to_tokens(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lambbb'"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.decode_sentences(tokenizer, t['input_ids'][7:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"(a(a)a)\".rindex(\")\"), \"(a(a)a)\".index(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test struct_in parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "_struct_in = \"\"\"| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , \\\n",
    "average | singer : singer_id , name ( First Last ) , country ( France , Germany , United States ) , \\\n",
    "song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , \\\n",
    "stadium_id , year ( 2008 , 2012 , 2022 ) | singer_in_concert : concert_id , singer_id\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 'stadium'),\n",
       "  [[(5, 'stadium_id'), []],\n",
       "   [(7, 'location'), []],\n",
       "   [(9, 'name'), []],\n",
       "   [(11, 'capacity'), []],\n",
       "   [(13, 'highest'), []],\n",
       "   [(15, 'lowest'), []],\n",
       "   [(17, 'average'), []]]),\n",
       " ((19, 'singer'),\n",
       "  [[(21, 'singer_id'), []],\n",
       "   [(23, 'name'), [(25, 'First Last')]],\n",
       "   [(29, 'country'), [(31, 'France'), (33, 'Germany'), (35, 'United States')]],\n",
       "   [(39, 'song_name'), []],\n",
       "   [(41, 'song_release_year'), []],\n",
       "   [(43, 'age'), []],\n",
       "   [(45, 'is_male'), []]]),\n",
       " ((47, 'concert'),\n",
       "  [[(49, 'concert_id'), []],\n",
       "   [(51, 'concert_name'), []],\n",
       "   [(53, 'theme'), []],\n",
       "   [(55, 'stadium_id'), []],\n",
       "   [(57, 'year'), [(59, '2008'), (61, '2012'), (63, '2022')]]]),\n",
       " ((66, 'singer_in_concert'),\n",
       "  [[(68, 'concert_id'), []], [(70, 'singer_id'), []]])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.parse_struct_in(_struct_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "_text_in = text_in\n",
    "\n",
    "enc_sentence = f\"{_text_in}; structed knowledge: {_struct_in}\"\n",
    "enc_tokenized = mt_uskg.tokenizer(enc_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ranges_dict = ctu.find_struct_name_ranges(mt_uskg.tokenizer, enc_tokenized['input_ids'], _struct_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id_ranges': defaultdict(list, {'concert_singer': [(25, 29)]}),\n",
       " 'table_name_ranges': defaultdict(list,\n",
       "             {'stadium': [(30, 31)],\n",
       "              'singer': [(56, 57)],\n",
       "              'concert': [(106, 107)],\n",
       "              'singer_in_concert': [(142, 149)]}),\n",
       " 'col_name_ranges': defaultdict(list,\n",
       "             {'stadium_id': [(33, 37), (123, 127)],\n",
       "              'location': [(39, 40)],\n",
       "              'name': [(42, 43), (65, 66)],\n",
       "              'capacity': [(45, 46)],\n",
       "              'highest': [(48, 49)],\n",
       "              'lowest': [(51, 52)],\n",
       "              'average': [(54, 55)],\n",
       "              'singer_id': [(59, 63), (157, 161)],\n",
       "              'country': [(73, 74)],\n",
       "              'song_name': [(87, 90)],\n",
       "              'song_release_year': [(92, 97)],\n",
       "              'age': [(99, 100)],\n",
       "              'is_male': [(102, 105)],\n",
       "              'concert_id': [(109, 113), (151, 155)],\n",
       "              'concert_name': [(115, 118)],\n",
       "              'theme': [(120, 121)],\n",
       "              'year': [(129, 130)]}),\n",
       " 'val_name_ranges': defaultdict(list,\n",
       "             {'First Last': [(67, 69)],\n",
       "              'France': [(75, 76)],\n",
       "              'Germany': [(78, 79)],\n",
       "              'United States': [(81, 83)],\n",
       "              '2008': [(131, 132)],\n",
       "              '2012': [(134, 135)],\n",
       "              '2022': [(137, 139)]})}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ranges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db_id_ranges\tconcert_singer\tconcert_singer\n",
      "table_name_ranges\tstadium\tstadium\n",
      "table_name_ranges\tsinger\tsinger\n",
      "table_name_ranges\tconcert\tconcert\n",
      "table_name_ranges\tsinger_in_concert\tsinger_in_concert\n",
      "col_name_ranges\tstadium_id\tstadium_id\n",
      "col_name_ranges\tstadium_id\tstadium_id\n",
      "col_name_ranges\tlocation\tlocation\n",
      "col_name_ranges\tname\tname\n",
      "col_name_ranges\tname\tname\n",
      "col_name_ranges\tcapacity\tcapacity\n",
      "col_name_ranges\thighest\thighest\n",
      "col_name_ranges\tlowest\tlowest\n",
      "col_name_ranges\taverage\taverage\n",
      "col_name_ranges\tsinger_id\tsinger_id\n",
      "col_name_ranges\tsinger_id\tsinger_id\n",
      "col_name_ranges\tcountry\tcountry ( France )\n",
      "col_name_ranges\tsong_name\tsong_name\n",
      "col_name_ranges\tsong_release_year\tsong_release_year\n",
      "col_name_ranges\tage\tage\n",
      "col_name_ranges\tis_male\tis_male\n",
      "col_name_ranges\tconcert_id\tconcert_id\n",
      "col_name_ranges\tconcert_id\tconcert_id\n",
      "col_name_ranges\tconcert_name\tconcert_name\n",
      "col_name_ranges\ttheme\ttheme\n",
      "col_name_ranges\tyear\tyear\n",
      "val_name_ranges\tFrance\tFrance\n"
     ]
    }
   ],
   "source": [
    "for d_key, d in token_ranges_dict.items():\n",
    "    for name, ranges in d.items():\n",
    "        for s, e in ranges:\n",
    "            recs_name = ctu.decode_sentences(mt_uskg.tokenizer, enc_tokenized['input_ids'][s:e])\n",
    "            print(f'{d_key}\\t{name}\\t{recs_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: cars \tlemma: car\n",
      "word: data \tlemma: datum\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('cars data')\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"id\": 1,\n",
       "  \"text\": \"cars\",\n",
       "  \"lemma\": \"car\",\n",
       "  \"upos\": \"NOUN\",\n",
       "  \"xpos\": \"NNS\",\n",
       "  \"feats\": \"Number=Plur\",\n",
       "  \"start_char\": 0,\n",
       "  \"end_char\": 4\n",
       "}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('cars data')\n",
    "doc.sentences[0].words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stanza_nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemm_cache = dict()\n",
    "\n",
    "# def _lemmatize(name):\n",
    "# #     toks = []\n",
    "# #     for t in name.split('_'):\n",
    "# #         lemm_t = lemm_cache.get(t, None)\n",
    "# #         if lemm_t is None:\n",
    "# #             _doc = stanza_nlp(t)\n",
    "# #             lemm_t = _doc.sentences[0].words[0].lemma\n",
    "# #             lemm_cache[t] = lemm_t\n",
    "# #         toks.append(lemm_t)\n",
    "# #     return '_'.join(toks)\n",
    "#     _name = ' '.join(name.split('_'))\n",
    "#     _doc = stanza_nlp(_name)\n",
    "#     lemm_name = [w.lemma for sent in _doc.sentences for w in sent.words]\n",
    "#     return lemm_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _lemmatize('ranking_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _check_text_match(spider_ex, result_d):\n",
    "#     col = result_d['expect']\n",
    "#     tab = result_d['table']\n",
    "#     node_name = f'<C>{tab}::{col}'\n",
    "#     nodes = spider_ex['rat_sql_graph']['nodes']\n",
    "    \n",
    "#     if node_name in nodes:\n",
    "#         node_idx = nodes.index(node_name)\n",
    "#     else:\n",
    "#         _col = _lemmatize(col)\n",
    "#         _tab = _lemmatize(tab)\n",
    "#         _node_name = f'<C>{_tab}::{_col}'\n",
    "# #         if col != _col:\n",
    "# #             print(f'* Lemmatize: {node_name} ==> {_node_name}')\n",
    "#         if _node_name in nodes:\n",
    "#             node_idx = nodes.index(_node_name)\n",
    "#         else:\n",
    "#             assert False, ((node_name, _node_name), nodes)\n",
    "    \n",
    "#     rel_matrix = json.loads(spider_ex['rat_sql_graph']['relations'])\n",
    "#     rel_row = rel_matrix[node_idx]\n",
    "    \n",
    "#     if REL2ID['cqCEM'] in rel_row:\n",
    "#         return 'exact'\n",
    "#     elif REL2ID['cqCPM'] in rel_row:\n",
    "#         return 'partial'\n",
    "#     else:\n",
    "#         return 'no-match'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer-fast / uskg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = processed_spider_dev[416]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  1738,     3,     9,   208,   122,   599,  5525,   834,   858,\n",
       "           834, 26416,    61,    45,  7071,   213,   539,   834,  1201, 32100,\n",
       "          2464,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: add special token (<, <=) to t5-tokenizer \n",
    "# could use uskg tokenizer, but it's not TokenizerFast \n",
    "\n",
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "txt = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "\n",
    "tokenized_txt = mt_uskg.tokenizer([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "\n",
    "device = mt_uskg.model.device\n",
    "_output = mt_uskg.model.generate(\n",
    "    torch.tensor(tokenized_txt.data['input_ids'], dtype=int, device=device),\n",
    "    torch.tensor(tokenized_txt.data['attention_mask'], dtype=int, device=device),\n",
    "    num_beams=1, \n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select avg(num_of_staff) from museum where open_year 2009']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.batch_decode(\n",
    "    _output,\n",
    "    skip_special_tokens=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select avg(num_of_staff) from museum where open_year  < 2009']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer_uskg.batch_decode(\n",
    "    _output,\n",
    "    skip_special_tokens=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input for chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = processed_spider_dev[503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many battles did not lose any ship with tonnage '225'?; structed knowledge: | battle_death | battle : id , name , date , bulgarian_commander , latin_commander , result | ship : lost_in_battle , id , name , tonnage , ship_type , location , disposition_of_ship | death : caused_by_ship_id , id , note , killed , injured => select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage = '225' );\n"
     ]
    }
   ],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "\n",
    "print(f\"{enc_sentence} => {ex['seq_out']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-time patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For exp2: ['trace_results'][.]: ['%col%_score'] -> ['%node%_score']\n",
    "\n",
    "# res_dir = '/home/yshao/Projects/rome/results/exp2_text_struct_interaction/'\n",
    "# in_path = os.path.join(res_dir, 'exp=2_train_column-tmp.jsonl')\n",
    "# out_path = os.path.join(res_dir, 'exp=2_train_column.jsonl')\n",
    "\n",
    "# with open(in_path, 'r') as f:\n",
    "#     all_results = [json.loads(l) for l in f]\n",
    "\n",
    "# for ex_d in all_results:\n",
    "#     for d in ex_d['trace_results']:\n",
    "#         if not d['is_good_sample']:\n",
    "#             continue\n",
    "#         d['r_node_score'] = d['r_col_score']\n",
    "#         d['r_struct_no_node_score'] = d['r_struct_no_col_score']\n",
    "#         d['r_node_corrupt_all_score'] = d['r_col_corrupt_all_score']\n",
    "#         del d['r_col_score']\n",
    "#         del d['r_struct_no_col_score']\n",
    "#         del d['r_col_corrupt_all_score']\n",
    "\n",
    "# all_results.sort(key=lambda d: d['ex_id'])\n",
    "        \n",
    "# with open(out_path, 'w') as f:\n",
    "#     for d in all_results:\n",
    "#         f.write(json.dumps(d, indent=None) + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor(3, dtype=int)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "706px",
    "left": "31px",
    "top": "319px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

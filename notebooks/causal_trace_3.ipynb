{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing\n",
    "\n",
    "A demonstration of the double-intervention causal tracing method.\n",
    "\n",
    "The strategy used by causal tracing is to understand important\n",
    "states within a transfomer by doing two interventions simultaneously:\n",
    "\n",
    "1. Corrupt a subset of the input.  In our paper, we corrupt the subject tokens\n",
    "   to frustrate the ability of the transformer to accurately complete factual\n",
    "   prompts about the subject.\n",
    "2. Restore a subset of the internal hidden states.  In our paper, we scan\n",
    "   hidden states at all layers and all tokens, searching for individual states\n",
    "   that carry the necessary information for the transformer to recover its\n",
    "   capability to complete the factual prompt.\n",
    "\n",
    "The traces of decisive states can be shown on a heatmap.  This notebook\n",
    "demonstrates the code for conducting causal traces and creating these heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `experiments.causal_trace` module contains a set of functions for running causal traces.\n",
    "\n",
    "In this notebook, we reproduce, demonstrate and discuss the interesting functions.\n",
    "\n",
    "We begin by importing several utility functions that deal with tokens and transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json\n",
    "import string\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import defaultdict, Counter\n",
    "from util import nethook\n",
    "from util.globals import DATA_DIR\n",
    "from experiments.causal_trace import (\n",
    "    ModelAndTokenizer,\n",
    "    layername,\n",
    "    guess_subject,\n",
    "    plot_trace_heatmap,\n",
    ")\n",
    "from experiments.causal_trace import (\n",
    "    make_inputs,\n",
    "    decode_tokens,\n",
    "    find_token_range,\n",
    "    predict_token,\n",
    "    predict_from_input,\n",
    "    collect_embedding_std,\n",
    ")\n",
    "from dsets import KnownsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f79bc4e45e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# from uskg.models.unified.prefixtuning import Model\n",
    "from uskg.models.unified import finetune, prefixtuning\n",
    "from uskg.utils.configue import Configure\n",
    "from uskg.utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "from uskg.seq2seq_construction import spider as s2s_spider\n",
    "from uskg.third_party.spider.preprocess.get_tables import dump_db_json_schema\n",
    "from uskg.third_party.spider import evaluation as sp_eval\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# import stanza\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "\n",
    "from experiments import causal_trace_uskg as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer_uskg: hkunlp/from_all_T5_large_prefix_spider_with_cell_value2\n",
      "Using tokenizer_fast: t5-large\n",
      "prefix-tuning sequence length is 10.\n"
     ]
    }
   ],
   "source": [
    "mt_uskg = ctu.ModelAndTokenizer_USKG('t5-large-prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('constructor', 'seq2seq_construction.spider'),\n",
       " ('schema_serialization_with_db_content', True),\n",
       " ('target_with_db_id', False)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mt_uskg.task_args.seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.model.pretrain_model.encoder.embed_tokens is mt_uskg.model.pretrain_model.shared, \\\n",
    "mt_uskg.model.pretrain_model.decoder.embed_tokens is mt_uskg.model.pretrain_model.shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.model.preseqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [k for k,v in mt_uskg.model.named_parameters()]\n",
    "# [k for k,v in mt_uskg.model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    enc_sentences=[\"Translate to German: My name is Wolfgang and I live in Berlin\"],\n",
    "    dec_prompts=[\"Mein Name ist Wolfgang\"],\n",
    "    device=\"cuda:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ctu.run_model_forward_uskg(mt_uskg.model, **inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(odict_keys(['logits', 'past_key_values', 'encoder_last_hidden_state']),\n",
       " torch.Size([1, 5, 32102]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys(), out['logits'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32102,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = out[\"logits\"][0, -1].detach().cpu().numpy()\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, -1.8642352),\n",
       " (6, -9.727753),\n",
       " (5, -10.966707),\n",
       " (27, -11.037394),\n",
       " (213, -12.864212)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5 = sorted(list(enumerate(logits)), key=lambda p: -p[1])[:5]\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', ',', '.', 'I', 'where']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mt_uskg.tokenizer.decode([p[0]]) for p in top_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spider dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_train_path = '/home/yshao/Projects/SDR-analysis/data/spider/train+ratsql_graph.json'\n",
    "spider_dev_path = '/home/yshao/Projects/SDR-analysis/data/spider/dev+ratsql_graph.json'\n",
    "spider_db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_spider_dev = ctu.load_raw_dataset(\n",
    "    data_filepath = spider_dev_path,\n",
    "    db_path=spider_db_dir,\n",
    "#     schema_cache=SCHEMA_CACHE\n",
    ")\n",
    "len(raw_spider_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_spider_dev[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.task_args.dataset.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_spider_dev = s2s_spider.DevDataset(\n",
    "    args=mt_uskg.task_args,\n",
    "    raw_datasets=raw_spider_dev,\n",
    "    cache_root='../cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What are the names of all European countries with at least 3 manufacturers?',\n",
       " '| car_1 | continents : contid , continent ( europe ) | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " \"select t1.countryname from countries as t1 join continents as t2 on t1.continent = t2.contid join car_makers as t3 on t1.countryid = t3.country where t2.continent = 'europe' group by t1.countryname having count(*) >= 3;\")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = 130\n",
    "processed_spider_dev[_id]['text_in'], \\\n",
    "processed_spider_dev[_id]['struct_in'], \\\n",
    "processed_spider_dev[_id]['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_enc_sentence = f\"{processed_spider_dev[_id]['text_in']}; structed knowledge: {processed_spider_dev[_id]['struct_in']}\"\n",
    "_toks = mt_uskg.tokenizer.tokenize(_enc_sentence)\n",
    "len(_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # _occ_punct = set()\n",
    "\n",
    "# for _id in range(len(processed_spider_dev)):\n",
    "#     ex = processed_spider_dev[_id]\n",
    "# #     _occ_punct.update(set(string.punctuation) & set(ex['seq_out']))\n",
    "#     if '_(' in ex['struct_in']:\n",
    "#         print(_id, ex['question'])\n",
    "#         print(ex['struct_in'])\n",
    "#         print(ex['seq_out'])\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Train set\n",
    "\n",
    "# raw_spider_train = ctu.load_raw_dataset(\n",
    "#     data_filepath = spider_train_path,\n",
    "#     db_path=spider_db_dir,\n",
    "# )\n",
    "# processed_spider_train = s2s_spider.TrainDataset(\n",
    "#     args=mt_uskg.task_args,\n",
    "#     raw_datasets=raw_spider_train,\n",
    "#     cache_root='../cache')\n",
    "# len(processed_spider_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_spider_train[5441]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "- merged in create_analysis_sample_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = '/home/yshao/Projects/language/language/xsp/data/spider/tables.json'\n",
    "db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmaps = sp_eval.build_foreign_key_map_from_json(table_path)\n",
    "evaluator = sp_eval.Evaluator(db_dir=db_dir, kmaps=kmaps, etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctu.evaluate_hardness.evaluator = evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0, 0, 'hard')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "_sql_str = 'select t1.birth_date from people as t1 join poker_player as t2 on t1.people_id = t2.people_id order by t2.earnings asc limit 1'\n",
    "db_name = 'poker_player'\n",
    "schema = evaluator.schemas[db_name]\n",
    "_sql = sp_eval.get_sql(schema, _sql_str)\n",
    "sp_eval.count_component1(_sql), sp_eval.count_component2(_sql), sp_eval.count_others(_sql), \\\n",
    "evaluator.eval_hardness(_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hard'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.evaluate_hardness(_sql_str, db_name, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<uskg.third_party.spider.evaluation.Evaluator at 0x7f7814ed2040>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.evaluate_hardness.evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_prompt = 'select avg(age), min(age), max(age) from'\n",
    "ctu.detect_node_role(dec_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dicts = ctu.create_analysis_sample_dicts(\n",
    "    mt=mt_uskg,\n",
    "    ex=processed_spider_dev[100],\n",
    "    subject_type='table'\n",
    ")\n",
    "len(a_dicts), [d['expect'] for d in a_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex = a_dicts[2]\n",
    "ctu.check_table_text_match(a_ex, 'car_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex['text_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-5.0: dirty attention vector effect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect_type = 'table_alias'\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp5_0_dirty_attention_vector_effect/exp=5_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2039, (164, 164), 339, 1536, 1875, 1700)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "#         # TEMP adjustment for column results \n",
    "#         d['low_score'] = d['trace_scores']['high_layers_corrupt'].get(\"0\", 0.0)  # \"0\" is key (for layer 0), 0.0 is default \n",
    "#         if d['base_score'] - d['low_score'] < 0.5:\n",
    "#             d['is_good_sample'] = False\n",
    "#         # END_TEMP\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5, (i, d)\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples), \\\n",
    "n_good_samples + n_too_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[s for s in bad_samples if s['correct_prediction']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg = {k: {str(l): 0 for l in range(24)} for k in good_samples[0]['trace_scores'].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    for k, layer_d in d['trace_scores'].items():\n",
    "        for l, s in layer_d.items():\n",
    "            trace_scores_avg[k][l] += s\n",
    "\n",
    "for k, layer_d in trace_scores_avg.items():\n",
    "    for l, s in layer_d.items():\n",
    "        layer_d[l] = s / len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg by aspects (category)\n",
    "- Still kind of linear, as in exp-2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_hardness': 'hard', 'node_role': 'where', 'text_match': 'partial'}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: (trace_key, aspect, asp_val, layer) -> [scores]\n",
    "trace_scores_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n",
    "trace_scores_avg_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "trace_scores_cnt_by_aspect = defaultdict(lambda: defaultdict(int))  # no trace key & layer key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    for trace_k, trace_layer_d in d['trace_scores'].items():\n",
    "        for aspect, asp_val in d['category'].items():\n",
    "            for l, s in trace_layer_d.items():\n",
    "                trace_scores_by_aspect[trace_k][aspect][asp_val][l].append(s)\n",
    "\n",
    "for trace_k, d1 in trace_scores_by_aspect.items():\n",
    "    for asp_k, d2 in d1.items():\n",
    "        for asp_v, d3 in d2.items():\n",
    "            for l, s in d3.items():\n",
    "                trace_scores_avg_by_aspect[trace_k][asp_k][asp_v][l] = np.mean(s)\n",
    "                trace_scores_cnt_by_aspect[asp_k][asp_v] = len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'sql_hardness': defaultdict(int,\n",
       "                         {'medium': 400,\n",
       "                          'hard': 155,\n",
       "                          'easy': 142,\n",
       "                          'extra': 170}),\n",
       "             'node_role': defaultdict(int,\n",
       "                         {'where': 268,\n",
       "                          'select': 412,\n",
       "                          'order by': 66,\n",
       "                          'join': 92,\n",
       "                          'group by': 25,\n",
       "                          'having': 4}),\n",
       "             'text_match': defaultdict(int,\n",
       "                         {'no-match': 360, 'partial': 148, 'exact': 359})})"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_cnt_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg_by_aspect['high_layers_corrupt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-5.2: attention section removal effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect_type = 'table'\n",
    "\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/exp=5.2.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1683, (1207, 1207), 136, 340, 476, 'good / correct = 1207 / 1547')"
      ]
     },
     "execution_count": 681,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples),\\\n",
    "f'good / correct = {n_good_samples} / {n_good_samples + n_too_easy}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enc_sentence': 'Find the number of concerts happened in the stadium with the highest capacity .; structed knowledge: | concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id',\n",
       " 'seq_out': 'select count(*) from concert where stadium_id = (select stadium_id from stadium order by capacity desc limit 1)',\n",
       " 'dec_prompt': 'select count(*) from',\n",
       " 'expect': 'concert',\n",
       " 'expect_type': 'table',\n",
       " 'db_id': 'concert_singer',\n",
       " 'expect_input_ranges': [[88, 89]],\n",
       " 'expect_table': 'concert',\n",
       " 'answer': 'stadium',\n",
       " 'base_score': 0.9941871166229248,\n",
       " 'answers_t': [14939],\n",
       " 'correct_prediction': False,\n",
       " 'category': {'sql_hardness': 'hard',\n",
       "  'node_role': 'from',\n",
       "  'text_match': 'exact'},\n",
       " 'self_ranges': [[87, 91]],\n",
       " 'struct_context_ranges': [[22, 87], [91, 132]],\n",
       " 'is_good_sample': False}"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in bad_samples if not s['correct_prediction']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg = {sect_k : defaultdict(int) for sect_k in good_samples[0]['trace_scores'].keys()}\n",
    "\n",
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for k, v in sect_d.items():\n",
    "            if k == 'window':\n",
    "                for l, s in v.items():\n",
    "                    trace_scores_avg[sect_k][f'{k}-{l}'] += s\n",
    "            else:\n",
    "                s = v\n",
    "                trace_scores_avg[sect_k][k] += s\n",
    "\n",
    "for sect_k, sect_d in trace_scores_avg.items():\n",
    "    for k, s in sect_d.items():\n",
    "        sect_d[k] = s / len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg by aspects (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP patch for node_len category \n",
    "for d in good_samples + bad_samples:\n",
    "    node_len = len(d['answers_t'])\n",
    "    assert len(mt_uskg.tokenizer.tokenize(d['expect'])) == node_len, (d['expect'], node_len)\n",
    "    d['category']['node_len'] = str(node_len) if node_len <= 3 else '4+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_hardness': 'medium',\n",
       " 'node_role': 'from',\n",
       " 'text_match': 'exact',\n",
       " 'node_len': '1'}"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: (sect_k, aspect, asp_val, layer) -> [scores]\n",
    "trace_scores_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n",
    "trace_scores_avg_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "trace_scores_cnt_by_aspect = defaultdict(lambda: defaultdict(int))  # no sect key & layer key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for aspect, asp_val in d['category'].items():\n",
    "            for k, v in sect_d.items():\n",
    "                if k == 'window':\n",
    "                    for l, s in v.items():\n",
    "                        if not (int(l) % 4 == 3): continue\n",
    "                        layer_k = f'{k}-{l}'\n",
    "                        trace_scores_by_aspect[sect_k][aspect][asp_val][layer_k].append(s)\n",
    "                else:\n",
    "                    layer_k = k\n",
    "                    s = v\n",
    "                    trace_scores_by_aspect[sect_k][aspect][asp_val][layer_k].append(s)\n",
    "                    \n",
    "for sect_k, d1 in trace_scores_by_aspect.items():\n",
    "    for asp_k, d2 in d1.items():\n",
    "        for asp_v, d3 in d2.items():\n",
    "            for layer_k, s in d3.items():\n",
    "                trace_scores_avg_by_aspect[sect_k][asp_k][asp_v][layer_k] = np.mean(s)\n",
    "                trace_scores_cnt_by_aspect[asp_k][asp_v] = len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sect_k, sect_d in trace_scores_avg_by_aspect.items():\n",
    "    sect_d['overall'] = dict()\n",
    "    for layer_k, s in trace_scores_avg[sect_k].items():\n",
    "        if layer_k.startswith('window'):\n",
    "            # only keep a subset of layers \n",
    "            _, l = layer_k.split('-')\n",
    "            if not (int(l) % 4 == 3): continue\n",
    "        sect_d['overall'][layer_k] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'sql_hardness': defaultdict(int,\n",
       "                         {'medium': 378,\n",
       "                          'hard': 148,\n",
       "                          'easy': 134,\n",
       "                          'extra': 160}),\n",
       "             'node_role': defaultdict(int,\n",
       "                         {'where': 248,\n",
       "                          'select': 393,\n",
       "                          'order by': 63,\n",
       "                          'join': 91,\n",
       "                          'group by': 21,\n",
       "                          'having': 4}),\n",
       "             'text_match': defaultdict(int,\n",
       "                         {'no-match': 345, 'partial': 142, 'exact': 333}),\n",
       "             'node_len': defaultdict(int,\n",
       "                         {'1': 329, '3': 238, '4+': 160, '2': 93})})"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_cnt_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_scores_avg_by_aspect['self']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_d = ctu.nested_json_processing(trace_scores_avg_by_aspect, func=lambda x: np.format_float_positional(x, precision=4, min_digits=4))\n",
    "# dump_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/summ-exp=5.2.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(dump_path, 'w') as f:\n",
    "    json.dump(dump_d, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (one-time temp patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect_type = 'table_alias'\n",
    "# orig_res_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/no_structcontext-exp=5.2.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "# add_res_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/exp=5.2.1+structcontext_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "# merge_res_path = f'/home/yshao/Projects/rome/results/exp5_2_attention_section_removal_effect/exp=5.2.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(orig_res_path, 'r') as f:\n",
    "#     orig_all_samples = [json.loads(l) for l in f]\n",
    "# with open(add_res_path, 'r') as f:\n",
    "#     add_all_samples = [json.loads(l) for l in f]\n",
    "\n",
    "# f = open(merge_res_path, 'w')\n",
    "    \n",
    "# for i, (orig_ex, add_ex) in enumerate(zip(orig_all_samples, add_all_samples)):\n",
    "#     assert len(orig_ex['trace_results']) == len(add_ex['trace_results']), i\n",
    "#     # There is randomness in the order of expected node (from set()), thus sorting here \n",
    "#     orig_ex['trace_results'].sort(key=lambda d: len(d['dec_prompt']))\n",
    "#     add_ex['trace_results'].sort(key=lambda d: len(d['dec_prompt']))\n",
    "#     for j, (orig_d, add_d) in enumerate(zip(orig_ex['trace_results'], add_ex['trace_results'])):\n",
    "#         assert orig_d['is_good_sample'] == add_d['is_good_sample'], (i, j)\n",
    "#         if not orig_d['is_good_sample']:\n",
    "#             continue\n",
    "            \n",
    "#         # is good sample: add the new sections \n",
    "#         orig_d['trace_scores']['struct_context'] = add_d['trace_scores']['struct_context']\n",
    "#         orig_d['trace_scores']['text+struct_context'] = add_d['trace_scores']['text+struct_context']\n",
    "        \n",
    "#     f.write(json.dumps(orig_ex, indent=None) + '\\n')\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single samples observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prefix', 'text', 'struct', 'text+struct', 'all', 'self', 'struct_context', 'text+struct_context'])"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples[0]['trace_scores'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = 0\n",
    "\n",
    "d = good_samples[_id]\n",
    "\n",
    "check_info_d = defaultdict(dict)\n",
    "\n",
    "for sect_k, sect_d in d['trace_scores'].items():\n",
    "    for layer_k, s in sect_d.items():\n",
    "        if layer_k == 'window':\n",
    "            layer_k = 'window-19'\n",
    "            s = s['19']\n",
    "        if s < 0.5:\n",
    "            check_info_d[sect_k][layer_k] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text+struct\": {\n",
      "    \"all_layers\": 0.3990614414215088\n",
      "  },\n",
      "  \"all\": {\n",
      "    \"all_layers\": 0.4772564172744751\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(check_info_d, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "844"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check \"breaking\" window layer, i.e. those with sudden changes \n",
    "### For now: single layer drop > _th\n",
    "\n",
    "_th = 0.4\n",
    "check_info_l = []\n",
    "for i, d in enumerate(good_samples):\n",
    "#     for sect_k, sect_d in d['trace_scores'].items():\n",
    "    sect_k = 'all'\n",
    "    sect_d = d['trace_scores'][sect_k]\n",
    "    window_d = sect_d['window']\n",
    "    for l in range(1, 24):\n",
    "        if window_d[str(l-1)] - window_d[str(l)] > _th:\n",
    "            _info_d = {\n",
    "                'id': i,\n",
    "                'sect_k': sect_k,\n",
    "                'layer': l,\n",
    "                'last_layer_score': window_d[str(l-1)],\n",
    "                'this_layer_score': window_d[str(l)],\n",
    "            }\n",
    "            check_info_l.append(_info_d)\n",
    "            break\n",
    "len(check_info_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(844, 1207)"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check_info_l), len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 10),\n",
       " (2, 25),\n",
       " (3, 31),\n",
       " (4, 81),\n",
       " (5, 24),\n",
       " (6, 3),\n",
       " (7, 15),\n",
       " (8, 87),\n",
       " (9, 69),\n",
       " (10, 19),\n",
       " (11, 38),\n",
       " (12, 36),\n",
       " (13, 70),\n",
       " (14, 101),\n",
       " (15, 27),\n",
       " (16, 31),\n",
       " (17, 31),\n",
       " (18, 79),\n",
       " (19, 67)]"
      ]
     },
     "execution_count": 707,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "break_layer_counter = Counter([_d['layer'] for _d in check_info_l])\n",
    "sorted(break_layer_counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info_d in check_info_l:\n",
    "    if info_d['layer'] < 6:\n",
    "        print(info_d)\n",
    "        sample_id = info_d['id']\n",
    "        d = good_samples[sample_id]\n",
    "        print(d['enc_sentence'])\n",
    "        print(d['dec_prompt'], '---->', d['expect'])\n",
    "        print('Categories:', d['category'])\n",
    "        print('--' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counter_by_aspect = defaultdict(Counter)  # [asp_k, asp_v] -> count \n",
    "sample_counter = Counter()\n",
    "\n",
    "for info_d in check_info_l:\n",
    "    if info_d['layer'] < 6:\n",
    "        sample_id = info_d['id']\n",
    "        d = good_samples[sample_id]\n",
    "        text_match = d['category']['text_match']\n",
    "        node_len = d['category']['node_len']\n",
    "        sample_counter[(text_match, node_len)] += 1\n",
    "        \n",
    "        for asp_k, asp_v in d['category'].items():\n",
    "            sample_counter_by_aspect[asp_k][asp_v] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'sql_hardness': Counter({'medium': 58,\n",
       "                      'hard': 42,\n",
       "                      'extra': 51,\n",
       "                      'easy': 20}),\n",
       "             'node_role': Counter({'from': 105, 'join': 66}),\n",
       "             'text_match': Counter({'partial': 59,\n",
       "                      'no-match': 74,\n",
       "                      'exact': 38}),\n",
       "             'node_len': Counter({'4+': 60, '3': 51, '2': 24, '1': 36})})"
      ]
     },
     "execution_count": 715,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_counter_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('partial', '3'), 30),\n",
       " (('partial', '4+'), 29),\n",
       " (('exact', '1'), 22),\n",
       " (('no-match', '3'), 21),\n",
       " (('no-match', '4+'), 21),\n",
       " (('no-match', '2'), 18),\n",
       " (('no-match', '1'), 14),\n",
       " (('exact', '4+'), 10),\n",
       " (('exact', '2'), 6)]"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counter_by_aspect = defaultdict(Counter)  # [asp_k, asp_v] -> count \n",
    "sample_counter = Counter()\n",
    "\n",
    "for info_d in check_info_l:\n",
    "    if info_d['layer'] > 18:\n",
    "        sample_id = info_d['id']\n",
    "        d = good_samples[sample_id]\n",
    "        text_match = d['category']['text_match']\n",
    "        node_len = d['category']['node_len']\n",
    "        sample_counter[(text_match, node_len)] += 1\n",
    "        \n",
    "        for asp_k, asp_v in d['category'].items():\n",
    "            sample_counter_by_aspect[asp_k][asp_v] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'sql_hardness': Counter({'hard': 9,\n",
       "                      'medium': 27,\n",
       "                      'easy': 11,\n",
       "                      'extra': 20}),\n",
       "             'node_role': Counter({'join': 21, 'from': 46}),\n",
       "             'text_match': Counter({'exact': 46,\n",
       "                      'no-match': 20,\n",
       "                      'partial': 1}),\n",
       "             'node_len': Counter({'3': 12, '1': 46, '2': 6, '4+': 3})})"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_counter_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('exact', '1'), 38),\n",
       " (('no-match', '3'), 9),\n",
       " (('no-match', '1'), 8),\n",
       " (('exact', '3'), 3),\n",
       " (('no-match', '2'), 3),\n",
       " (('exact', '2'), 3),\n",
       " (('exact', '4+'), 2),\n",
       " (('partial', '4+'), 1)]"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.385318918917694e-12"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _min_p = 1.0\n",
    "\n",
    "# for i, d in enumerate(good_samples):\n",
    "#     sect_k = 'text'\n",
    "#     sect_d = d['trace_scores'][sect_k]\n",
    "#     _min_p = min(_min_p, sect_d['all_layers'])\n",
    "\n",
    "# _min_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 916, 1207, 1207)"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Systematic \n",
    "\n",
    "ob_sect_k = 'all'\n",
    "\n",
    "_th = 0.4\n",
    "\n",
    "check_info_l = []\n",
    "all_layers_eff_cnt = 0  # for this section to observe, how many samples are effective with all_layers\n",
    "window_eff_cnt = 0      # for this section to observe, how many samples are effective with any window \n",
    "\n",
    "for i, d in enumerate(good_samples):\n",
    "#     for sect_k, sect_d in d['trace_scores'].items():\n",
    "    sect_k = ob_sect_k\n",
    "    sect_d = d['trace_scores'][sect_k]\n",
    "    if sect_d['all_layers'] > 0.5:\n",
    "        # not effective\n",
    "        continue\n",
    "    else:\n",
    "        all_layers_eff_cnt += 1\n",
    "        \n",
    "    if min(sect_d['window'].values()) > 0.5:\n",
    "        # not effective\n",
    "        continue\n",
    "    else:\n",
    "        window_eff_cnt += 1\n",
    "        \n",
    "    window_d = sect_d['window']\n",
    "    for l in range(1, 24):\n",
    "        if window_d[str(l-1)] - window_d[str(l)] > _th:\n",
    "            _info_d = {\n",
    "                'id': i,\n",
    "                'sect_k': sect_k,\n",
    "                'layer': l,\n",
    "                'last_layer_score': window_d[str(l-1)],\n",
    "                'this_layer_score': window_d[str(l)],\n",
    "            }\n",
    "            check_info_l.append(_info_d)\n",
    "            break\n",
    "len(check_info_l), window_eff_cnt, all_layers_eff_cnt, len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctg_list = [(tm, nl) for tm in ['exact', 'partial', 'no-match'] for nl in ['1', '2', '3', '4+']]\n",
    "layer_list = [str(l) for l in range(1, 24)]\n",
    "\n",
    "ctg_elem2id = {elem : i for i, elem in enumerate(ctg_list)}\n",
    "layer_elem2id = {elem : i for i, elem in enumerate(layer_list)}\n",
    "\n",
    "cnt_matrix = np.zeros((len(ctg_list), len(layer_list)), int)\n",
    "\n",
    "for info_d in check_info_l:\n",
    "    sample_id = info_d['id']\n",
    "    d = good_samples[sample_id]\n",
    "    text_match = d['category']['text_match']\n",
    "    node_len = d['category']['node_len']\n",
    "    _ctg = (text_match, node_len)\n",
    "    _layer = str(info_d['layer'])\n",
    "    \n",
    "    _ctg_idx = ctg_elem2id[_ctg]\n",
    "    _layer_idx = layer_elem2id[_layer]\n",
    "    cnt_matrix[_ctg_idx, _layer_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Display the matrix using imshow\n",
    "im = ax.imshow(cnt_matrix, cmap='Blues')\n",
    "\n",
    "# Set the tick labels for the first and second dimensions\n",
    "ax.set_xticks(np.arange(len(layer_list)))\n",
    "ax.set_yticks(np.arange(len(ctg_list)))\n",
    "\n",
    "# Set the tick labels using the ctg_list and layer_list\n",
    "ax.set_xticklabels(layer_list)\n",
    "ax.set_yticklabels(ctg_list)\n",
    "\n",
    "ax.set_title(f'Section: {ob_sect_k}\\n')\n",
    "\n",
    "# Rotate the x-axis tick labels if needed\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Display the matrix using imshow\n",
    "im = ax.imshow(cnt_matrix, cmap='Blues')\n",
    "\n",
    "# Set the tick labels for the first and second dimensions\n",
    "ax.set_xticks(np.arange(len(layer_list)))\n",
    "ax.set_yticks(np.arange(len(ctg_list)))\n",
    "\n",
    "# Set the tick labels using the ctg_list and layer_list\n",
    "ax.set_xticklabels(layer_list)\n",
    "ax.set_yticklabels(ctg_list)\n",
    "\n",
    "ax.set_title(f'Section: {ob_sect_k}\\n')\n",
    "\n",
    "# Rotate the x-axis tick labels if needed\n",
    "# plt.xticks(rotation=90)\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-5.3: attention section mutual removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 837,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect_type = 'table_alias'\n",
    "\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/exp=5.3.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2039, (364, 364), 339, 1336, 1675, 'good / correct = 364 / 1700')"
      ]
     },
     "execution_count": 839,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples), \\\n",
    "f'good / correct = {n_good_samples} / {n_good_samples + n_too_easy}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg = {sect_k : defaultdict(int) for sect_k in good_samples[0]['trace_scores'].keys()}\n",
    "\n",
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for k, v in sect_d.items():\n",
    "            if k == 'window':\n",
    "                for l, s in v.items():\n",
    "                    trace_scores_avg[sect_k][f'{k}-{l}'] += s\n",
    "            else:\n",
    "                s = v\n",
    "                trace_scores_avg[sect_k][k] += s\n",
    "\n",
    "for sect_k, sect_d in trace_scores_avg.items():\n",
    "    for k, s in sect_d.items():\n",
    "        sect_d[k] = s / len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg by aspects (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_hardness': 'medium', 'node_role': 'where', 'text_match': 'no-match'}"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMP patch for node_len category \n",
    "# for d in good_samples + bad_samples:\n",
    "#     node_len = len(d['answers_t'])\n",
    "#     assert len(mt_uskg.tokenizer.tokenize(d['expect'])) == node_len, (d['expect'], node_len)\n",
    "#     d['category']['node_len'] = str(node_len) if node_len <= 3 else '4+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql_hardness': 'medium',\n",
       " 'node_role': 'group by',\n",
       " 'text_match': 'exact',\n",
       " 'node_len': '3'}"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: (sect_k, aspect, asp_val, layer) -> [scores]\n",
    "trace_scores_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n",
    "trace_scores_avg_by_aspect = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(float))))\n",
    "trace_scores_cnt_by_aspect = defaultdict(lambda: defaultdict(int))  # no sect key & layer key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for aspect, asp_val in d['category'].items():\n",
    "            for k, v in sect_d.items():\n",
    "                if k == 'window':\n",
    "                    for l, s in v.items():\n",
    "                        if not (int(l) % 4 == 3): continue\n",
    "                        layer_k = f'{k}-{l}'\n",
    "                        trace_scores_by_aspect[sect_k][aspect][asp_val][layer_k].append(s)\n",
    "                else:\n",
    "                    layer_k = k\n",
    "                    s = v\n",
    "                    trace_scores_by_aspect[sect_k][aspect][asp_val][layer_k].append(s)\n",
    "                    \n",
    "for sect_k, d1 in trace_scores_by_aspect.items():\n",
    "    for asp_k, d2 in d1.items():\n",
    "        for asp_v, d3 in d2.items():\n",
    "            for layer_k, s in d3.items():\n",
    "                trace_scores_avg_by_aspect[sect_k][asp_k][asp_v][layer_k] = np.mean(s)\n",
    "                trace_scores_cnt_by_aspect[asp_k][asp_v] = len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sect_k, sect_d in trace_scores_avg_by_aspect.items():\n",
    "    sect_d['overall'] = dict()\n",
    "    for layer_k, s in trace_scores_avg[sect_k].items():\n",
    "        if layer_k.startswith('window'):\n",
    "            # only keep a subset of layers \n",
    "            _, l = layer_k.split('-')\n",
    "            if not (int(l) % 4 == 3): continue\n",
    "        sect_d['overall'][layer_k] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'sql_hardness': defaultdict(int,\n",
       "                         {'medium': 123, 'extra': 175, 'hard': 64, 'easy': 2}),\n",
       "             'node_role': defaultdict(int,\n",
       "                         {'select': 191,\n",
       "                          'group by': 33,\n",
       "                          'join': 12,\n",
       "                          'where': 111,\n",
       "                          'order by': 17}),\n",
       "             'text_match': defaultdict(int,\n",
       "                         {'exact': 230, 'partial': 28, 'no-match': 106}),\n",
       "             'node_len': defaultdict(int, {'3': 364})})"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_cnt_by_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_scores_avg_by_aspect['c->p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_d = ctu.nested_json_processing(trace_scores_avg_by_aspect, func=lambda x: np.format_float_positional(x, precision=4, min_digits=4))\n",
    "# dump_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/summ-exp=5.3.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "with open(dump_path, 'w') as f:\n",
    "    json.dump(dump_d, f, indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (one-time temp patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect_type = 'table_alias'\n",
    "# orig_res_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/no_c2p_exp=5.3.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "# add_res_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/exp=5.3.1+c2p_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'\n",
    "\n",
    "# merge_res_path = f'/home/yshao/Projects/rome/results/exp5_3_attention_section_mutual_removal/exp=5.3.1_dev_{expect_type}_encoder-attn=self_attn-corrupt=zero.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(orig_res_path, 'r') as f:\n",
    "#     orig_all_samples = [json.loads(l) for l in f]\n",
    "# with open(add_res_path, 'r') as f:\n",
    "#     add_all_samples = [json.loads(l) for l in f]\n",
    "\n",
    "# f = open(merge_res_path, 'w')\n",
    "    \n",
    "# for i, (orig_ex, add_ex) in enumerate(zip(orig_all_samples, add_all_samples)):\n",
    "#     assert len(orig_ex['trace_results']) == len(add_ex['trace_results']), i\n",
    "#     # There is randomness in the order of expected node (from set()), thus sorting here \n",
    "#     orig_ex['trace_results'].sort(key=lambda d: len(d['dec_prompt']))\n",
    "#     add_ex['trace_results'].sort(key=lambda d: len(d['dec_prompt']))\n",
    "#     for j, (orig_d, add_d) in enumerate(zip(orig_ex['trace_results'], add_ex['trace_results'])):\n",
    "#         assert orig_d['is_good_sample'] == add_d['is_good_sample'], (i, j)\n",
    "#         if not orig_d['is_good_sample']:\n",
    "#             continue\n",
    "            \n",
    "#         # is good sample: add the new sections \n",
    "#         orig_d['trace_scores']['c->p'] = add_d['trace_scores']['c->p']\n",
    "        \n",
    "#         # put all at end in the dict \n",
    "#         _t = orig_d['trace_scores']['all']\n",
    "#         del orig_d['trace_scores']['all']\n",
    "#         orig_d['trace_scores']['all'] = _t\n",
    "        \n",
    "#     f.write(json.dumps(orig_ex, indent=None) + '\\n')\n",
    "    \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-5.4: attention section mutual removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expect_type = 'table_alias'\n",
    "\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp5_4_decoder_cross_attention_removal/exp=5.4_dev_{expect_type}-corrupt=zero.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2039, (395, 395), 339, 1305, 1644, 'good / correct = 395 / 1700')"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "\n",
    "        if d['is_good_sample']:\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples), \\\n",
    "f'good / correct = {n_good_samples} / {n_good_samples + n_too_easy}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict[sect_k, Dict[layer_k, s]]\n",
    "trace_scores_avg = {sect_k : defaultdict(int) for sect_k in good_samples[0]['trace_scores'].keys()}\n",
    "\n",
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for k, s in sect_d.items():\n",
    "            trace_scores_avg[sect_k][k] += s\n",
    "\n",
    "for sect_k, sect_d in trace_scores_avg.items():\n",
    "    for k, s in sect_d.items():\n",
    "        sect_d[k] = s / len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': defaultdict(int,\n",
       "             {'low_layers': 0.5355958729745377,\n",
       "              'high_layers': 0.8155413844211263,\n",
       "              'all_layers': 0.1056054830929244}),\n",
       " 'ans->t': defaultdict(int,\n",
       "             {'low_layers': 0.9304732342151586,\n",
       "              'high_layers': 0.9741443378360842,\n",
       "              'all_layers': 0.8695232256844865}),\n",
       " 'all->t': defaultdict(int,\n",
       "             {'low_layers': 0.9166845399916218,\n",
       "              'high_layers': 0.9720493624806168,\n",
       "              'all_layers': 0.8491711175152609}),\n",
       " 'ans->s': defaultdict(int,\n",
       "             {'low_layers': 0.9289467685215815,\n",
       "              'high_layers': 0.9696432898008954,\n",
       "              'all_layers': 0.9363590360931409}),\n",
       " 'all->s': defaultdict(int,\n",
       "             {'low_layers': 0.82528036536228,\n",
       "              'high_layers': 0.9648957481430593,\n",
       "              'all_layers': 0.8213527668459242}),\n",
       " 'ans->p': defaultdict(int,\n",
       "             {'low_layers': 0.8902059622092646,\n",
       "              'high_layers': 0.8411666841540542,\n",
       "              'all_layers': 0.7503435674796515}),\n",
       " 'all->p': defaultdict(int,\n",
       "             {'low_layers': 0.796402538478354,\n",
       "              'high_layers': 0.8223322679915522,\n",
       "              'all_layers': 0.6521803225023696}),\n",
       " 'ans->c': defaultdict(int,\n",
       "             {'low_layers': 0.9459983647684429,\n",
       "              'high_layers': 0.9759769369582085,\n",
       "              'all_layers': 0.9602334115893773}),\n",
       " 'all->c': defaultdict(int,\n",
       "             {'low_layers': 0.9093552527139159,\n",
       "              'high_layers': 0.9688374730927097,\n",
       "              'all_layers': 0.9209699824665751}),\n",
       " 'ans->self': defaultdict(int,\n",
       "             {'low_layers': 0.9771139679830285,\n",
       "              'high_layers': 0.9713836791111699,\n",
       "              'all_layers': 0.9578520661066698}),\n",
       " 'all->self': defaultdict(int,\n",
       "             {'low_layers': 0.9348924317305076,\n",
       "              'high_layers': 0.974151099973087,\n",
       "              'all_layers': 0.9145273294408152})}"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all     \t0.5356\t0.8155\t0.1056\n",
      "ans->t  \t0.9305\t0.9741\t0.8695\n",
      "ans->s  \t0.9289\t0.9696\t0.9364\n",
      "ans->p  \t0.8902\t0.8412\t0.7503\n",
      "ans->c  \t0.9460\t0.9760\t0.9602\n",
      "ans->self\t0.9771\t0.9714\t0.9579\n"
     ]
    }
   ],
   "source": [
    "for sect_k, sect_d in trace_scores_avg.items():\n",
    "    # 'all->?' results seem similar to 'ans->?' and make less intuitive sense; skip for now \n",
    "    if sect_k.startswith('all->'):\n",
    "        continue\n",
    "    print_l = f'{sect_k:<8s}'\n",
    "    for k, s in sect_d.items(): \n",
    "        print_l += f'\\t{s:.4f}'\n",
    "    print(print_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-6.0: corruption effect - syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expect_type = 'table_alias'\n",
    "\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp6_0_encoding_corruption_effect_syntax/exp=6.0_dev.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10233, (2261, 2261), 1623, 6349, 7972, 'good / correct = 2261 / 8610')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "\n",
    "        if d.get('is_good_sample', True):\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples), \\\n",
    "f'good / correct = {n_good_samples} / {n_good_samples + n_too_easy}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg = {sect_k : defaultdict(int) for sect_k in good_samples[0]['trace_scores'].keys()}\n",
    "\n",
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for k, v in sect_d.items():\n",
    "            trace_scores_avg[sect_k][k] += v\n",
    "\n",
    "for sect_k, sect_d in trace_scores_avg.items():\n",
    "    for k, s in sect_d.items():\n",
    "        sect_d[k] = s / len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': defaultdict(int,\n",
       "             {'embed': 0.2704069729491695, 'final_enc': 0.43288231847139375}),\n",
       " 'struct': defaultdict(int,\n",
       "             {'embed': 0.8434888537914244, 'final_enc': 0.7056294551667914}),\n",
       " 'columns': defaultdict(int,\n",
       "             {'embed': 0.8977011346416999, 'final_enc': 0.9094602057515592}),\n",
       " 'tables': defaultdict(int,\n",
       "             {'embed': 0.9401333304200568, 'final_enc': 0.9652279544981762}),\n",
       " 'all': defaultdict(int,\n",
       "             {'embed': 0.04223158108438767, 'final_enc': 0.14583704401879738})}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corruption overall effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict[str, int]: expect_tok -> num of effective / not effective corruptions (all)\n",
    "eff_counter = Counter()\n",
    "neff_counter = Counter()\n",
    "\n",
    "for d in good_samples:\n",
    "    eff_counter[d['expect']] += 1\n",
    "for d in bad_samples:\n",
    "    if d['correct_prediction']:\n",
    "        # \"too easy\", corruption not effective \n",
    "        neff_counter[d['expect']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_rate_d = dict()\n",
    "\n",
    "for k in list(set(eff_counter.keys()) | set(neff_counter.keys())):\n",
    "    eff_c = eff_counter[k]\n",
    "    neff_c = neff_counter[k]\n",
    "    eff_r = 1.0 * eff_c / (eff_c + neff_c)\n",
    "    eff_rate_d[k] = eff_r\n",
    "    # print(f'{k:<10s}{eff_c:5d} /{eff_c + neff_c:5d} = {eff_r:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union\t6\t6\t1.0000\n",
      "!=\t20\t20\t1.0000\n",
      "like\t12\t12\t1.0000\n",
      "or\t34\t34\t1.0000\n",
      "min\t18\t18\t1.0000\n",
      "asc\t19\t19\t1.0000\n",
      "max\t30\t30\t1.0000\n",
      "between\t6\t6\t1.0000\n",
      "except\t21\t21\t1.0000\n",
      "avg\t65\t65\t1.0000\n",
      "intersect\t34\t34\t1.0000\n",
      "having\t80\t81\t0.9877\n",
      "distinct\t25\t26\t0.9615\n",
      "sum\t21\t22\t0.9545\n",
      "where\t484\t516\t0.9380\n",
      "not\t42\t46\t0.9130\n",
      "group\t225\t265\t0.8491\n",
      "and\t31\t39\t0.7949\n",
      "count\t267\t406\t0.6576\n",
      "order\t142\t221\t0.6425\n",
      ">\t61\t101\t0.6040\n",
      ")\t11\t23\t0.4783\n",
      "=\t191\t968\t0.1973\n",
      "as\t93\t952\t0.0977\n",
      "desc\t16\t164\t0.0976\n",
      "in\t4\t50\t0.0800\n",
      "join\t39\t496\t0.0786\n",
      "from\t49\t1196\t0.0410\n",
      "limit\t6\t177\t0.0339\n",
      ">=\t1\t30\t0.0333\n",
      "(\t22\t675\t0.0326\n",
      "*\t0\t381\t0.0000\n",
      "by\t0\t516\t0.0000\n",
      "on\t0\t516\t0.0000\n",
      "select\t0\t88\t0.0000\n"
     ]
    }
   ],
   "source": [
    "for k, eff_r in sorted(eff_rate_d.items(), key=lambda x: x[1], reverse=True):\n",
    "    eff_c = eff_counter[k]\n",
    "    neff_c = neff_counter[k]\n",
    "    all_c = eff_c + neff_c\n",
    "    if all_c <= 2: continue\n",
    "    if k.isnumeric(): continue\n",
    "#     print(f'{k:<10s}{eff_c:5d} /{all_c:5d} = {eff_r:.4f}')\n",
    "    print(f'{k}\\t{eff_c}\\t{all_c}\\t{eff_r:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg by expect syntax token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: (expect_tok, sect_k, layer) -> [scores]\n",
    "trace_scores_by_exp_tok = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "trace_scores_avg_by_exp_tok = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "trace_scores_cnt_by_exp_tok = defaultdict(int)  # no sect key & layer key \n",
    "\n",
    "trace_sample_ids_by_exp_tok = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(good_samples):\n",
    "    expect = d['expect']\n",
    "    trace_sample_ids_by_exp_tok[expect].append(i)\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for layer_k, v in sect_d.items():\n",
    "            trace_scores_by_exp_tok[expect][sect_k][layer_k].append(v)\n",
    "\n",
    "for exp_tok, d1 in trace_scores_by_exp_tok.items():\n",
    "    if exp_tok.isnumeric(): continue\n",
    "    for sect_k, d2 in d1.items():\n",
    "        for layer_k, scores in d2.items():\n",
    "            if len(scores) <= 2: continue\n",
    "            trace_scores_avg_by_exp_tok[exp_tok][sect_k][layer_k] = np.mean(scores)\n",
    "            trace_scores_cnt_by_exp_tok[exp_tok] = len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'count': 267,\n",
       "             'order': 142,\n",
       "             'avg': 65,\n",
       "             'min': 18,\n",
       "             'max': 30,\n",
       "             'where': 484,\n",
       "             'distinct': 25,\n",
       "             '>': 61,\n",
       "             'group': 225,\n",
       "             '(': 22,\n",
       "             'between': 6,\n",
       "             'from': 49,\n",
       "             'desc': 16,\n",
       "             'or': 34,\n",
       "             'not': 42,\n",
       "             'intersect': 34,\n",
       "             'except': 21,\n",
       "             'as': 93,\n",
       "             'join': 39,\n",
       "             'like': 12,\n",
       "             'and': 31,\n",
       "             '=': 191,\n",
       "             'having': 80,\n",
       "             '!=': 20,\n",
       "             'union': 6,\n",
       "             'limit': 6,\n",
       "             'sum': 21,\n",
       "             'asc': 19,\n",
       "             ')': 11,\n",
       "             'in': 4})"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_cnt_by_exp_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "            {'text': defaultdict(float,\n",
       "                         {'embed': 0.07181598544092065,\n",
       "                          'final_enc': 0.08561127370659098}),\n",
       "             'struct': defaultdict(float,\n",
       "                         {'embed': 0.9997616432579269,\n",
       "                          'final_enc': 0.9906901482785686}),\n",
       "             'columns': defaultdict(float,\n",
       "                         {'embed': 0.9989915059300397,\n",
       "                          'final_enc': 0.9982228384035804}),\n",
       "             'tables': defaultdict(float,\n",
       "                         {'embed': 0.9940267473124387,\n",
       "                          'final_enc': 0.9992919242783879}),\n",
       "             'all': defaultdict(float,\n",
       "                         {'embed': 0.022402080392785025,\n",
       "                          'final_enc': 0.07584266595464821})})"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_avg_by_exp_tok['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "sect_k = 'struct'\n",
    "layer_k = 'embed'\n",
    "scores_d = dict()\n",
    "\n",
    "for exp_tok, d1 in trace_scores_avg_by_exp_tok.items():\n",
    "    s = d1[sect_k][layer_k]\n",
    "    scores_d[exp_tok] = s\n",
    "    # print(f'{exp_tok:<10s}{s:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like      1.0000\n",
      "min       1.0000\n",
      "count     0.9998\n",
      "avg       0.9997\n",
      "limit     0.9993\n",
      "!=        0.9968\n",
      "union     0.9831\n",
      "or        0.9807\n",
      ">         0.9722\n",
      "having    0.9691\n",
      "intersect 0.9634\n",
      "except    0.9442\n",
      "sum       0.9386\n",
      "group     0.9373\n",
      "order     0.9079\n",
      ")         0.9057\n",
      "max       0.9033\n",
      "desc      0.8710\n",
      "where     0.8684\n",
      "distinct  0.8665\n",
      "(         0.8557\n",
      "between   0.8547\n",
      "asc       0.8026\n",
      "not       0.7477\n",
      "and       0.6863\n",
      "from      0.6251\n",
      "=         0.6110\n",
      "in        0.4806\n",
      "as        0.2753\n",
      "join      0.0001\n"
     ]
    }
   ],
   "source": [
    "for k, s in sorted(scores_d.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f'{k:<10s}{s:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from      0.8645\n",
      "as        0.8363\n",
      "=         0.8272\n",
      "in        0.7407\n",
      ")         0.7101\n",
      "desc      0.4644\n",
      "where     0.3572\n",
      ">         0.3472\n",
      "join      0.3131\n",
      "not       0.2707\n",
      "and       0.2300\n",
      "having    0.1737\n",
      "min       0.1698\n",
      "(         0.1657\n",
      "like      0.1639\n",
      "union     0.1435\n",
      "order     0.1126\n",
      "group     0.1046\n",
      "asc       0.0877\n",
      "count     0.0718\n",
      "avg       0.0311\n",
      "or        0.0211\n",
      "except    0.0106\n",
      "distinct  0.0097\n",
      "max       0.0073\n",
      "sum       0.0017\n",
      "intersect 0.0005\n",
      "!=        0.0000\n",
      "between   0.0000\n",
      "limit     0.0000\n"
     ]
    }
   ],
   "source": [
    "sect_k = 'text'\n",
    "layer_k = 'embed'\n",
    "scores_d = dict()\n",
    "\n",
    "for exp_tok, d1 in trace_scores_avg_by_exp_tok.items():\n",
    "    s = d1[sect_k][layer_k]\n",
    "    scores_d[exp_tok] = s\n",
    "    # print(f'{exp_tok:<10s}{s:.4f}')\n",
    "\n",
    "for k, s in sorted(scores_d.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f'{k:<10s}{s:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corrupted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict[str, Dict[str, int]]: exp_tok -> c_ans, cnt\n",
    "confusion_counter = defaultdict(Counter)\n",
    "\n",
    "for d in good_samples:\n",
    "    exp_tok = d['expect']\n",
    "    c_ans = d['corrupted_answer']\n",
    "    if exp_tok.isnumeric():\n",
    "        exp_tok = 'NUM'\n",
    "    if c_ans.isnumeric():\n",
    "        c_ans = 'NUM'\n",
    "    confusion_counter[exp_tok][c_ans] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter,\n",
       "            {'count': Counter({'*': 182, '': 64, 'sum': 19, 'count': 2}),\n",
       "             'order': Counter({'</s>': 96,\n",
       "                      'join': 12,\n",
       "                      ')': 2,\n",
       "                      'where': 9,\n",
       "                      'NUM': 5,\n",
       "                      'order': 3,\n",
       "                      'union': 2,\n",
       "                      'and': 2,\n",
       "                      'select': 4,\n",
       "                      's': 6,\n",
       "                      '_': 1}),\n",
       "             'avg': Counter({'*tvg': 43,\n",
       "                      'maxtvg': 8,\n",
       "                      'maxavg': 2,\n",
       "                      'tvg': 6,\n",
       "                      'mintvg': 2,\n",
       "                      'counttvg': 4}),\n",
       "             'min': Counter({'': 9, 'max': 5, '*': 4}),\n",
       "             'max': Counter({'': 13, '*': 16, 'min': 1}),\n",
       "             'where': Counter({'group': 19,\n",
       "                      'except': 2,\n",
       "                      '</s>': 239,\n",
       "                      'order': 114,\n",
       "                      'where': 9,\n",
       "                      ')': 16,\n",
       "                      ';': 5,\n",
       "                      'join': 36,\n",
       "                      'in': 3,\n",
       "                      'union': 2,\n",
       "                      'r': 3,\n",
       "                      '.': 5,\n",
       "                      'as': 17,\n",
       "                      'select': 2,\n",
       "                      '': 1,\n",
       "                      'class': 1,\n",
       "                      's': 10}),\n",
       "             'distinct': Counter({'*': 25}),\n",
       "             '>': Counter({'=': 55, 'in': 2, '</s>': 1, 'not': 1, 'des': 2}),\n",
       "             'NUM': Counter({'=': 70,\n",
       "                      't': 2,\n",
       "                      'NUM': 55,\n",
       "                      '\"': 22,\n",
       "                      '': 12,\n",
       "                      '(': 2,\n",
       "                      '=000': 6,\n",
       "                      ' and': 3,\n",
       "                      '=</s>': 4,\n",
       "                      '= join': 1,\n",
       "                      '3-': 1,\n",
       "                      '3%': 1,\n",
       "                      '%': 2,\n",
       "                      ')': 2,\n",
       "                      '</s>': 2}),\n",
       "             'group': Counter({'join': 21,\n",
       "                      '</s>': 98,\n",
       "                      'order': 29,\n",
       "                      'as': 16,\n",
       "                      'where': 25,\n",
       "                      'group': 3,\n",
       "                      'air': 5,\n",
       "                      '.': 4,\n",
       "                      'r': 1,\n",
       "                      'in': 2,\n",
       "                      'NUM': 2,\n",
       "                      'from': 2,\n",
       "                      'form': 2,\n",
       "                      'union': 4,\n",
       "                      's': 2,\n",
       "                      'express': 2,\n",
       "                      'l': 1,\n",
       "                      ';': 2,\n",
       "                      '=': 2,\n",
       "                      'and': 2}),\n",
       "             '(': Counter({'=': 11, '': 5, 'NUM': 6}),\n",
       "             'between': Counter({'>': 4, '=': 2}),\n",
       "             'from': Counter({'(': 1,\n",
       "                      'a': 2,\n",
       "                      '': 5,\n",
       "                      ')': 5,\n",
       "                      'ious': 2,\n",
       "                      ';': 1,\n",
       "                      'from': 2,\n",
       "                      'up': 2,\n",
       "                      'i': 2,\n",
       "                      'air': 1,\n",
       "                      'de': 8,\n",
       "                      'le': 2,\n",
       "                      'group': 2,\n",
       "                      '=': 2,\n",
       "                      '-': 2,\n",
       "                      ',': 2,\n",
       "                      'al': 4,\n",
       "                      'ture': 2,\n",
       "                      'call': 2}),\n",
       "             'desc': Counter({'desc': 3,\n",
       "                      'c': 2,\n",
       "                      'sc': 1,\n",
       "                      'ec': 2,\n",
       "                      '</s>c': 4,\n",
       "                      ',c': 1,\n",
       "                      'fromc': 2,\n",
       "                      'limitc': 1}),\n",
       "             'or': Counter({'</s>': 28, 'NUM': 2, 'order': 2, 'and': 2}),\n",
       "             '>=': Counter({'= 60': 1}),\n",
       "             'not': Counter({'=': 40, '': 2}),\n",
       "             'intersect': Counter({'and': 3,\n",
       "                      '</s>': 22,\n",
       "                      '-': 1,\n",
       "                      '-04': 4,\n",
       "                      'order': 4}),\n",
       "             'except': Counter({'order': 10, 'join': 4, '</s>': 7}),\n",
       "             'as': Counter({'_': 2,\n",
       "                      'i': 8,\n",
       "                      'order': 6,\n",
       "                      '</s>': 23,\n",
       "                      'er': 21,\n",
       "                      're': 1,\n",
       "                      'base': 6,\n",
       "                      'join': 7,\n",
       "                      'as': 3,\n",
       "                      ';': 2,\n",
       "                      'ment': 12,\n",
       "                      'in': 2}),\n",
       "             'join': Counter({'</s>': 32,\n",
       "                      ')': 2,\n",
       "                      'where': 1,\n",
       "                      'y': 2,\n",
       "                      'order': 1,\n",
       "                      'join': 1}),\n",
       "             'like': Counter({'=': 8, 'not': 1, 'in': 2, '': 1}),\n",
       "             'and': Counter({'</s>': 29, 'group': 2}),\n",
       "             '=': Counter({'a': 2,\n",
       "                      '</s>': 11,\n",
       "                      'y': 4,\n",
       "                      'ance': 4,\n",
       "                      '_': 5,\n",
       "                      'up': 2,\n",
       "                      'al': 2,\n",
       "                      'not': 39,\n",
       "                      'in': 34,\n",
       "                      '': 33,\n",
       "                      '.': 1,\n",
       "                      'd': 2,\n",
       "                      '=': 8,\n",
       "                      ';': 1,\n",
       "                      '>': 4,\n",
       "                      '\"': 9,\n",
       "                      's': 19,\n",
       "                      'ment': 1,\n",
       "                      'language': 4,\n",
       "                      'foreign': 3,\n",
       "                      '*': 1,\n",
       "                      'c': 2}),\n",
       "             'having': Counter({'</s>': 47, 'order': 31, 'a': 2}),\n",
       "             '!=': Counter({'=!=': 16, 'in!=': 2, '=t=': 1, '!=': 1}),\n",
       "             'union': Counter({'NUM': 2, '</s>': 2, 'as': 2}),\n",
       "             'limit': Counter({'des': 6}),\n",
       "             'sum': Counter({'*': 12, 'count': 5, '': 4}),\n",
       "             'asc': Counter({'desc': 17, '</s>c': 2}),\n",
       "             ')': Counter({'group)': 2, ')': 1, 't': 4, '))': 2, ');': 2}),\n",
       "             ',': Counter({'joint': 1}),\n",
       "             'in': Counter({'=': 4})})"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select song_name from singer where age > --> = (()\n",
      "select song_name from singer where age > --> = (()\n",
      "select count(*) from concert where stadium_id = -->  (()\n",
      "select count(*) from concert where stadium_id = -->  (()\n",
      "select t2.make, t1.year from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t1.year = --> 2004 (()\n",
      "select t2.make, t1.year from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t1.year = --> 2004 (()\n",
      "select count(*) from cars_data where accelerate > --> = (()\n",
      "select count(*) from cars_data where accelerate > --> = (()\n",
      "select t2.makeid, t2.make from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t1.horsepower > -->  (()\n",
      "select name from shop where number_products > --> = (()\n",
      "select name from shop where number_products > --> = (()\n",
      "select name from museum where num_of_staff > --> = (()\n",
      "select name from country where surfacearea > --> = (()\n",
      "select name from country where surfacearea > --> = (()\n",
      "select name from country where continent = \"Asia\" and population > --> = (()\n",
      "select name from country where continent = \"Asia\" and population > --> = (()\n",
      "select count(*), district from city where population > --> 100 (()\n",
      "select count(*), district from city where population > --> 100 (()\n",
      "select t1.name, t2.date_of_treatment from dogs as t1 join treatments as t2 on t1.dog_id = t2.dog_id where t1.breed_code = -->  (()\n",
      "select t1.name, t2.date_of_treatment from dogs as t1 join treatments as t2 on t1.dog_id = t2.dog_id where t1.breed_code = -->  (()\n",
      "select t1.last_name from owners as t1 join dogs as t2 on t1.owner_id = t2.owner_id where t2.age = --> 1 (()\n",
      "select t1.last_name from owners as t1 join dogs as t2 on t1.owner_id = t2.owner_id where t2.age = --> 1 (()\n"
     ]
    }
   ],
   "source": [
    "for idx in trace_sample_ids_by_exp_tok['(']:\n",
    "    d = good_samples[idx]\n",
    "    print(f\"{d['dec_prompt']} --> {d['corrupted_answer']} ({d['expect']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp-6.1: attention corruption effect - syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expect_type = 'table_alias'\n",
    "\n",
    "res_path = f'/home/yshao/Projects/rome/results/exp6_1_attention_corruption_effect_syntax/exp=6.1_dev_encoder_corrupt=zero.jsonl'\n",
    "\n",
    "with open(res_path, 'r') as f:\n",
    "    all_samples = [json.loads(l) for l in f]\n",
    "len(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = 0\n",
    "n_good_samples = 0\n",
    "n_too_hard = 0      # wrong answer \n",
    "n_too_easy = 0      # base - low < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10233, (2524, 2524), 1623, 6086, 7709, 'good / correct = 2524 / 8610')"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_samples = []\n",
    "bad_samples = []\n",
    "\n",
    "for i, ex in enumerate(all_samples):\n",
    "    for d in ex['trace_results']:\n",
    "        total_samples += 1\n",
    "\n",
    "        if d.get('is_good_sample', True):\n",
    "            n_good_samples += 1\n",
    "            d['ex_id'] = i\n",
    "            good_samples.append(d)\n",
    "        elif not d['correct_prediction']:\n",
    "            n_too_hard += 1\n",
    "            bad_samples.append(d)\n",
    "        else:\n",
    "            assert d['base_score'] - d['low_score'] < 0.5\n",
    "            n_too_easy += 1\n",
    "            bad_samples.append(d)\n",
    "            \n",
    "total_samples, (n_good_samples, len(good_samples)), n_too_hard, n_too_easy, len(bad_samples), \\\n",
    "f'good / correct = {n_good_samples} / {n_good_samples + n_too_easy}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples[0]['trace_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_scores_avg = {sect_k : defaultdict(int) for sect_k in good_samples[0]['trace_scores'].keys()}\n",
    "\n",
    "for d in good_samples:\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for k, v in sect_d.items():\n",
    "            trace_scores_avg[sect_k][k] += v\n",
    "\n",
    "for sect_k, sect_d in trace_scores_avg.items():\n",
    "    for k, s in sect_d.items():\n",
    "        sect_d[k] = s / len(good_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t->s': defaultdict(int,\n",
       "             {'low_layers': 0.990021244916296,\n",
       "              'high_layers': 0.9839921388335588,\n",
       "              'all_layers': 0.9773429089978287}),\n",
       " 's->t': defaultdict(int,\n",
       "             {'low_layers': 0.9933494876220781,\n",
       "              'high_layers': 0.9826286324625741,\n",
       "              'all_layers': 0.9748112447138703}),\n",
       " 't<->s': defaultdict(int,\n",
       "             {'low_layers': 0.988124023103812,\n",
       "              'high_layers': 0.9725914119834563,\n",
       "              'all_layers': 0.9583643697837391}),\n",
       " 't->p': defaultdict(int,\n",
       "             {'low_layers': 0.9813802007457678,\n",
       "              'high_layers': 0.9107886133535986,\n",
       "              'all_layers': 0.8583388319168208}),\n",
       " 's->p': defaultdict(int,\n",
       "             {'low_layers': 0.9871366122154147,\n",
       "              'high_layers': 0.949990724645636,\n",
       "              'all_layers': 0.9415907011434735}),\n",
       " 'ts->p': defaultdict(int,\n",
       "             {'low_layers': 0.9728856815640369,\n",
       "              'high_layers': 0.8155521331662294,\n",
       "              'all_layers': 0.6506533935901703}),\n",
       " 't->t': defaultdict(int,\n",
       "             {'low_layers': 0.9668341711986165,\n",
       "              'high_layers': 0.9418916222192101,\n",
       "              'all_layers': 0.8061599506561392}),\n",
       " 's->s': defaultdict(int,\n",
       "             {'low_layers': 0.9435112398249293,\n",
       "              'high_layers': 0.9211638388779134,\n",
       "              'all_layers': 0.7678922974595231}),\n",
       " 'all': defaultdict(int,\n",
       "             {'low_layers': 0.7464991536140425,\n",
       "              'high_layers': 0.44798142381179973,\n",
       "              'all_layers': 0.06831362223590848})}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corruption overall effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict[str, int]: expect_tok -> num of effective / not effective corruptions (all)\n",
    "eff_counter = Counter()\n",
    "neff_counter = Counter()\n",
    "\n",
    "for d in good_samples:\n",
    "    eff_counter[d['expect']] += 1\n",
    "for d in bad_samples:\n",
    "    if d['correct_prediction']:\n",
    "        # \"too easy\", corruption not effective \n",
    "        neff_counter[d['expect']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_rate_d = dict()\n",
    "\n",
    "for k in list(set(eff_counter.keys()) | set(neff_counter.keys())):\n",
    "    eff_c = eff_counter[k]\n",
    "    neff_c = neff_counter[k]\n",
    "    eff_r = 1.0 * eff_c / (eff_c + neff_c)\n",
    "    eff_rate_d[k] = eff_r\n",
    "    # print(f'{k:<10s}{eff_c:5d} /{eff_c + neff_c:5d} = {eff_r:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union\t6\t6\t1.0000\n",
      "!=\t20\t20\t1.0000\n",
      "like\t12\t12\t1.0000\n",
      "or\t34\t34\t1.0000\n",
      "asc\t19\t19\t1.0000\n",
      "distinct\t26\t26\t1.0000\n",
      "between\t6\t6\t1.0000\n",
      "except\t21\t21\t1.0000\n",
      "intersect\t34\t34\t1.0000\n",
      "not\t45\t46\t0.9783\n",
      "avg\t63\t65\t0.9692\n",
      "max\t29\t30\t0.9667\n",
      "having\t77\t81\t0.9506\n",
      "group\t241\t265\t0.9094\n",
      "sum\t20\t22\t0.9091\n",
      "order\t197\t221\t0.8914\n",
      "min\t14\t18\t0.7778\n",
      "and\t29\t39\t0.7436\n",
      "count\t294\t406\t0.7241\n",
      "where\t350\t516\t0.6783\n",
      ">\t68\t101\t0.6733\n",
      ")\t13\t23\t0.5652\n",
      "desc\t82\t164\t0.5000\n",
      ">=\t11\t30\t0.3667\n",
      "from\t263\t1196\t0.2199\n",
      "in\t8\t50\t0.1600\n",
      "limit\t26\t177\t0.1469\n",
      "(\t98\t675\t0.1452\n",
      "=\t128\t968\t0.1322\n",
      "join\t44\t496\t0.0887\n",
      "as\t52\t952\t0.0546\n",
      "*\t10\t381\t0.0262\n",
      "by\t0\t516\t0.0000\n",
      "on\t0\t516\t0.0000\n",
      "select\t0\t88\t0.0000\n"
     ]
    }
   ],
   "source": [
    "for k, eff_r in sorted(eff_rate_d.items(), key=lambda x: x[1], reverse=True):\n",
    "    eff_c = eff_counter[k]\n",
    "    neff_c = neff_counter[k]\n",
    "    all_c = eff_c + neff_c\n",
    "    if all_c <= 2: continue\n",
    "    if k.isnumeric(): continue\n",
    "#     print(f'{k:<10s}{eff_c:5d} /{all_c:5d} = {eff_r:.4f}')\n",
    "    print(f'{k}\\t{eff_c}\\t{all_c}\\t{eff_r:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg by expect syntax token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key: (expect_tok, sect_k, layer) -> [scores]\n",
    "trace_scores_by_exp_tok = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "trace_scores_avg_by_exp_tok = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "trace_scores_cnt_by_exp_tok = defaultdict(int)  # no sect key & layer key \n",
    "\n",
    "trace_sample_ids_by_exp_tok = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(good_samples):\n",
    "    expect = d['expect']\n",
    "    trace_sample_ids_by_exp_tok[expect].append(i)\n",
    "    for sect_k, sect_d in d['trace_scores'].items():\n",
    "        for layer_k, v in sect_d.items():\n",
    "            trace_scores_by_exp_tok[expect][sect_k][layer_k].append(v)\n",
    "\n",
    "for exp_tok, d1 in trace_scores_by_exp_tok.items():\n",
    "    if exp_tok.isnumeric(): continue\n",
    "    for sect_k, d2 in d1.items():\n",
    "        for layer_k, scores in d2.items():\n",
    "            if len(scores) <= 2: continue\n",
    "            trace_scores_avg_by_exp_tok[exp_tok][sect_k][layer_k] = np.mean(scores)\n",
    "            trace_scores_cnt_by_exp_tok[exp_tok] = len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'count': 294,\n",
       "             'order': 197,\n",
       "             'desc': 82,\n",
       "             'avg': 63,\n",
       "             'min': 14,\n",
       "             'max': 29,\n",
       "             'where': 350,\n",
       "             '=': 128,\n",
       "             'distinct': 26,\n",
       "             'from': 263,\n",
       "             '>': 68,\n",
       "             'group': 241,\n",
       "             '(': 98,\n",
       "             'between': 6,\n",
       "             'limit': 26,\n",
       "             'or': 34,\n",
       "             '>=': 11,\n",
       "             'not': 45,\n",
       "             'intersect': 34,\n",
       "             'except': 21,\n",
       "             'as': 52,\n",
       "             'join': 44,\n",
       "             'like': 12,\n",
       "             'and': 29,\n",
       "             'in': 8,\n",
       "             'having': 77,\n",
       "             '!=': 20,\n",
       "             '*': 10,\n",
       "             'union': 6,\n",
       "             'sum': 20,\n",
       "             'asc': 19,\n",
       "             ')': 13})"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_cnt_by_exp_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "            {'t->s': defaultdict(float,\n",
       "                         {'low_layers': 0.9978861685107354,\n",
       "                          'high_layers': 0.9932368714425738,\n",
       "                          'all_layers': 0.9911034725571596}),\n",
       "             's->t': defaultdict(float,\n",
       "                         {'low_layers': 0.9994523721892817,\n",
       "                          'high_layers': 0.9979493310865091,\n",
       "                          'all_layers': 0.997748848329596}),\n",
       "             't<->s': defaultdict(float,\n",
       "                         {'low_layers': 0.998062480874613,\n",
       "                          'high_layers': 0.995554579890707,\n",
       "                          'all_layers': 0.9959644333643167}),\n",
       "             't->p': defaultdict(float,\n",
       "                         {'low_layers': 0.9941589293855347,\n",
       "                          'high_layers': 0.9844484830047099,\n",
       "                          'all_layers': 0.9640641826280487}),\n",
       "             's->p': defaultdict(float,\n",
       "                         {'low_layers': 0.9997543060049718,\n",
       "                          'high_layers': 0.9865310551390487,\n",
       "                          'all_layers': 0.9850442271527587}),\n",
       "             'ts->p': defaultdict(float,\n",
       "                         {'low_layers': 0.992053749685993,\n",
       "                          'high_layers': 0.9076947870914203,\n",
       "                          'all_layers': 0.7469466893840571}),\n",
       "             't->t': defaultdict(float,\n",
       "                         {'low_layers': 0.95961485076646,\n",
       "                          'high_layers': 0.9813218236079703,\n",
       "                          'all_layers': 0.8608877147373191}),\n",
       "             's->s': defaultdict(float,\n",
       "                         {'low_layers': 0.9938991388657068,\n",
       "                          'high_layers': 0.9873157823699856,\n",
       "                          'all_layers': 0.8691012370157356}),\n",
       "             'all': defaultdict(float,\n",
       "                         {'low_layers': 0.7315069800832695,\n",
       "                          'high_layers': 0.6354717421335181,\n",
       "                          'all_layers': 0.02197208333187588})})"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_scores_avg_by_exp_tok['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "sect_k = 't<->s'\n",
    "layer_k = 'all_layers'\n",
    "scores_d = dict()\n",
    "\n",
    "for exp_tok, d1 in trace_scores_avg_by_exp_tok.items():\n",
    "    s = d1[sect_k][layer_k]\n",
    "    scores_d[exp_tok] = s\n",
    "    # print(f'{exp_tok:<10s}{s:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">=        1.0000\n",
      ">         1.0000\n",
      "like      1.0000\n",
      "*         1.0000\n",
      "min       1.0000\n",
      "between   1.0000\n",
      "in        0.9992\n",
      "avg       0.9972\n",
      "count     0.9960\n",
      "(         0.9946\n",
      "desc      0.9942\n",
      "having    0.9894\n",
      "order     0.9826\n",
      "!=        0.9825\n",
      "=         0.9786\n",
      "group     0.9771\n",
      "max       0.9741\n",
      ")         0.9716\n",
      "where     0.9702\n",
      "limit     0.9684\n",
      "from      0.9671\n",
      "or        0.9582\n",
      "intersect 0.9522\n",
      "asc       0.9437\n",
      "except    0.9415\n",
      "not       0.9360\n",
      "sum       0.8969\n",
      "as        0.8349\n",
      "distinct  0.8284\n",
      "union     0.8037\n",
      "and       0.6727\n",
      "join      0.4976\n"
     ]
    }
   ],
   "source": [
    "for k, s in sorted(scores_d.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f'{k:<10s}{s:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exp_toks = sorted(list(trace_scores_cnt_by_exp_tok.keys()))\n",
    "all_sections = list(good_samples[0]['trace_scores'].keys())\n",
    "\n",
    "print_str = '\\t'.join(['Syntax-tok'] + all_sections + ['Eff_cnt', 'All_cnt', 'Eff_rate']) + '\\n'\n",
    "\n",
    "for exp_tok in all_exp_toks:\n",
    "    print_str += f'{exp_tok:<10s}'\n",
    "    for sect_k in all_sections:\n",
    "        s = trace_scores_avg_by_exp_tok[exp_tok][sect_k]['all_layers']\n",
    "        print_str += f'\\t{s:.4f}'\n",
    "    eff_c = eff_counter[exp_tok]\n",
    "    neff_c = neff_counter[exp_tok]\n",
    "    all_c = eff_c + neff_c\n",
    "    eff_r = eff_rate_d[exp_tok]\n",
    "    print_str += f'\\t{eff_c:<7d}\\t{all_c:<7d}\\t{eff_r:.4f}'\n",
    "    print_str += '\\n'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax-tok\tt->s\ts->t\tt<->s\tt->p\ts->p\tts->p\tt->t\ts->s\tall\tEff_cnt\tAll_cnt\tEff_rate\n",
      "!=        \t0.9984\t0.9995\t0.9825\t0.8485\t0.9998\t0.9070\t0.3025\t0.9193\t0.0284\t20     \t20     \t1.0000\n",
      "(         \t0.9999\t0.9794\t0.9946\t0.9719\t0.9795\t0.9685\t0.8983\t0.9467\t0.1856\t98     \t675    \t0.1452\n",
      ")         \t0.9257\t0.9926\t0.9716\t0.9914\t0.9074\t0.8420\t0.9244\t0.4715\t0.1108\t13     \t23     \t0.5652\n",
      "*         \t1.0000\t0.9999\t1.0000\t0.9997\t0.9999\t0.9998\t0.9999\t0.9982\t0.1802\t10     \t381    \t0.0262\n",
      "=         \t0.9784\t0.9923\t0.9786\t0.9378\t0.8940\t0.7439\t0.9466\t0.5933\t0.1149\t128    \t968    \t0.1322\n",
      ">         \t1.0000\t1.0000\t1.0000\t0.9509\t0.9990\t0.9652\t0.8193\t0.9845\t0.0219\t68     \t101    \t0.6733\n",
      ">=        \t1.0000\t1.0000\t1.0000\t0.9151\t1.0000\t0.9048\t0.2593\t0.9964\t0.1703\t11     \t30     \t0.3667\n",
      "and       \t0.7931\t0.8981\t0.6727\t0.4962\t0.8409\t0.3468\t0.3603\t0.6749\t0.0554\t29     \t39     \t0.7436\n",
      "as        \t0.9212\t0.8936\t0.8349\t0.7992\t0.5034\t0.3457\t0.8478\t0.3288\t0.1002\t52     \t952    \t0.0546\n",
      "asc       \t0.9723\t0.9096\t0.9437\t0.4939\t0.8596\t0.3612\t0.6133\t0.6725\t0.0408\t19     \t19     \t1.0000\n",
      "avg       \t0.9996\t0.9940\t0.9972\t0.7660\t0.9996\t0.3619\t0.7743\t0.9513\t0.0360\t63     \t65     \t0.9692\n",
      "between   \t0.9999\t0.9999\t1.0000\t0.7788\t0.9999\t0.3407\t0.0009\t0.9999\t0.0000\t6      \t6      \t1.0000\n",
      "count     \t0.9911\t0.9977\t0.9960\t0.9641\t0.9850\t0.7469\t0.8609\t0.8691\t0.0220\t294    \t406    \t0.7241\n",
      "desc      \t0.9939\t0.9992\t0.9942\t0.9198\t0.9191\t0.7296\t0.9044\t0.6997\t0.1144\t82     \t164    \t0.5000\n",
      "distinct  \t0.8663\t0.8651\t0.8284\t0.6270\t0.9118\t0.4787\t0.6194\t0.7636\t0.0014\t26     \t26     \t1.0000\n",
      "except    \t0.9517\t0.9847\t0.9415\t0.6213\t0.9311\t0.0783\t0.4062\t0.7137\t0.0002\t21     \t21     \t1.0000\n",
      "from      \t0.9924\t0.9703\t0.9671\t0.9781\t0.9519\t0.6276\t0.8407\t0.5194\t0.1384\t263    \t1196   \t0.2199\n",
      "group     \t0.9895\t0.9925\t0.9771\t0.8332\t0.9838\t0.5694\t0.9167\t0.7823\t0.0282\t241    \t265    \t0.9094\n",
      "having    \t0.9978\t0.9924\t0.9894\t0.5962\t0.9993\t0.3893\t0.8985\t0.9348\t0.0262\t77     \t81     \t0.9506\n",
      "in        \t0.9973\t0.9995\t0.9992\t0.9995\t0.9253\t0.5859\t0.7717\t0.3267\t0.1096\t8      \t50     \t0.1600\n",
      "intersect \t0.9649\t0.9938\t0.9522\t0.2385\t0.9592\t0.0575\t0.4348\t0.8243\t0.0001\t34     \t34     \t1.0000\n",
      "join      \t0.9099\t0.6489\t0.4976\t0.7286\t0.3030\t0.1984\t0.7286\t0.1891\t0.0158\t44     \t496    \t0.0887\n",
      "like      \t1.0000\t1.0000\t1.0000\t0.9997\t0.9997\t0.8233\t0.2700\t0.8809\t0.0062\t12     \t12     \t1.0000\n",
      "limit     \t0.9932\t0.9854\t0.9684\t0.8400\t0.9992\t0.7895\t0.8076\t0.7534\t0.1531\t26     \t177    \t0.1469\n",
      "max       \t0.8483\t0.9925\t0.9741\t0.8141\t0.9458\t0.4659\t0.5214\t0.9008\t0.0633\t29     \t30     \t0.9667\n",
      "min       \t0.9962\t1.0000\t1.0000\t0.9906\t0.9993\t0.9377\t0.7412\t0.9999\t0.0450\t14     \t18     \t0.7778\n",
      "not       \t0.9884\t0.9515\t0.9360\t0.9510\t0.9067\t0.6624\t0.8928\t0.7208\t0.0117\t45     \t46     \t0.9783\n",
      "or        \t0.9788\t0.9728\t0.9582\t0.7773\t0.9856\t0.9181\t0.4193\t0.9718\t0.0311\t34     \t34     \t1.0000\n",
      "order     \t0.9964\t0.9891\t0.9826\t0.7079\t0.9802\t0.4836\t0.8542\t0.8393\t0.0680\t197    \t221    \t0.8914\n",
      "sum       \t0.9054\t0.9988\t0.8969\t0.8431\t0.9998\t0.6812\t0.4672\t0.9511\t0.0119\t20     \t22     \t0.9091\n",
      "union     \t0.9990\t0.8977\t0.8037\t0.2034\t0.7390\t0.3218\t0.0174\t0.7111\t0.0001\t6      \t6      \t1.0000\n",
      "where     \t0.9820\t0.9900\t0.9702\t0.9035\t0.9576\t0.7533\t0.8924\t0.7699\t0.0858\t350    \t516    \t0.6783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '!', '=']\n",
      "['(']\n",
      "['', ')']\n",
      "['*']\n",
      "['=']\n",
      "['>']\n",
      "['>', '=']\n",
      "['and']\n",
      "['as']\n",
      "['as', 'c']\n",
      "['', 'a', 'v', 'g']\n",
      "['between']\n",
      "['count']\n",
      "['des', 'c']\n",
      "['distinct']\n",
      "['except']\n",
      "['from']\n",
      "['group']\n",
      "['having']\n",
      "['in']\n",
      "['intersect']\n",
      "['join']\n",
      "['like']\n",
      "['limit']\n",
      "['max']\n",
      "['min']\n",
      "['not']\n",
      "['or']\n",
      "['order']\n",
      "['sum']\n",
      "['union']\n",
      "['where']\n"
     ]
    }
   ],
   "source": [
    "# Issue checking: multi-token \n",
    "for exp_tok in all_exp_toks:\n",
    "    print(mt_uskg.tokenizer.tokenize(exp_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_analysis_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the accelerate of the car make amc hornet sportabout (sw)?',\n",
       " '| car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker ( amc ) , fullname , country | model_list : modelid , maker , model ( amc ) | car_names : makeid , model ( amc ) , make ( amc hornet , amc hornet sportabout (sw) ) | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " \"select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t2.make = 'amc hornet sportabout (sw)';\")"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_id = 111\n",
    "a_ex_id = 0\n",
    "\n",
    "ex = processed_spider_dev[ex_id]\n",
    "ex['text_in'], \\\n",
    "ex['struct_in'], \\\n",
    "ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp test\n",
    "# ex['seq_out'] = 'select year from cars_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex_list = ctu.create_analysis_sample_dicts(\n",
    "                mt_uskg, ex,\n",
    "                subject_type='column',\n",
    "                remove_struct_duplicate_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph', 'serialized_schema', 'struct_in', 'text_in', 'seq_out', 'enc_sentence', 'enc_tokenized', 'text_range', 'struct_range', 'struct_node_ranges_dict', 'dec_prompt', 'expect', 'expect_type', 'remove_struct_duplicate_nodes', 'parsed_struct_in', 'col2table', 'token_ranges_dict', 'node_name_ranges', 'expect_input_ranges', 'alias2table', 'self_ranges', 'context_ranges', 'category'])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex_list[a_ex_id].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1': 'cars_data', 't2': 'car_names'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex_list[a_ex_id]['alias2table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(d['dec_prompt'], d['expect'], d['node_name_ranges'], d['expect_input_ranges'], '------',\\\n",
    "  d['self_ranges'], d['context_ranges'],\\\n",
    "  d['category'], '------' * 2) for d in a_ex_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(a_ex_list[a_ex_id])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ctu.add_clean_prediction(mt_uskg, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse_sql_alias2table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1': 'table_name', 't2': 'other_table', 't3': 'ttt'}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sql = 'SELECT t2.aaa , t3.ccc FROM table_name as t1 JOIN other_table as t2 on table_name.a_a = other_table.b_a JOIN ttt as t3 on other_table.asth = ttt.asth'\n",
    "ctu.parse_sql_alias2table(_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from \t None \t False\n",
      "as \t None \t False\n",
      "join \t None \t False\n",
      "as \t None \t False\n",
      "on \t None \t False\n",
      "= \t None \t False\n",
      "where \t None \t False\n",
      "= \t None \t False\n",
      "' \t ' \t True\n",
      "amc \t ' \t True\n",
      "hornet \t ' \t True\n",
      "sportabout \t ' \t True\n",
      "( \t ' \t True\n",
      "sw \t ' \t True\n",
      ") \t ' \t True\n",
      "' \t None \t True\n"
     ]
    }
   ],
   "source": [
    "_ex = copy.deepcopy(ex)\n",
    "# _ex['seq_out'] += 'order by t1.mpg'\n",
    "a_ex_list_syntax = ctu.create_syntax_analysis_sample_dicts(mt_uskg, _ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select t1.accelerate  -->  from\n",
      "select t1.accelerate from cars_data  -->  as\n",
      "select t1.accelerate from cars_data as t1  -->  join\n",
      "select t1.accelerate from cars_data as t1 join car_names  -->  as\n",
      "select t1.accelerate from cars_data as t1 join car_names as t2  -->  on\n",
      "select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id  -->  =\n",
      "select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid  -->  where\n",
      "select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t2.make  -->  =\n"
     ]
    }
   ],
   "source": [
    "for a_ex in a_ex_list_syntax:\n",
    "    print(a_ex['dec_prompt'], ' --> ', a_ex['expect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [363, 19, 8, 16845, 13, 8, 443, 143, 183, 75, 3, 6293, 15, 17, 2600, 7932, 41, 7, 210, 61, 58, 117, 3, 7593, 15, 26, 1103, 10, 1820, 443, 834, 536, 1820, 10829, 7, 3, 10, 3622, 23, 26, 3, 6, 10829, 1820, 1440, 3, 10, 684, 23, 26, 3, 6, 684, 4350, 3, 6, 10829, 1820, 443, 834, 8910, 3, 10, 3, 23, 26, 3, 6, 13762, 41, 183, 75, 3, 61, 3, 6, 423, 4350, 3, 6, 684, 1820, 825, 834, 3350, 3, 10, 825, 23, 26, 3, 6, 13762, 3, 6, 825, 41, 183, 75, 3, 61, 1820, 443, 834, 4350, 7, 3, 10, 143, 23, 26, 3, 6, 825, 41, 183, 75, 3, 61, 3, 6, 143, 41, 183, 75, 3, 6293, 15, 17, 3, 6, 183, 75, 3, 6293, 15, 17, 2600, 7932, 41, 7, 210, 61, 3, 61, 1820, 2948, 834, 6757, 3, 10, 3, 23, 26, 3, 6, 3, 1167, 122, 3, 6, 3, 12980, 7, 3, 6, 3, 15, 10475, 40, 3, 6, 28906, 3, 6, 1293, 3, 6, 16845, 3, 6, 215, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex['enc_tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contid']\n",
      "['continent', 'continent']\n",
      "['countryid']\n",
      "['countryname']\n",
      "['id', 'id']\n",
      "['maker ( amc )', 'maker']\n",
      "['fullname']\n",
      "['country']\n",
      "['modelid']\n",
      "['model ( amc )', 'model ( amc )']\n",
      "['makeid']\n",
      "['make ( amc hornet, amc hornet sportabout (sw) )']\n",
      "['mpg']\n",
      "['cylinders']\n",
      "['edispl']\n",
      "['horsepower']\n",
      "['weight']\n",
      "['accelerate']\n",
      "['year']\n"
     ]
    }
   ],
   "source": [
    "col_name_ranges = a_ex['token_ranges_dict']['col_name_ranges']\n",
    "# col_name_indices = [i for s, e in col_name_ranges.values() for i in range(s, e)]\n",
    "for ranges in col_name_ranges.values():\n",
    "    print([mt_uskg.tokenizer.decode(a_ex['enc_tokenized']['input_ids'][s:e]) for s, e in ranges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['continents']\n",
      "['countries']\n",
      "['car_makers']\n",
      "['model_list']\n",
      "['car_names']\n",
      "['cars_data']\n"
     ]
    }
   ],
   "source": [
    "table_name_ranges = a_ex['token_ranges_dict']['table_name_ranges']\n",
    "# col_name_indices = [i for s, e in col_name_ranges.values() for i in range(s, e)]\n",
    "for ranges in table_name_ranges.values():\n",
    "    print([mt_uskg.tokenizer.decode(a_ex['enc_tokenized']['input_ids'][s:e]) for s, e in ranges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'continents': [(33, 35)],\n",
       "             'countries': [(44, 45)],\n",
       "             'car_makers': [(58, 61)],\n",
       "             'model_list': [(82, 85)],\n",
       "             'car_names': [(102, 106)],\n",
       "             'cars_data': [(146, 149)]})"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['select', 't2.', 'aaa', ',', 'distinct', '(', 't3.', 'ccc', ')', ',', 'count', '(', '*', ')', 'from', 'table_name', 'as', 't1', 'join', 'other_table', 'as', 't2', 'on', 'table_name', '.', 'a_a', '=', 'other_table', '.', 'b_a', 'join', 'ttt', 'as', 't3', 'on', 'other_table', '.', 'asth', '=', 'ttt', '.', 'asth', 'where', 't2.', 'col', 'like', '%', 'hey', '%', 'and', 't3.', 'p', '<=', '40']\n"
     ]
    }
   ],
   "source": [
    "_sql = 'SELECT t2.aaa, DISTINCT(t3.ccc), COUNT(*) FROM table_name as t1 JOIN other_table as t2 on table_name.a_a = other_table.b_a JOIN ttt as t3 on other_table.asth = ttt.asth WHERE t2.col like %hey% AND t3.p <= 40'.lower()\n",
    "_tok_ranges = ctu.separate_punct_by_offset(_sql)\n",
    "print([_sql[s:e] for s, e in _tok_ranges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['which', 'school', 'is', 'good', '?', '', 'struct', 'e', 'd', '_', 'know', 'ledge', ':', 'school', '|', 'school', '', ':', 'school', '_', 'name', ',', 'is', '_', 'good']\n"
     ]
    }
   ],
   "source": [
    "_toks = mt_uskg.tokenizer.tokenize('which school is good? structed_knowledge: school | school : school_name, is_good')\n",
    "print(len(_toks), _toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_a_ex = {\n",
    "    'enc_sentence': 'which school is good? structed_knowledge: school | school : school_name, is_good',\n",
    "    'dec_prompt': 'select distinct',\n",
    "    'expect': 'school_name',  # ['school', '_', 'name']\n",
    "    'answers_t': [1,2,3],\n",
    "    'answer': 'school_name',\n",
    "    'text_range': [0, 5],\n",
    "    'struct_range': [15, 25],\n",
    "    'self_ranges': [[18, 21]],\n",
    "    'context_ranges': [[15, 18], [21, 25]],\n",
    "}\n",
    "\n",
    "_test_att_masks = ctu.build_dec_cross_attention_mask(\n",
    "    a_ex=_test_a_ex,\n",
    "    mt=mt_uskg,\n",
    "    use_self_node=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_att_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex = dict(a_ex_list[a_ex_id])\n",
    "a_ex = ctu.add_clean_prediction(mt_uskg, a_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enc_sentence': 'Which city has the most frequent destination airport?; structed knowledge: | flight_2 | airlines : uid , airline , abbreviation , country | airports : city , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport',\n",
       " 'seq_out': 'select t1.city from airports as t1 join flights as t2 on t1.airportcode = t2.destairport group by t1.city order by count(*) desc limit 1',\n",
       " 'dec_prompt': 'select t1.city from airports as t1 join flights as t2 on t1.airportcode = t2.destairport group by t1.',\n",
       " 'expect': 'city',\n",
       " 'expect_type': 'column',\n",
       " 'db_id': 'flight_2',\n",
       " 'expect_input_ranges': [(45, 46)],\n",
       " 'expect_table': 'airports',\n",
       " 'answer': 'city',\n",
       " 'base_score': 0.9983423948287964,\n",
       " 'answers_t': [6726],\n",
       " 'correct_prediction': True,\n",
       " 'category': {'sql_hardness': 'extra',\n",
       "  'node_role': 'group by',\n",
       "  'text_match': 'exact'}}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ctu.make_basic_result_dict(a_ex)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sentence = a_ex['enc_sentence']\n",
    "dec_prompt = a_ex['dec_prompt']\n",
    "expect = a_ex['expect']\n",
    "answer = result['answer']\n",
    "answers_t = result['answers_t']\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence] * 11,\n",
    "    [dec_prompt] * 11,\n",
    "    answer=expect)\n",
    "\n",
    "text_range = a_ex['text_range']\n",
    "struct_range = a_ex['struct_range']\n",
    "\n",
    "self_ranges = a_ex['self_ranges']\n",
    "context_ranges = a_ex['context_ranges']\n",
    "\n",
    "self_tok_indices = [i for s, e in self_ranges for i in range(s, e)]\n",
    "context_tok_indices = corrupt_tok_indices = [i for s, e in context_ranges for i in range(s, e)]\n",
    "text_tok_indices = list(range(*text_range))\n",
    "struct_tok_indices = list(range(*struct_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "#                     for tnum in range(*struct_range)],\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6726], 'city', 0.8450507521629333)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_t, answer, _score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8450507521629333"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_to_corrupt = [(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", 0, \"embed\"))\n",
    "                for tnum in text_tok_indices]\n",
    "\n",
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    states_to_corrupt=states_to_corrupt,\n",
    "#     tokens_to_mix=corrupt_tok_indices,\n",
    "#     tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ").item()\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9952, device='cuda:0')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pair of identical input to test correctness \n",
    "\n",
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "#                     for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_patch=[],\n",
    "    states_to_patch_1st_pass=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 12))\n",
    "                    for tnum in text_tok_indices],\n",
    "    states_to_unpatch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 23))\n",
    "                    for tnum in struct_tok_indices],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    tokens_to_mix_1st_pass=context_tok_indices,\n",
    "    replace=True,\n",
    ").item()\n",
    "\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9952, device='cuda:0')"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "#                     for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_patch=[],\n",
    "    states_to_patch_1st_pass=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 12))\n",
    "                    for tnum in text_tok_indices],\n",
    "    states_to_unpatch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 23))\n",
    "                    for tnum in struct_tok_indices],\n",
    "    answers_t=answers_t,\n",
    "    states_to_corrupt=[(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", 0, \"embed\"))\n",
    "                    for tnum in text_tok_indices],\n",
    "    states_to_corrupt_1st_pass=[(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", 0, \"embed\"))\n",
    "                    for tnum in context_tok_indices],\n",
    "    replace=True,\n",
    ").item()\n",
    "\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7253002524375916"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test corrupting attention \n",
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "#                     for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    states_to_corrupt=[(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", l, \"self_attn\"))\n",
    "                    for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers)],\n",
    "    replace=True,\n",
    ").item()\n",
    "\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n, w in mt_uskg.model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_probs = ctu.run_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "#                     for tnum in range(*struct_range)],\n",
    "    states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "                    for tnum in self_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_unpatch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "                    for tnum in self_tok_indices],\n",
    "    answer_len=len(answers_t),\n",
    "    tokens_to_mix=corrupt_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32102])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.], device='cuda:0'),\n",
       "indices=tensor([7634], device='cuda:0'))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(vocab_probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs[0, 7634]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2642e-25, 1.2223e-15, 7.3942e-18,  ..., 9.1578e-20, 2.6884e-39,\n",
       "         2.8131e-39]], device='cuda:0')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = processed_spider_dev[97]\n",
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "\n",
    "enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "dec_prompt = \"select t1.model from\"\n",
    "expect = \"car_names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    enc_sentences=[enc_sentence]*11,\n",
    "    dec_prompts=[dec_prompt]*11,\n",
    "    answer=expect\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 'aa,bb< cc  \\t dd(  )ee <= ff=5 %h% \"06-15\".'\n",
    "sep_pattern = r'\\s+|\\W'\n",
    "\n",
    "all_matches = re.finditer(sep_pattern, seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<re.Match object; span=(2, 3), match=','>,\n",
       " <re.Match object; span=(5, 6), match='<'>,\n",
       " <re.Match object; span=(6, 7), match=' '>,\n",
       " <re.Match object; span=(9, 13), match='  \\t '>,\n",
       " <re.Match object; span=(15, 16), match='('>,\n",
       " <re.Match object; span=(16, 18), match='  '>,\n",
       " <re.Match object; span=(18, 19), match=')'>,\n",
       " <re.Match object; span=(21, 22), match=' '>,\n",
       " <re.Match object; span=(22, 23), match='<'>,\n",
       " <re.Match object; span=(23, 24), match='='>,\n",
       " <re.Match object; span=(24, 25), match=' '>,\n",
       " <re.Match object; span=(27, 28), match='='>,\n",
       " <re.Match object; span=(29, 30), match=' '>,\n",
       " <re.Match object; span=(30, 31), match='%'>,\n",
       " <re.Match object; span=(32, 33), match='%'>,\n",
       " <re.Match object; span=(33, 34), match=' '>,\n",
       " <re.Match object; span=(34, 35), match='\"'>,\n",
       " <re.Match object; span=(37, 38), match='-'>,\n",
       " <re.Match object; span=(40, 41), match='\"'>,\n",
       " <re.Match object; span=(41, 42), match='.'>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_matches = list(all_matches)\n",
    "all_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_matches[0].span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 5, 6, 7, 9, 13, 15, 16, 18, 19, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 40, 41, 42]\n"
     ]
    }
   ],
   "source": [
    "splits = [0] + [i for m in all_matches for i in m.span()] + [len(seq)]\n",
    "splits = sorted(list(set(splits)))\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 0\n",
    "SP = [\"<=\", \">=\", \"<>\", \"!=\"]\n",
    "toks = []\n",
    "\n",
    "for s, e in zip(splits[:-1], splits[1:]):\n",
    "    if not seq[s:e].strip():\n",
    "        # is a whitespace\n",
    "        st = e\n",
    "    else:\n",
    "        # is a punct\n",
    "        if seq[s:s+2] in SP:\n",
    "            # wait next\n",
    "            continue\n",
    "        toks.append(seq[st:e])\n",
    "        st = e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', ',', 'bb', '<', 'cc', 'dd', '(', ')', 'ee', '<=', 'ff', '=', '5', '%', 'h', '%', '\"', '06', '-', '15', '\"', '.']\n"
     ]
    }
   ],
   "source": [
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['school', '_', 'name']"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.tokenize('school_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'cylinder']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.tokenize('cylinder ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'cylinder', '', 'x', 'a']"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.tokenize('cylinder xa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'struct', 'e', 'd', '_', 'in', 'put', '', ':', '', 'a', '|', '', 'b']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.tokenize('structed_input : a | b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'SEL',\n",
       " 'ECT',\n",
       " '',\n",
       " 't',\n",
       " '2.',\n",
       " 'a',\n",
       " 'a',\n",
       " 'a',\n",
       " '',\n",
       " ',',\n",
       " 'CO',\n",
       " 'UNT',\n",
       " '(',\n",
       " 'distin',\n",
       " 'c',\n",
       " 't',\n",
       " '',\n",
       " 't',\n",
       " '1.',\n",
       " 'name',\n",
       " ')',\n",
       " 'FROM',\n",
       " 'cars',\n",
       " '_',\n",
       " 'data',\n",
       " 'as',\n",
       " '',\n",
       " 't',\n",
       " '1',\n",
       " '',\n",
       " 'JO',\n",
       " 'IN',\n",
       " 'models',\n",
       " 'as',\n",
       " '',\n",
       " 't',\n",
       " '2',\n",
       " 'on',\n",
       " 'cars',\n",
       " '_',\n",
       " 'data',\n",
       " '.',\n",
       " 'a',\n",
       " '_',\n",
       " 'a',\n",
       " '=',\n",
       " 'models',\n",
       " '.',\n",
       " 'b',\n",
       " '_',\n",
       " 'a']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sql = 'SELECT t2.aaa , COUNT(distinct t1.name) FROM cars_data as t1 JOIN models as t2 on cars_data.a_a = models.b_a'\n",
    "\n",
    "mt_uskg.tokenizer.tokenize(_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "706px",
    "left": "30px",
    "top": "220px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

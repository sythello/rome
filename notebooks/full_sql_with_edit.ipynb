{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Tracing\n",
    "\n",
    "A demonstration of the double-intervention causal tracing method.\n",
    "\n",
    "The strategy used by causal tracing is to understand important\n",
    "states within a transfomer by doing two interventions simultaneously:\n",
    "\n",
    "1. Corrupt a subset of the input.  In our paper, we corrupt the subject tokens\n",
    "   to frustrate the ability of the transformer to accurately complete factual\n",
    "   prompts about the subject.\n",
    "2. Restore a subset of the internal hidden states.  In our paper, we scan\n",
    "   hidden states at all layers and all tokens, searching for individual states\n",
    "   that carry the necessary information for the transformer to recover its\n",
    "   capability to complete the factual prompt.\n",
    "\n",
    "The traces of decisive states can be shown on a heatmap.  This notebook\n",
    "demonstrates the code for conducting causal traces and creating these heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `experiments.causal_trace` module contains a set of functions for running causal traces.\n",
    "\n",
    "In this notebook, we reproduce, demonstrate and discuss the interesting functions.\n",
    "\n",
    "We begin by importing several utility functions that deal with tokens and transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, json\n",
    "import string\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from collections import defaultdict, Counter\n",
    "from util import nethook\n",
    "from util.globals import DATA_DIR\n",
    "from experiments.causal_trace import (\n",
    "    ModelAndTokenizer,\n",
    "    layername,\n",
    "    guess_subject,\n",
    "    plot_trace_heatmap,\n",
    ")\n",
    "from experiments.causal_trace import (\n",
    "    make_inputs,\n",
    "    decode_tokens,\n",
    "    find_token_range,\n",
    "    predict_token,\n",
    "    predict_from_input,\n",
    "    collect_embedding_std,\n",
    ")\n",
    "from dsets import KnownsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f4d4c49bb80>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USKG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "# from uskg.models.unified.prefixtuning import Model\n",
    "from uskg.models.unified import finetune, prefixtuning\n",
    "from uskg.utils.configue import Configure\n",
    "from uskg.utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "from uskg.seq2seq_construction import spider as s2s_spider\n",
    "from uskg.third_party.spider.preprocess.get_tables import dump_db_json_schema\n",
    "from uskg.third_party.spider import evaluation as sp_eval\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# import stanza\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from experiments import causal_trace_uskg as ctu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tokenizer_uskg: hkunlp/from_all_T5_large_prefix_spider_with_cell_value2\n",
      "Using tokenizer_fast: t5-large\n",
      "prefix-tuning sequence length is 10.\n"
     ]
    }
   ],
   "source": [
    "mt_uskg = ctu.ModelAndTokenizer_USKG('t5-large-prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('constructor', 'seq2seq_construction.spider'),\n",
       " ('schema_serialization_with_db_content', True),\n",
       " ('target_with_db_id', False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mt_uskg.task_args.seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.model.pretrain_model.encoder.embed_tokens is mt_uskg.model.pretrain_model.shared, \\\n",
    "mt_uskg.model.pretrain_model.decoder.embed_tokens is mt_uskg.model.pretrain_model.shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.model.preseqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [k for k,v in mt_uskg.model.named_parameters()]\n",
    "# [k for k,v in mt_uskg.model.named_modules()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load spider dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spider_train_path = '/home/yshao/Projects/SDR-analysis/data/spider/train+ratsql_graph.json'\n",
    "spider_dev_path = '/home/yshao/Projects/SDR-analysis/data/spider/dev+ratsql_graph.json'\n",
    "spider_db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_spider_dev = ctu.load_raw_dataset(\n",
    "    data_filepath = spider_dev_path,\n",
    "    db_path=spider_db_dir,\n",
    "#     schema_cache=SCHEMA_CACHE\n",
    ")\n",
    "len(raw_spider_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_spider_dev[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.task_args.dataset.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_spider_dev = s2s_spider.DevDataset(\n",
    "    args=mt_uskg.task_args,\n",
    "    raw_datasets=raw_spider_dev,\n",
    "    cache_root='../cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What are the names of all European countries with at least 3 manufacturers?',\n",
       " '| car_1 | continents : contid , continent ( europe ) | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " \"select t1.countryname from countries as t1 join continents as t2 on t1.continent = t2.contid join car_makers as t3 on t1.countryid = t3.country where t2.continent = 'europe' group by t1.countryname having count(*) >= 3;\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_id = 130\n",
    "processed_spider_dev[_id]['text_in'], \\\n",
    "processed_spider_dev[_id]['struct_in'], \\\n",
    "processed_spider_dev[_id]['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_enc_sentence = f\"{processed_spider_dev[_id]['text_in']}; structed knowledge: {processed_spider_dev[_id]['struct_in']}\"\n",
    "_toks = mt_uskg.tokenizer.tokenize(_enc_sentence)\n",
    "len(_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # _occ_punct = set()\n",
    "\n",
    "# for _id in range(len(processed_spider_dev)):\n",
    "#     ex = processed_spider_dev[_id]\n",
    "# #     _occ_punct.update(set(string.punctuation) & set(ex['seq_out']))\n",
    "#     if '_(' in ex['struct_in']:\n",
    "#         print(_id, ex['question'])\n",
    "#         print(ex['struct_in'])\n",
    "#         print(ex['seq_out'])\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Train set\n",
    "\n",
    "# raw_spider_train = ctu.load_raw_dataset(\n",
    "#     data_filepath = spider_train_path,\n",
    "#     db_path=spider_db_dir,\n",
    "# )\n",
    "# processed_spider_train = s2s_spider.TrainDataset(\n",
    "#     args=mt_uskg.task_args,\n",
    "#     raw_datasets=raw_spider_train,\n",
    "#     cache_root='../cache')\n",
    "# len(processed_spider_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_spider_train[5441]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "- merged in create_analysis_sample_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = '/home/yshao/Projects/language/language/xsp/data/spider/tables.json'\n",
    "db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmaps = sp_eval.build_foreign_key_map_from_json(table_path)\n",
    "evaluator = sp_eval.Evaluator(db_dir=db_dir, kmaps=kmaps, etype='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctu.evaluate_hardness.evaluator = evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0, 0, 'hard')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "_sql_str = 'select t1.birth_date from people as t1 join poker_player as t2 on t1.people_id = t2.people_id order by t2.earnings asc limit 1'\n",
    "db_name = 'poker_player'\n",
    "schema = evaluator.schemas[db_name]\n",
    "_sql = sp_eval.get_sql(schema, _sql_str)\n",
    "sp_eval.count_component1(_sql), sp_eval.count_component2(_sql), sp_eval.count_others(_sql), \\\n",
    "evaluator.eval_hardness(_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USKG error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from play_pred()\n",
    "\n",
    "def pred_sql(mt, ex, padding=\"max_length\"):\n",
    "    text_in = ex['text_in']\n",
    "    struct_in = ex['struct_in']\n",
    "\n",
    "    txt = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "    \n",
    "    tokenized_txt = mt.tokenizer_uskg([txt], max_length=1024, padding=padding, truncation=True)\n",
    "    \n",
    "    device = mt.model.device\n",
    "    pred = mt.tokenizer_uskg.batch_decode(\n",
    "      mt.model.generate(\n",
    "        torch.tensor(tokenized_txt.data['input_ids'], dtype=int, device=device),\n",
    "        torch.tensor(tokenized_txt.data['attention_mask'], dtype=int, device=device),\n",
    "        num_beams=1, \n",
    "        max_length=256\n",
    "        ), \n",
    "      skip_special_tokens=True \n",
    "    )\n",
    "    return pred[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from evaluator.evaluate_one()\n",
    "\n",
    "def evaluate_sql(evaluator, db_name, gold, predicted):\n",
    "    schema = evaluator.schemas[db_name]\n",
    "    g_sql = sp_eval.get_sql(schema, gold)\n",
    "    hardness = evaluator.eval_hardness(g_sql)\n",
    "    # self.scores[hardness][\"count\"] += 1\n",
    "    # self.scores[\"all\"][\"count\"] += 1\n",
    "\n",
    "    parse_error = False\n",
    "    try:\n",
    "        p_sql = sp_eval.get_sql(schema, predicted)\n",
    "    except:\n",
    "        # If p_sql is not valid, then we will use an empty sql to evaluate with the correct sql\n",
    "        p_sql = {\n",
    "            \"except\": None,\n",
    "            \"from\": {\"conds\": [], \"table_units\": []},\n",
    "            \"groupBy\": [],\n",
    "            \"having\": [],\n",
    "            \"intersect\": None,\n",
    "            \"limit\": None,\n",
    "            \"orderBy\": [],\n",
    "            \"select\": [False, []],\n",
    "            \"union\": None,\n",
    "            \"where\": [],\n",
    "        }\n",
    "\n",
    "        # TODO fix\n",
    "        parse_error = True\n",
    "\n",
    "    # rebuild sql for value evaluation\n",
    "    kmap = evaluator.kmaps[db_name]\n",
    "    g_valid_col_units = sp_eval.build_valid_col_units(g_sql[\"from\"][\"table_units\"], schema)\n",
    "    g_sql = sp_eval.rebuild_sql_val(g_sql)\n",
    "    g_sql = sp_eval.rebuild_sql_col(g_valid_col_units, g_sql, kmap)\n",
    "    p_valid_col_units = sp_eval.build_valid_col_units(p_sql[\"from\"][\"table_units\"], schema)\n",
    "    p_sql = sp_eval.rebuild_sql_val(p_sql)\n",
    "    p_sql = sp_eval.rebuild_sql_col(p_valid_col_units, p_sql, kmap)\n",
    "    \n",
    "    exec_score = None\n",
    "    partial_scores = None\n",
    "    exact_score = None\n",
    "    if evaluator.etype in [\"all\", \"exec\"]:\n",
    "        try:\n",
    "            exec_score = sp_eval.eval_exec_match(\n",
    "                evaluator.db_paths[db_name], predicted, gold, p_sql, g_sql\n",
    "            )\n",
    "            exec_score = int(exec_score)\n",
    "        except:\n",
    "            exec_score = 0\n",
    "    if evaluator.etype in [\"all\", \"match\"]:\n",
    "        partial_scores = evaluator.eval_partial_match(p_sql, g_sql)\n",
    "        exact_score = evaluator.eval_exact_match(p_sql, g_sql, partial_scores)\n",
    "        # update_scores_match(self.scores, exact_score, hardness, partial_scores, PARTIAL_TYPES)\n",
    "\n",
    "    return {\n",
    "        \"predicted\": predicted,\n",
    "        \"gold\": gold,\n",
    "        \"predicted_parse_error\": parse_error,\n",
    "        \"hardness\": hardness,\n",
    "        \"exact\": exact_score,\n",
    "        \"partial\": partial_scores,\n",
    "        \"exec\": exec_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dir = '/home/yshao/Projects/language/language/xsp/data/spider/database'\n",
    "\n",
    "def execute_sql(db, sql_str):\n",
    "    db_path = os.path.join(db_dir, db, f'{db}.sqlite')\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(sql_str)\n",
    "        res = cursor.fetchall()\n",
    "    except:\n",
    "        res = 'ERROR'\n",
    "    conn.close()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph', 'serialized_schema', 'struct_in', 'text_in', 'seq_out'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = processed_spider_dev[123]\n",
    "ex.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('car_1',\n",
       " 'What are the countries having at least one car maker? List name and id.',\n",
       " 'select t1.countryname, t1.countryid from countries as t1 join car_makers as t2 on t1.countryid = t2.country group by t1.countryid having count(*) >= 1;',\n",
       " 'select t1.countryname, t1.id from countries as t1 join car_makers as t2 on t1.countryid = t2.country group by t1.countryname having count(*) >= 1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred_sql(mt_uskg, ex)\n",
    "ex['db_id'], ex['text_in'], ex['seq_out'], pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('car_1',\n",
       " 'What are the countries having at least one car maker? List name and id.',\n",
       " 'select t1.countryname, t1.countryid from countries as t1 join car_makers as t2 on t1.countryid = t2.country group by t1.countryid having count(*) >= 1;',\n",
       " 'select t1.countryname, t1.id from countries as t1 join car_makers as t2 on t1.countryid = t2.country group by t1.countryname having count(*) >= 1')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "ex['db_id'], ex['text_in'], ex['seq_out'], pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in = ex['text_in']\n",
    "struct_in = ex['struct_in']\n",
    "txt = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "txt_toks = mt_uskg.tokenizer_uskg.tokenize(txt)\n",
    "len(txt_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage = '225' );\",\n",
       " [(7,)])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_res = execute_sql(ex['db_id'], ex['seq_out'])\n",
    "ex['seq_out'], exec_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('select count(*) from battle where id not in ( select lost_in_battle from ship where tonnage > 225 )',\n",
       " [(3,)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exec_res = execute_sql(ex['db_id'], pred)\n",
    "pred, exec_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8,)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatgpt_pred = \"\"\"\n",
    "SELECT COUNT(DISTINCT battle.id) AS num_battles\n",
    "FROM battle\n",
    "LEFT JOIN ship ON battle.id = ship.lost_in_battle\n",
    "WHERE (ship.tonnage != '225' OR ship.tonnage IS NULL)\n",
    "\"\"\"\n",
    "\n",
    "execute_sql(ex['db_id'], chatgpt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_res = evaluate_sql(evaluator, db_name=spider_ex['db_id'], gold=spider_ex['seq_out'], predicted=pred)\n",
    "# eval_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a765918a08614fbc982989ab115dc3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_preds = []\n",
    "eval_sql_results = []\n",
    "\n",
    "for ex_id, ex in enumerate(tqdm(processed_spider_dev)):\n",
    "    pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "    eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "    \n",
    "    all_preds.append(pred)\n",
    "    eval_sql_results.append(eval_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6692456479690522, 0.6808510638297872)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no padding; identical!\n",
    "avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "avg_exact, avg_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = '/home/yshao/Projects/rome/results/clean_predictions'\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "\n",
    "pred_path = os.path.join(res_dir, 'predictions.txt')\n",
    "eval_path = os.path.join(res_dir, 'evals.jsonl')\n",
    "\n",
    "with open(pred_path, 'w') as f:\n",
    "    for p in all_preds:\n",
    "        f.write(p + '\\n')\n",
    "\n",
    "with open(eval_path, 'w') as f:\n",
    "    for e in eval_sql_results:\n",
    "        f.write(json.dumps(e, indent=None) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6692456479690522, 0.6808510638297872)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding to 1024 \n",
    "avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "avg_exact, avg_exec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual error cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(eval_sql_results):\n",
    "    if d['exact'] and d['exec']:\n",
    "        continue\n",
    "    err_msg = ('A' if not d['exact'] else '') + ('X' if not d['exec'] else '')\n",
    "    ex = processed_spider_dev[i]\n",
    "    print(f'ID = {i}: {err_msg}  ({ex[\"db_id\"]}) {ex[\"text_in\"]}')\n",
    "    print(f'Pred: {d[\"predicted\"]}')\n",
    "    print(f'Gold: {d[\"gold\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator.schemas['dog_kennels'].schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acc by db_id \n",
    "eval_sql_results_by_db_id = defaultdict(list)\n",
    "\n",
    "for i, d in enumerate(eval_sql_results):\n",
    "    d['ex_id'] = i\n",
    "    ex = processed_spider_dev[i]\n",
    "    db_id = ex['db_id']\n",
    "    eval_sql_results_by_db_id[db_id].append(d)\n",
    "\n",
    "len(eval_sql_results_by_db_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concert_singer\t0.8889\t0.8889\n",
      "pets_1\t0.5714\t0.7381\n",
      "car_1\t0.3478\t0.3913\n",
      "flight_2\t0.7000\t0.7500\n",
      "employee_hire_evaluation\t0.9474\t0.9737\n",
      "cre_Doc_Template_Mgt\t0.8333\t0.9048\n",
      "course_teach\t0.8667\t0.9333\n",
      "museum_visit\t0.7222\t0.8333\n",
      "wta_1\t0.6774\t0.6129\n",
      "battle_death\t0.5000\t0.5000\n",
      "student_transcripts_tracking\t0.6667\t0.6795\n",
      "tvshow\t0.7258\t0.6613\n",
      "poker_player\t0.8750\t0.8750\n",
      "voter_1\t0.6000\t0.6667\n",
      "world_1\t0.5083\t0.4833\n",
      "orchestra\t0.8000\t0.8750\n",
      "network_1\t0.6250\t0.4643\n",
      "dog_kennels\t0.5854\t0.5976\n",
      "singer\t0.8667\t0.8667\n",
      "real_estate_properties\t0.5000\t0.5000\n"
     ]
    }
   ],
   "source": [
    "for db_id, results in eval_sql_results_by_db_id.items():\n",
    "    _avg_exact = np.mean([d['exact'] for d in results])\n",
    "    _avg_exec = np.mean([d['exec'] for d in results])\n",
    "    print(f'{db_id}\\t{_avg_exact:.4f}\\t{_avg_exec:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full SQL prediction with edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder attention (exp A1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enc_self_attention_mask_for_section_pair(\n",
    "    ex,\n",
    "    q_sect,\n",
    "    k_sect,\n",
    "    seq_len=None,\n",
    "    mt=None,\n",
    "    prefix_len=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    For attention-related experiments: build encoder self-attention masks for a section pair\n",
    "\n",
    "    Args:\n",
    "    a_ex (Dict)\n",
    "    q_sect (str)\n",
    "    k_sect (str)\n",
    "    seq_len (int): sequence length\n",
    "    prefix_len (int): prefix length\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    enc_sentence = ex['enc_sentence']\n",
    "\n",
    "    text_range = ex['text_range']\n",
    "    struct_range = ex['struct_range']\n",
    "    text_st, text_ed = text_range\n",
    "    struct_st, struct_ed = struct_range\n",
    "    # text_tok_indices = list(range(*text_range))\n",
    "    # struct_tok_indices = list(range(*struct_range))\n",
    "    \n",
    "    token_ranges_dict = ex['struct_node_ranges_dict']\n",
    "\n",
    "    if seq_len is None:\n",
    "        # need to tokenize and decide seq_len\n",
    "        assert mt is not None\n",
    "        _tok_ids = mt.tokenizer.encode(enc_sentence, add_special_tokens=True)\n",
    "        seq_len = len(_tok_ids)\n",
    "\n",
    "    # seq_len should include EOS, while struct shouldn't \n",
    "    assert seq_len == struct_ed + 1, (seq_len, struct_ed)\n",
    "    \n",
    "    ## Config for each section\n",
    "    ## 'is_range': whether the section is a single range \n",
    "    ## 'pos': if is_range, a tuple for the range; otherwise, a list of indices \n",
    "    ## Note: not including \"self\" and \"context\" sect for now; they don't fit into this config format \n",
    "    section_configs = {\n",
    "        'text': {\n",
    "            'is_range': True,\n",
    "            'q_pos': (text_st, text_ed),\n",
    "            'k_pos': (text_st + prefix_len, text_ed + prefix_len),\n",
    "        },\n",
    "        'struct': {\n",
    "            'is_range': True,\n",
    "            'q_pos': (struct_st, struct_ed),\n",
    "            'k_pos': (struct_st + prefix_len, struct_ed + prefix_len),\n",
    "        },\n",
    "        'prefix': {\n",
    "            'is_range': True,\n",
    "            'q_pos': None,  # prefix does not have q \n",
    "            'k_pos': (0, prefix_len),\n",
    "        },\n",
    "        'eos': {\n",
    "            'is_range': True,\n",
    "            'q_pos': (seq_len - 1, seq_len),\n",
    "            'k_pos': (seq_len + prefix_len - 1, seq_len + prefix_len),\n",
    "        },\n",
    "        'all': {\n",
    "            'is_range': True,\n",
    "            'q_pos': (0, seq_len),\n",
    "            'k_pos': (0, seq_len + prefix_len),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # mix_mask: (batch, head, src_len, tgt_len)\n",
    "    mix_mask = torch.zeros(1, 1, seq_len, seq_len + prefix_len).bool()\n",
    "    \n",
    "    if q_sect == 'self':\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        q_config = section_configs[q_sect]\n",
    "        k_config = section_configs[k_sect]\n",
    "        \n",
    "        # For now, all config is range \n",
    "        assert q_config['is_range'] and k_config['is_range']\n",
    "        \n",
    "        q_st, q_ed = q_config['q_pos']\n",
    "        k_st, k_ed = k_config['k_pos']\n",
    "        \n",
    "        mix_mask[:, :, q_st : q_ed, k_st : k_ed] = True\n",
    "    \n",
    "    return mix_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def add_enc_attention_edit(mt, ex, mix_layers, section_pairs, attn_corrupt_type='weights'):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        mix_layers: List[int], the layers to edit\n",
    "        section_pairs: List[Tuple(str, str)]: corrupting attention from which to which\n",
    "            sections: \"text\", \"struct\", \"prefix\", \"eos\"; \"self\", \"context\"\n",
    "            (notice that \"self\" is per-node; \"context->context\" is invalid here)\n",
    "        attn_corrupt_type: (same as trace exp) weights / logits \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Code to be executed when entering the context\n",
    "    \n",
    "    ex = copy.deepcopy(ex)\n",
    "    \n",
    "    text_in = ex['text_in']\n",
    "    struct_in = ex['struct_in']\n",
    "\n",
    "    enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "    enc_tokenized = mt.tokenizer(enc_sentence)\n",
    "    ex['enc_sentence'] = enc_sentence\n",
    "    ex['enc_tokenized'] = enc_tokenized\n",
    "    \n",
    "    parsed_struct_in = ctu.parse_struct_in(struct_in)\n",
    "    \n",
    "    token_ranges_dict = ctu.find_struct_name_ranges(mt.tokenizer, ex)\n",
    "    text_range = ex['text_range']\n",
    "    struct_range = ex['struct_range']\n",
    "    # token_ranges_dict = ex['struct_node_ranges_dict']\n",
    "\n",
    "    all_mix_masks = [build_enc_self_attention_mask_for_section_pair(\n",
    "            ex,\n",
    "            q_sect,\n",
    "            k_sect,\n",
    "            mt=mt) for q_sect, k_sect in section_pairs]\n",
    "    \n",
    "    mix_mask = torch.logical_or(*all_mix_masks) if len(all_mix_masks) > 1 else all_mix_masks[0]\n",
    "    \n",
    "    def _attn_w_fn(attn):\n",
    "        _mix_mask = mix_mask.to(device=attn.device)\n",
    "        _zero = torch.tensor(0, dtype=attn.dtype, device=attn.device)\n",
    "        attn = torch.where(_mix_mask, _zero, attn)     # no need to keep batch_idx=0 clean here\n",
    "        return attn\n",
    "\n",
    "    def _attn_lg_fn(attn):\n",
    "        _mix_mask = mix_mask.to(device=attn.device)\n",
    "        _neg = torch.tensor(-1e9, dtype=attn.dtype, device=attn.device)\n",
    "        attn = torch.where(_mix_mask, _neg, attn)\n",
    "        return attn\n",
    "\n",
    "#     def p_hook_fn(m, inp):\n",
    "#         if attn_corrupt_type == 'weights':\n",
    "#             m.ext_attention_weights_fn = _attn_w_fn\n",
    "#         elif attn_corrupt_type == 'logits':\n",
    "#             m.ext_attention_logits_fn = _attn_lg_fn\n",
    "#         else:\n",
    "#             raise ValueError(attn_corrupt_type)\n",
    "\n",
    "#     def f_hook_fn(m, inp, outp):\n",
    "#         m.ext_attention_weights_fn = None\n",
    "#         m.ext_attention_logits_fn = None\n",
    "\n",
    "#     all_hooks = []\n",
    "\n",
    "    edit_module_list = [nethook.get_module(mt.model, layer) for layer in mix_layers]\n",
    "    for m in edit_module_list:\n",
    "        if attn_corrupt_type == 'weights':\n",
    "            m.ext_attention_weights_fn = _attn_w_fn\n",
    "        elif attn_corrupt_type == 'logits':\n",
    "            m.ext_attention_logits_fn = _attn_lg_fn\n",
    "\n",
    "    try:\n",
    "        ## You can yield any object that will be bound to the variable after 'as' in 'with' statement\n",
    "        yield\n",
    "        \n",
    "    finally:\n",
    "        ## Code to be executed when exiting the context\n",
    "        for m in edit_module_list:\n",
    "            m.ext_attention_weights_fn = None\n",
    "            m.ext_attention_logits_fn = None\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_0_result_dir = '/home/yshao/Projects/rome/results/exp_A1_0_enc_attention_removal_eval'\n",
    "os.makedirs(a1_0_result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running for all configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Current exp: A1.0.1 (attn_corrupt_type=logits)\n",
    "\n",
    "# layer_k -> layer range \n",
    "layer_configs = {\n",
    "#     'low': range(12),\n",
    "    'mid': range(6, 18),\n",
    "    'high': range(12, 24),\n",
    "#     'all': range(24),\n",
    "}\n",
    "\n",
    "# sect_k -> section pairs\n",
    "sect_configs = {\n",
    "    's->t': [('struct', 'text')],\n",
    "    't->s': [('text', 'struct')],\n",
    "    't<->s': [('struct', 'text'), ('text', 'struct')],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06e7d344c274dce856d3295debf9599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = mid, Sect = s->t : Exact = 0.5019, Exec = 0.5184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ef2a5328d14bf2b09a742f100a79a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = mid, Sect = t->s : Exact = 0.6451, Exec = 0.6586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22cc3c194f64c719b648030b0a2fc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = mid, Sect = t<->s : Exact = 0.4294, Exec = 0.4468\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648bb186aada4fd395ac55e383dd832f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = high, Sect = s->t : Exact = 0.4072, Exec = 0.4362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e620e30e5147909770407bf1367436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = high, Sect = t->s : Exact = 0.6151, Exec = 0.6412\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86941ea5f20d479daf69a7107915fa2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = high, Sect = t<->s : Exact = 0.3250, Exec = 0.3559\n"
     ]
    }
   ],
   "source": [
    "for layer_k, layer_range in layer_configs.items():\n",
    "    for sect_k, section_pairs in sect_configs.items():\n",
    "        mix_layers = [ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in layer_range]\n",
    "\n",
    "        all_preds = []\n",
    "        eval_sql_results = []\n",
    "\n",
    "        for ex_id, ex in enumerate(tqdm(processed_spider_dev)):\n",
    "            with add_enc_attention_edit(mt=mt_uskg,\n",
    "                                    ex=ex,\n",
    "                                    mix_layers=mix_layers,\n",
    "                                    section_pairs=section_pairs,\n",
    "                                    attn_corrupt_type='logits'):\n",
    "                pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "\n",
    "            eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "\n",
    "            all_preds.append(pred)\n",
    "            eval_sql_results.append(eval_res)\n",
    "        \n",
    "        avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "        avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "        print(f'Layer = {layer_k}, Sect = {sect_k} : Exact = {avg_exact:.4f}, Exec = {avg_exec:.4f}')\n",
    "        \n",
    "        pred_path = os.path.join(a1_0_result_dir, f'exp=A1.0.1_dev_predictions-layer={layer_k}-sect={sect_k}.txt')\n",
    "        eval_path = os.path.join(a1_0_result_dir, f'exp=A1.0.1_dev_evals-layer={layer_k}-sect={sect_k}.jsonl')\n",
    "\n",
    "        with open(pred_path, 'w') as f:\n",
    "            for p in all_preds:\n",
    "                f.write(p + '\\n')\n",
    "\n",
    "        with open(eval_path, 'w') as f:\n",
    "            for e in eval_sql_results:\n",
    "                f.write(json.dumps(e, indent=None) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder cross attention (exp A1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dec_cross_attention_mask_for_section(\n",
    "    ex,\n",
    "    k_sect,\n",
    "    seq_len=None,\n",
    "    mt=None,\n",
    "    prefix_len=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    For attention-related experiments: build decoder cross-attention mask for an input section, block all-step X-att to this section \n",
    "    The q dimension is set to 1; in edit fn, it will be broadcasted to the correct size\n",
    "\n",
    "    Args:\n",
    "    a_ex (Dict)\n",
    "    k_sect (str)\n",
    "    seq_len (int): sequence length\n",
    "    prefix_len (int): prefix length\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    enc_sentence = ex['enc_sentence']\n",
    "\n",
    "    text_range = ex['text_range']\n",
    "    struct_range = ex['struct_range']\n",
    "    text_st, text_ed = text_range\n",
    "    struct_st, struct_ed = struct_range\n",
    "    # text_tok_indices = list(range(*text_range))\n",
    "    # struct_tok_indices = list(range(*struct_range))\n",
    "    \n",
    "    token_ranges_dict = ex['struct_node_ranges_dict']\n",
    "\n",
    "    if seq_len is None:\n",
    "        # need to tokenize and decide seq_len\n",
    "        assert mt is not None\n",
    "        _tok_ids = mt.tokenizer.encode(enc_sentence, add_special_tokens=True)\n",
    "        seq_len = len(_tok_ids)\n",
    "\n",
    "    # seq_len should include EOS, while struct shouldn't \n",
    "    assert seq_len == struct_ed + 1, (seq_len, struct_ed)\n",
    "    \n",
    "    ## Config for each section\n",
    "    ## 'is_range': whether the section is a single range \n",
    "    ## 'pos': if is_range, a tuple for the range; otherwise, a list of indices \n",
    "    ## Note: not including \"self\" and \"context\" sect for now; they don't fit into this config format \n",
    "    section_configs = {\n",
    "        'text': {\n",
    "            'is_range': True,\n",
    "            'k_pos': (text_st + prefix_len, text_ed + prefix_len),\n",
    "        },\n",
    "        'struct': {\n",
    "            'is_range': True,\n",
    "            'k_pos': (struct_st + prefix_len, struct_ed + prefix_len),\n",
    "        },\n",
    "        'prefix': {\n",
    "            'is_range': True,\n",
    "            'k_pos': (0, prefix_len),\n",
    "        },\n",
    "        'others': {\n",
    "            'is_range': False,\n",
    "            'k_pos': list(range(text_ed + prefix_len, struct_st + prefix_len)) + list(range(seq_len + prefix_len - 1, seq_len + prefix_len)),\n",
    "        },\n",
    "        'all': {\n",
    "            'is_range': True,\n",
    "            'k_pos': (0, seq_len + prefix_len),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # mix_mask: (batch, head, src_len, tgt_len)\n",
    "    mix_mask = torch.zeros(1, 1, 1, seq_len + prefix_len).bool()\n",
    "    \n",
    "    k_config = section_configs[k_sect]\n",
    "\n",
    "    if k_config['is_range']:\n",
    "        k_st, k_ed = k_config['k_pos']\n",
    "        mix_mask[:, :, :, k_st : k_ed] = True\n",
    "    else:\n",
    "        k_toks = k_config['k_pos']\n",
    "        mix_mask[:, :, :, k_toks] = True\n",
    "    \n",
    "    return mix_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def add_dec_cross_attention_edit(mt, ex, mix_layers, sections, attn_corrupt_type='weights'):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "        mix_layers: List[int], the layers to edit\n",
    "        section: List[str]: corrupting attention from decoder to which sections\n",
    "            sections: \"text\", \"struct\", \"prefix\", \"others\"; \"self\", \"context\" (not implemented)\n",
    "            (notice that \"self\" is per-node; \"context->context\" is invalid here)\n",
    "        attn_corrupt_type: (same as trace exp) weights / logits \n",
    "    \"\"\"\n",
    "    \n",
    "    ## Code to be executed when entering the context\n",
    "    \n",
    "    ex = copy.deepcopy(ex)\n",
    "    \n",
    "    text_in = ex['text_in']\n",
    "    struct_in = ex['struct_in']\n",
    "\n",
    "    enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "    enc_tokenized = mt.tokenizer(enc_sentence)\n",
    "    ex['enc_sentence'] = enc_sentence\n",
    "    ex['enc_tokenized'] = enc_tokenized\n",
    "    \n",
    "    parsed_struct_in = ctu.parse_struct_in(struct_in)\n",
    "    \n",
    "    token_ranges_dict = ctu.find_struct_name_ranges(mt.tokenizer, ex)\n",
    "    text_range = ex['text_range']\n",
    "    struct_range = ex['struct_range']\n",
    "    # token_ranges_dict = ex['struct_node_ranges_dict']\n",
    "\n",
    "    all_mix_masks = [build_dec_cross_attention_mask_for_section(\n",
    "            ex,\n",
    "            k_sect,\n",
    "            mt=mt) for k_sect in sections]\n",
    "    \n",
    "    mix_mask = torch.logical_or(*all_mix_masks) if len(all_mix_masks) > 1 else all_mix_masks[0]\n",
    "    # print(mix_mask)\n",
    "    \n",
    "    def _attn_w_fn(attn):\n",
    "        _mix_mask = mix_mask.to(device=attn.device)\n",
    "        _zero = torch.tensor(0, dtype=attn.dtype, device=attn.device)\n",
    "        attn = torch.where(_mix_mask, _zero, attn)     # no need to keep batch_idx=0 clean here\n",
    "        return attn\n",
    "\n",
    "    def _attn_lg_fn(attn):\n",
    "        _mix_mask = mix_mask.to(device=attn.device)\n",
    "        _neg = torch.tensor(-1e9, dtype=attn.dtype, device=attn.device)\n",
    "        attn = torch.where(_mix_mask, _neg, attn)\n",
    "        return attn\n",
    "\n",
    "#     def p_hook_fn(m, inp):\n",
    "#         if attn_corrupt_type == 'weights':\n",
    "#             m.ext_attention_weights_fn = _attn_w_fn\n",
    "#         elif attn_corrupt_type == 'logits':\n",
    "#             m.ext_attention_logits_fn = _attn_lg_fn\n",
    "#         else:\n",
    "#             raise ValueError(attn_corrupt_type)\n",
    "\n",
    "#     def f_hook_fn(m, inp, outp):\n",
    "#         m.ext_attention_weights_fn = None\n",
    "#         m.ext_attention_logits_fn = None\n",
    "\n",
    "#     all_hooks = []\n",
    "\n",
    "    edit_module_list = [nethook.get_module(mt.model, layer) for layer in mix_layers]\n",
    "    for m in edit_module_list:\n",
    "        if attn_corrupt_type == 'weights':\n",
    "            m.ext_attention_weights_fn = _attn_w_fn\n",
    "        elif attn_corrupt_type == 'logits':\n",
    "            m.ext_attention_logits_fn = _attn_lg_fn\n",
    "\n",
    "    try:\n",
    "        ## You can yield any object that will be bound to the variable after 'as' in 'with' statement\n",
    "        yield\n",
    "        \n",
    "    finally:\n",
    "        ## Code to be executed when exiting the context\n",
    "        for m in edit_module_list:\n",
    "            m.ext_attention_weights_fn = None\n",
    "            m.ext_attention_logits_fn = None\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_1_result_dir = '/home/yshao/Projects/rome/results/exp_A1_1_dec_cross_attention_removal_eval'\n",
    "os.makedirs(a1_1_result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running for all configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Current exp: A1.1.0 (attn_corrupt_type=weights); A1.1.1 (attn_corrupt_type=logits);\n",
    "\n",
    "# layer_k -> layer range \n",
    "layer_configs = {\n",
    "    'low': range(12),\n",
    "    'mid': range(6, 18),\n",
    "    'high': range(12, 24),\n",
    "    'all': range(24),\n",
    "}\n",
    "\n",
    "# sect_k -> section pairs\n",
    "sect_configs = {\n",
    "    'text': ['text'],\n",
    "    'struct': ['struct'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89feb73a54c4e07a69e21ad5d4e3c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = all, Sect = text : Exact = 0.0996, Exec = 0.1015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53d05757a73416dac734dcd7da012c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = all, Sect = struct : Exact = 0.0010, Exec = 0.0010\n"
     ]
    }
   ],
   "source": [
    "for layer_k, layer_range in layer_configs.items():\n",
    "    for sect_k, sections in sect_configs.items():\n",
    "        mix_layers = [ctu.layername_uskg(mt_uskg.model, 'decoder', l, 'cross_attn') for l in layer_range]\n",
    "\n",
    "        all_preds = []\n",
    "        eval_sql_results = []\n",
    "\n",
    "        for ex_id, ex in enumerate(tqdm(processed_spider_dev)):\n",
    "            with add_dec_cross_attention_edit(mt=mt_uskg,\n",
    "                                            ex=ex,\n",
    "                                            mix_layers=mix_layers,\n",
    "                                            sections=sections,\n",
    "                                            attn_corrupt_type='weights'):\n",
    "                pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "\n",
    "            eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "\n",
    "            all_preds.append(pred)\n",
    "            eval_sql_results.append(eval_res)\n",
    "        \n",
    "        avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "        avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "        print(f'Layer = {layer_k}, Sect = {sect_k} : Exact = {avg_exact:.4f}, Exec = {avg_exec:.4f}')\n",
    "        \n",
    "        pred_path = os.path.join(a1_1_result_dir, f'exp=A1.1.0_dev_predictions-layer={layer_k}-sect={sect_k}.txt')\n",
    "        eval_path = os.path.join(a1_1_result_dir, f'exp=A1.1.0_dev_evals-layer={layer_k}-sect={sect_k}.jsonl')\n",
    "\n",
    "        with open(pred_path, 'w') as f:\n",
    "            for p in all_preds:\n",
    "                f.write(p + '\\n')\n",
    "\n",
    "        with open(eval_path, 'w') as f:\n",
    "            for e in eval_sql_results:\n",
    "                f.write(json.dumps(e, indent=None) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76dd594921049c8b631408c9be40bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = low, Sect = text : Exact = 0.4400, Exec = 0.4700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74705ecef3e84208a668c740401677d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = low, Sect = struct : Exact = 0.2592, Exec = 0.2679\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa601990f4a448a847010e9766987f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = mid, Sect = text : Exact = 0.1818, Exec = 0.2002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbfec44615144dcabc6873de5c9d6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = mid, Sect = struct : Exact = 0.2147, Exec = 0.2128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef16df11e53b4b35b2ec04db73b489dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = high, Sect = text : Exact = 0.3994, Exec = 0.3627\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a1b06d206e416ba7b54edd09f4a1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = high, Sect = struct : Exact = 0.1015, Exec = 0.1044\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ac3211848d40419a157ce71041f36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = all, Sect = text : Exact = 0.0706, Exec = 0.0812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00a29fe1ccf4f169326e40b971c54c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = all, Sect = struct : Exact = 0.0648, Exec = 0.0638\n"
     ]
    }
   ],
   "source": [
    "for layer_k, layer_range in layer_configs.items():\n",
    "    for sect_k, sections in sect_configs.items():\n",
    "        mix_layers = [ctu.layername_uskg(mt_uskg.model, 'decoder', l, 'cross_attn') for l in layer_range]\n",
    "\n",
    "        all_preds = []\n",
    "        eval_sql_results = []\n",
    "\n",
    "        for ex_id, ex in enumerate(tqdm(processed_spider_dev)):\n",
    "            with add_dec_cross_attention_edit(mt=mt_uskg,\n",
    "                                            ex=ex,\n",
    "                                            mix_layers=mix_layers,\n",
    "                                            sections=sections,\n",
    "                                            attn_corrupt_type='logits'):\n",
    "                pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "\n",
    "            eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "\n",
    "            all_preds.append(pred)\n",
    "            eval_sql_results.append(eval_res)\n",
    "        \n",
    "        avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "        avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "        print(f'Layer = {layer_k}, Sect = {sect_k} : Exact = {avg_exact:.4f}, Exec = {avg_exec:.4f}')\n",
    "        \n",
    "        pred_path = os.path.join(a1_1_result_dir, f'exp=A1.1.1_dev_predictions-layer={layer_k}-sect={sect_k}.txt')\n",
    "        eval_path = os.path.join(a1_1_result_dir, f'exp=A1.1.1_dev_evals-layer={layer_k}-sect={sect_k}.jsonl')\n",
    "\n",
    "        with open(pred_path, 'w') as f:\n",
    "            for p in all_preds:\n",
    "                f.write(p + '\\n')\n",
    "\n",
    "        with open(eval_path, 'w') as f:\n",
    "            for e in eval_sql_results:\n",
    "                f.write(json.dumps(e, indent=None) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_k = 'mid'\n",
    "sect_k = 'text'\n",
    "\n",
    "# eval_path = f'/home/yshao/Projects/rome/results/exp_A1_0_enc_attention_removal_eval/exp=A1.0.0_dev_evals-layer={layer_k}-sect={sect_k}.jsonl'\n",
    "eval_path = f'/home/yshao/Projects/rome/results/exp_A1_1_dec_cross_attention_removal_eval/exp=A1.1.0_dev_evals-layer={layer_k}-sect={sect_k}.jsonl'\n",
    "\n",
    "with open(eval_path, 'r') as f:\n",
    "    eval_results = [json.loads(l) for l in f]\n",
    "len(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer = mid, Sect = text : Exact = 0.2737, Exec = 0.2950\n"
     ]
    }
   ],
   "source": [
    "avg_exact = np.mean([d['exact'] for d in eval_results])\n",
    "avg_exec = np.mean([d['exec'] for d in eval_results])\n",
    "print(f'Layer = {layer_k}, Sect = {sect_k} : Exact = {avg_exact:.4f}, Exec = {avg_exec:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted': 'select count(*) from singer',\n",
       " 'gold': 'select count(*) from singer',\n",
       " 'predicted_parse_error': False,\n",
       " 'hardness': 'easy',\n",
       " 'exact': True,\n",
       " 'partial': {'select': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 1},\n",
       "  'select(no AGG)': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 1,\n",
       "   'pred_total': 1},\n",
       "  'where': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 0, 'pred_total': 0},\n",
       "  'where(no OP)': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 0,\n",
       "   'pred_total': 0},\n",
       "  'group(no Having)': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 0,\n",
       "   'pred_total': 0},\n",
       "  'group': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 0, 'pred_total': 0},\n",
       "  'order': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 0, 'pred_total': 0},\n",
       "  'and/or': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 1, 'pred_total': 1},\n",
       "  'IUEN': {'acc': 1, 'rec': 1, 'f1': 1, 'label_total': 0, 'pred_total': 0},\n",
       "  'keywords': {'acc': 1,\n",
       "   'rec': 1,\n",
       "   'f1': 1,\n",
       "   'label_total': 0,\n",
       "   'pred_total': 0}},\n",
       " 'exec': 1}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 1034)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_k = 'all'\n",
    "sect_k = 's->t'\n",
    "\n",
    "eval_path = os.path.join(f'/home/yshao/Projects/rome/results/exp_A1_0_enc_attention_removal_eval/exp=A1.0.1_dev_evals-layer={layer_k}-sect={sect_k}.jsonl')\n",
    "clean_eval_path = os.path.join(f'/home/yshao/Projects/rome/results/clean_predictions/evals.jsonl')\n",
    "\n",
    "with open(eval_path, 'r') as f:\n",
    "    eval_sql_results = [json.loads(l) for l in f]\n",
    "\n",
    "with open(clean_eval_path, 'r') as f:\n",
    "    clean_sql_results = [json.loads(l) for l in f]\n",
    "    \n",
    "len(eval_sql_results), len(clean_sql_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupted wrong, clean right \n",
    "\n",
    "_tally = 0\n",
    "\n",
    "for i, (e_d, c_d) in enumerate(zip(eval_sql_results, clean_sql_results)):\n",
    "    assert e_d['gold'] == c_d['gold'], (e_d, c_d)\n",
    "    \n",
    "    if e_d['exact'] and e_d['exec']:\n",
    "        # corrupted pred is right \n",
    "        continue\n",
    "    if not (c_d['exact'] and c_d['exec']):\n",
    "        # clean pred is also wrong \n",
    "        continue\n",
    "        \n",
    "    err_msg = ('A' if not e_d['exact'] else '') + ('X' if not e_d['exec'] else '')\n",
    "    ex = processed_spider_dev[i]\n",
    "    _tally += 1\n",
    "    print(f'#{_tally} (ID = {i}): {err_msg}  ({ex[\"db_id\"]}) {ex[\"text_in\"]} {ex[\"struct_in\"]}')\n",
    "    print(f'E-Pred: {e_d[\"predicted\"]}')\n",
    "    print(f'C-Pred: {c_d[\"predicted\"]}')\n",
    "    print(f'Gold: {e_d[\"gold\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupted right, clean wrong \n",
    "\n",
    "_tally = 0\n",
    "\n",
    "for i, (e_d, c_d) in enumerate(zip(eval_sql_results, clean_sql_results)):\n",
    "    assert e_d['gold'] == c_d['gold'], (e_d, c_d)\n",
    "    \n",
    "    if c_d['exact'] and c_d['exec']:\n",
    "        # clean pred is right \n",
    "        continue\n",
    "    if not (e_d['exact'] and e_d['exec']):\n",
    "        # corrupted pred is wrong \n",
    "        continue\n",
    "        \n",
    "    err_msg = ('A' if not c_d['exact'] else '') + ('X' if not c_d['exec'] else '')\n",
    "    ex = processed_spider_dev[i]\n",
    "    _tally += 1\n",
    "    print(f'#{_tally} (ID = {i}): {err_msg}  ({ex[\"db_id\"]}) {ex[\"text_in\"]} {ex[\"struct_in\"]}')\n",
    "    print(f'E-Pred: {e_d[\"predicted\"]}')\n",
    "    print(f'C-Pred: {c_d[\"predicted\"]}')\n",
    "    print(f'Gold: {e_d[\"gold\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator.schemas['dog_kennels'].schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acc by db_id \n",
    "eval_sql_results_by_db_id = defaultdict(list)\n",
    "\n",
    "for i, d in enumerate(eval_sql_results):\n",
    "    d['ex_id'] = i\n",
    "    ex = processed_spider_dev[i]\n",
    "    db_id = ex['db_id']\n",
    "    eval_sql_results_by_db_id[db_id].append(d)\n",
    "\n",
    "len(eval_sql_results_by_db_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concert_singer\t0.8889\t0.8889\n",
      "pets_1\t0.5714\t0.7381\n",
      "car_1\t0.3478\t0.3913\n",
      "flight_2\t0.7000\t0.7500\n",
      "employee_hire_evaluation\t0.9474\t0.9737\n",
      "cre_Doc_Template_Mgt\t0.8333\t0.9048\n",
      "course_teach\t0.8667\t0.9333\n",
      "museum_visit\t0.7222\t0.8333\n",
      "wta_1\t0.6774\t0.6129\n",
      "battle_death\t0.5000\t0.5000\n",
      "student_transcripts_tracking\t0.6667\t0.6795\n",
      "tvshow\t0.7258\t0.6613\n",
      "poker_player\t0.8750\t0.8750\n",
      "voter_1\t0.6000\t0.6667\n",
      "world_1\t0.5083\t0.4833\n",
      "orchestra\t0.8000\t0.8750\n",
      "network_1\t0.6250\t0.4643\n",
      "dog_kennels\t0.5854\t0.5976\n",
      "singer\t0.8667\t0.8667\n",
      "real_estate_properties\t0.5000\t0.5000\n"
     ]
    }
   ],
   "source": [
    "for db_id, results in eval_sql_results_by_db_id.items():\n",
    "    _avg_exact = np.mean([d['exact'] for d in results])\n",
    "    _avg_exec = np.mean([d['exec'] for d in results])\n",
    "    print(f'{db_id}\\t{_avg_exact:.4f}\\t{_avg_exec:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_analysis_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the accelerate of the car make amc hornet sportabout (sw)?',\n",
       " '| car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker ( amc ) , fullname , country | model_list : modelid , maker , model ( amc ) | car_names : makeid , model ( amc ) , make ( amc hornet , amc hornet sportabout (sw) ) | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " \"select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t2.make = 'amc hornet sportabout (sw)';\")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_id = 111\n",
    "a_ex_id = 0\n",
    "\n",
    "ex = processed_spider_dev[ex_id]\n",
    "ex['text_in'], \\\n",
    "ex['struct_in'], \\\n",
    "ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp test\n",
    "# ex['seq_out'] = 'select year from cars_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex_list = ctu.create_analysis_sample_dicts(\n",
    "                mt_uskg, ex,\n",
    "                subject_type='column',\n",
    "                remove_struct_duplicate_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex_list[a_ex_id].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex_list[a_ex_id]['alias2table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(d['dec_prompt'], d['expect'], d['node_name_ranges'], d['expect_input_ranges'], '------',\\\n",
    "  d['self_ranges'], d['context_ranges'],\\\n",
    "  d['category'], '------' * 2) for d in a_ex_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(a_ex_list[a_ex_id])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ctu.add_clean_prediction(mt_uskg, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse_sql_alias2table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1': 'table_name', 't2': 'other_table', 't3': 'ttt'}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sql = 'SELECT t2.aaa , t3.ccc FROM table_name as t1 JOIN other_table as t2 on table_name.a_a = other_table.b_a JOIN ttt as t3 on other_table.asth = ttt.asth'\n",
    "ctu.parse_sql_alias2table(_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select t2.aaa, distinct(t3.ccc), count(*) from table_name as t1 join other_table as t2 on table_name.a_a = other_table.b_a join ttt as t3 on other_table.asth = ttt.asth where t2.col like',\n",
       " 'select t2.aaa, distinct(t3.ccc), count(*) from table_name as t1 join other_table as t2 on table_name.a_a = other_table.b_a join ttt as t3 on other_table.asth = ttt.asth where t2.col like %hey']"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sql = 'SELECT t2.aaa, DISTINCT(t3.ccc), COUNT(*) FROM table_name as t1 JOIN other_table as t2 on table_name.a_a = other_table.b_a JOIN ttt as t3 on other_table.asth = ttt.asth WHERE t2.col like %hey%'.lower()\n",
    "prompts = ctu.make_syntax_dec_prompt(_sql, '%', is_punct=True)\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "('.', ['select', 't1', '.', 'accelerate', 'from', 'cars_data', 'as', 't1', 'join', 'car_names', 'as', 't2', 'on', 't1', '.', 'id', '=', 't2', '.', 'makeid', 'where', 't2', '.', 'make', '=', \"'\", 'amc', 'hornet', 'sportabout', '(', 'sw', ')', \"'\", ';'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [875]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a_ex_list_syntax \u001b[38;5;241m=\u001b[39m \u001b[43mctu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_syntax_analysis_sample_dicts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmt_uskg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/rome/notebooks/experiments/causal_trace_uskg.py:2424\u001b[0m, in \u001b[0;36mcreate_syntax_analysis_sample_dicts\u001b[0;34m(mt, ex)\u001b[0m\n\u001b[1;32m   2422\u001b[0m _phrase \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(_phrase_cache)\n\u001b[1;32m   2423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _phrase:\n\u001b[0;32m-> 2424\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m _phrase \u001b[38;5;129;01min\u001b[39;00m SQL_SYNTAX_PHRASES \u001b[38;5;241m+\u001b[39m SQL_SYNTAX_PUNCTS, (_phrase, sql_tokens)\n\u001b[1;32m   2425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _phrase \u001b[38;5;129;01min\u001b[39;00m SQL_SYNTAX_PHRASES:\n\u001b[1;32m   2426\u001b[0m         syntax_phrases\u001b[38;5;241m.\u001b[39madd(_phrase)\n",
      "\u001b[0;31mAssertionError\u001b[0m: ('.', ['select', 't1', '.', 'accelerate', 'from', 'cars_data', 'as', 't1', 'join', 'car_names', 'as', 't2', 'on', 't1', '.', 'id', '=', 't2', '.', 'makeid', 'where', 't2', '.', 'make', '=', \"'\", 'amc', 'hornet', 'sportabout', '(', 'sw', ')', \"'\", ';'])"
     ]
    }
   ],
   "source": [
    "a_ex_list_syntax = ctu.create_syntax_analysis_sample_dicts(\n",
    "                mt_uskg, ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### context_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the name of the different car makers who produced a car in 1970?',\n",
       " '| car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " 'select year from cars_data',\n",
       " 'year')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex = dict(a_ex_list[0])\n",
    "a_ex['text_in'], a_ex['struct_in'], a_ex['seq_out'], a_ex['expect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_ex['alias2table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_range = a_ex['text_range']\n",
    "struct_range = a_ex['struct_range']\n",
    "\n",
    "# For full context tokens, use [0, L] and [R, -1]\n",
    "# L: node left max end index ; R: node right min start index\n",
    "\n",
    "token_ranges_dict = a_ex['token_ranges_dict']\n",
    "_all_node_range_lists = list(token_ranges_dict['col_name_ranges'].values()) + list(token_ranges_dict['table_name_ranges'].values()) + list(token_ranges_dict['db_id_ranges'].values())\n",
    "_all_node_ranges = [rg\n",
    "                    for rg_list in _all_node_range_lists\n",
    "                    for rg in rg_list]\n",
    "_all_left_endpoint = [s for s, e in _all_node_ranges] + [struct_range[1]]\n",
    "_all_right_endpoint = [e for s, e in _all_node_ranges] + [struct_range[0]]\n",
    "# TODO: pull this part out to the shared function (e.g. create_analysis_sample_dicts)\n",
    "# TODO: test for columns on ends\n",
    "\n",
    "expect_input_ranges = a_ex['expect_input_ranges']    # list of ranges of node-of-interest (code allows dup)\n",
    "# tok_indices = [i for s, e in expect_input_ranges for i in range(s, e)]\n",
    "# expect_input_indices = [i for s, e in expect_input_ranges for i in range(s, e)]\n",
    "enc_sentence = a_ex['enc_sentence']\n",
    "dec_prompt = a_ex['dec_prompt']\n",
    "# node = a_ex['expect']\n",
    "\n",
    "context_range_endpoints = [struct_range[0]]\n",
    "self_range_endpoints = []       # This is different from `expect_input_ranges`: this includes boundary toks\n",
    "for tok_s, tok_e in expect_input_ranges:\n",
    "    _l = max([e for e in _all_right_endpoint if e <= tok_s])\n",
    "    _r = min([s for s in _all_left_endpoint if s >= tok_e])\n",
    "    context_range_endpoints.extend([_l, _r])\n",
    "    self_range_endpoints.extend([_l, _r])\n",
    "context_range_endpoints.append(struct_range[1])\n",
    "\n",
    "self_ranges = [(self_range_endpoints[i], self_range_endpoints[i+1])\n",
    "                for i in range(0, len(self_range_endpoints), 2)]\n",
    "self_tok_indices = [i for s, e in self_ranges for i in range(s, e)]\n",
    "\n",
    "context_ranges = [(context_range_endpoints[i], context_range_endpoints[i+1])\n",
    "                    for i in range(0, len(context_range_endpoints), 2)]\n",
    "context_ranges = [(s, e) for s, e in context_ranges if e > s]\n",
    "context_tok_indices = corrupt_tok_indices = [i for s, e in context_ranges for i in range(s, e)]\n",
    "\n",
    "text_tok_indices = list(range(*text_range))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(137, 140)], [(24, 137)])"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_ranges, context_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tok_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", year\n"
     ]
    }
   ],
   "source": [
    "for s, e in self_ranges:\n",
    "    _piece = tokenizer.decode(a_ex['enc_tokenized']['input_ids'][s : e])\n",
    "    print(_piece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| car_1 | continents : contid, continent | countries : countryid, countryname, continent | car_makers : id, maker, fullname, country | model_list : modelid, maker, model | car_names : makeid, model, make | cars_data : id, mpg, cylinders, edispl, horsepower, weight, accelerate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, e in context_ranges:\n",
    "    _piece = tokenizer.decode(a_ex['enc_tokenized']['input_ids'][s : e])\n",
    "    print(_piece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_uskg_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex = dict(a_ex_list[a_ex_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sentence = a_ex['enc_sentence']\n",
    "dec_prompt = a_ex['dec_prompt']\n",
    "expect = a_ex['expect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the name of the different car makers who produced a car in 1970?; structed knowledge: | car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker , fullname , country | model_list : modelid , maker , model | car_names : makeid , model , make | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " 'select distinct t1.maker from car_makers as t1 join model_list as t2 on t1.id = t2.maker join car_names as t3 on t2.model = t3.model join cars_data as t4 on t3.',\n",
       " 'makeid')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sentence, dec_prompt, expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence] * 11,\n",
    "    [dec_prompt] * 11,\n",
    "    answer=expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = ctu.run_model_forward_uskg(mt_uskg.model, **inp, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'logits', 'past_key_values', 'decoder_hidden_states', 'decoder_attentions', 'cross_attentions', 'encoder_last_hidden_state', 'encoder_hidden_states', 'encoder_attentions'])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, torch.Size([11, 16, 141, 151]))"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers, (bsz, n_heads, seq_len, seq_len + prev_len)\n",
    "len(_out.encoder_attentions), _out.encoder_attentions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, torch.Size([11, 16, 2, 151]))"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers, (bsz, n_heads, prompt_len, seq_len + prev_len)\n",
    "len(_out.cross_attentions), _out.cross_attentions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, torch.Size([11, 16, 2, 12]))"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers, (bsz, n_heads, prompt_len, prompt_len + prev_len)\n",
    "len(_out.decoder_attentions), _out.decoder_attentions[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'decoder_input_ids', 'decoder_attention_mask'])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex = dict(a_ex_list[a_ex_id])\n",
    "a_ex = ctu.add_clean_prediction(mt_uskg, a_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enc_sentence': 'Which city has the most frequent destination airport?; structed knowledge: | flight_2 | airlines : uid , airline , abbreviation , country | airports : city , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport',\n",
       " 'seq_out': 'select t1.city from airports as t1 join flights as t2 on t1.airportcode = t2.destairport group by t1.city order by count(*) desc limit 1',\n",
       " 'dec_prompt': 'select t1.city from airports as t1 join flights as t2 on t1.airportcode = t2.destairport group by t1.',\n",
       " 'expect': 'city',\n",
       " 'expect_type': 'column',\n",
       " 'db_id': 'flight_2',\n",
       " 'expect_input_ranges': [(45, 46)],\n",
       " 'expect_table': 'airports',\n",
       " 'answer': 'city',\n",
       " 'base_score': 0.9983423948287964,\n",
       " 'answers_t': [6726],\n",
       " 'correct_prediction': True,\n",
       " 'category': {'sql_hardness': 'extra',\n",
       "  'node_role': 'group by',\n",
       "  'text_match': 'exact'}}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ctu.make_basic_result_dict(a_ex)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sentence = a_ex['enc_sentence']\n",
    "dec_prompt = a_ex['dec_prompt']\n",
    "expect = a_ex['expect']\n",
    "answer = result['answer']\n",
    "answers_t = result['answers_t']\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence] * 11,\n",
    "    [dec_prompt] * 11,\n",
    "    answer=expect)\n",
    "\n",
    "text_range = a_ex['text_range']\n",
    "struct_range = a_ex['struct_range']\n",
    "\n",
    "self_ranges = a_ex['self_ranges']\n",
    "context_ranges = a_ex['context_ranges']\n",
    "\n",
    "self_tok_indices = [i for s, e in self_ranges for i in range(s, e)]\n",
    "context_tok_indices = corrupt_tok_indices = [i for s, e in context_ranges for i in range(s, e)]\n",
    "text_tok_indices = list(range(*text_range))\n",
    "struct_tok_indices = list(range(*struct_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "#                     for tnum in range(*struct_range)],\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6726], 'city', 0.8450507521629333)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_t, answer, _score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8450507521629333"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_to_corrupt = [(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", 0, \"embed\"))\n",
    "                for tnum in text_tok_indices]\n",
    "\n",
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    states_to_corrupt=states_to_corrupt,\n",
    "#     tokens_to_mix=corrupt_tok_indices,\n",
    "#     tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ").item()\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9952, device='cuda:0')"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pair of identical input to test correctness \n",
    "\n",
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "#                     for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_patch=[],\n",
    "    states_to_patch_1st_pass=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 12))\n",
    "                    for tnum in text_tok_indices],\n",
    "    states_to_unpatch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 23))\n",
    "                    for tnum in struct_tok_indices],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=text_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    tokens_to_mix_1st_pass=context_tok_indices,\n",
    "    replace=True,\n",
    ").item()\n",
    "\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9952, device='cuda:0')"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "#                     for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_patch=[],\n",
    "    states_to_patch_1st_pass=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 12))\n",
    "                    for tnum in text_tok_indices],\n",
    "    states_to_unpatch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', 23))\n",
    "                    for tnum in struct_tok_indices],\n",
    "    answers_t=answers_t,\n",
    "    states_to_corrupt=[(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", 0, \"embed\"))\n",
    "                    for tnum in text_tok_indices],\n",
    "    states_to_corrupt_1st_pass=[(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", 0, \"embed\"))\n",
    "                    for tnum in context_tok_indices],\n",
    "    replace=True,\n",
    ").item()\n",
    "\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7253002524375916"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test corrupting attention \n",
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "#                     for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    states_to_corrupt=[(tnum, ctu.layername_uskg(mt_uskg.model, \"encoder\", l, \"self_attn\"))\n",
    "                    for tnum in text_tok_indices for l in range(mt_uskg.num_enc_layers)],\n",
    "    replace=True,\n",
    ").item()\n",
    "\n",
    "_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n, w in mt_uskg.model.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_probs = ctu.run_repatch_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "#                     for tnum in range(*struct_range)],\n",
    "    states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', l))\n",
    "                    for tnum in self_tok_indices for l in range(mt_uskg.num_enc_layers - 1)],\n",
    "    states_to_unpatch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "                    for tnum in self_tok_indices],\n",
    "    answer_len=len(answers_t),\n",
    "    tokens_to_mix=corrupt_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32102])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.], device='cuda:0'),\n",
       "indices=tensor([7634], device='cuda:0'))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(vocab_probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs[0, 7634]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2642e-25, 1.2223e-15, 7.3942e-18,  ..., 9.1578e-20, 2.6884e-39,\n",
       "         2.8131e-39]], device='cuda:0')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trace - partial edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uskg.models.prompt.modeling_t5 import T5Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_t5_config = copy.deepcopy(mt_uskg.model.config)\n",
    "_t5_config.d_model = 10\n",
    "_t5_config.d_kv = 4\n",
    "_t5_config.num_heads = 3\n",
    "_t5_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Attention(\n",
       "  (q): Linear(in_features=10, out_features=12, bias=False)\n",
       "  (k): Linear(in_features=10, out_features=12, bias=False)\n",
       "  (v): Linear(in_features=10, out_features=12, bias=False)\n",
       "  (o): Linear(in_features=12, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_attn_module = T5Attention(config=_t5_config)\n",
    "test_attn_module.eval()\n",
    "test_attn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('q.weight', torch.Size([12, 10])),\n",
       " ('k.weight', torch.Size([12, 10])),\n",
       " ('v.weight', torch.Size([12, 10])),\n",
       " ('o.weight', torch.Size([10, 12]))]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v.size()) for k, v in test_attn_module.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'q', 'k', 'v', 'o']"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k, v in test_attn_module.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7, 10]),\n",
       " [('prev_key', torch.Size([1, 3, 2, 4])),\n",
       "  ('prev_value', torch.Size([1, 3, 2, 4])),\n",
       "  ('prev_key_padding_mask', torch.Size([1, 2]))])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_bs = 1\n",
    "_test_seqlen = 7\n",
    "_test_prevlen = 2\n",
    "\n",
    "_test_h = torch.randn(_test_bs, _test_seqlen, _t5_config.d_model)\n",
    "# _test_h[:, 0] = 999.0\n",
    "_test_prefix = {\n",
    "    'prev_key': torch.randn(_test_bs, _t5_config.num_heads, _test_prevlen, _t5_config.d_kv),\n",
    "    'prev_value': torch.randn(_test_bs, _t5_config.num_heads, _test_prevlen, _t5_config.d_kv),\n",
    "    'prev_key_padding_mask': torch.zeros(_test_bs, _test_prevlen).bool()\n",
    "}\n",
    "_test_h.size(), [(k, v.size()) for k, v in _test_prefix.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8561, -0.0552,  0.3845,  1.1441, -1.2765, -0.6869, -0.9646,\n",
       "          -1.0255, -2.3659, -1.7153],\n",
       "         [ 1.0685, -0.1617,  0.8310, -1.7257,  0.3674,  1.7559,  0.5763,\n",
       "          -0.9344,  0.9016,  0.7490],\n",
       "         [-1.1887, -1.0820, -0.5925,  0.7623, -0.6538, -0.0067,  0.5618,\n",
       "           1.3310,  1.2580, -0.6973],\n",
       "         [ 0.2807,  0.0763, -0.3539,  0.9494, -0.1557, -0.7645, -0.2103,\n",
       "          -1.0175, -0.3029, -0.0376],\n",
       "         [ 0.0984,  0.5610, -2.3323,  1.3421, -1.0381, -1.8568, -0.7754,\n",
       "          -1.6037,  0.2501, -1.4155],\n",
       "         [-1.4474, -0.4784,  0.0972, -0.3393,  1.2340,  0.7611, -0.4786,\n",
       "           0.0506, -0.1188,  2.7051],\n",
       "         [ 2.0642, -0.0186, -0.8283,  1.0852,  0.9819, -0.4044,  0.9831,\n",
       "          -0.2723,  0.2037,  1.6401]]])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev_key': tensor([[[[-2.2578,  0.2019,  0.3287,  0.3906],\n",
       "           [-1.2143, -0.7039, -1.0298,  0.1425]],\n",
       " \n",
       "          [[ 0.3967,  1.5168,  0.0967,  1.4454],\n",
       "           [ 0.1648,  0.2483,  1.5992,  1.2469]],\n",
       " \n",
       "          [[ 1.3875,  0.4460, -0.2676, -1.2290],\n",
       "           [ 2.0209,  1.1736,  0.8446,  0.8827]]]]),\n",
       " 'prev_value': tensor([[[[ 0.8351,  1.2052,  1.4187, -0.5358],\n",
       "           [-1.4274,  0.2792,  2.0149,  1.3695]],\n",
       " \n",
       "          [[-0.0379,  1.8999, -0.4236, -0.9176],\n",
       "           [ 1.5794,  1.1735, -0.2925,  2.2855]],\n",
       " \n",
       "          [[ 1.1935,  0.9343, -0.5582,  0.8163],\n",
       "           [-1.3765,  0.4046,  1.0941,  0.4058]]]]),\n",
       " 'prev_key_padding_mask': tensor([[False, False]])}"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_mask = torch.zeros(1, 1, 1, _test_seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out = test_attn_module.forward(\n",
    "    _test_h,\n",
    "    mask=_test_mask,\n",
    "    prefix=_test_prefix,\n",
    "    output_attentions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3544, 0.3264, 0.2939, 0.3878, 0.4695, 0.2997, 0.1775],\n",
       "         [0.4028, 0.1655, 0.2949, 0.5148, 0.4162, 0.4189, 0.4683],\n",
       "         [0.5839, 0.4639, 0.1393, 0.4925, 0.6272, 0.1917, 0.4634]]])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out[3].sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_mask_attention(att, mix_mask):\n",
    "    # token 4,5,6 (in real_seq) not attending to 2 (in full_seq, i.e. 0 in real_seq)\n",
    "    # att: (bs, n_head, real_seq, full_seq)\n",
    "#     att[:, :, 4:, 2] = 0.0\n",
    "\n",
    "    _zero = torch.tensor(0, dtype=att.dtype)\n",
    "    att = torch.where(mix_mask, _zero, att)\n",
    "\n",
    "    print(att)\n",
    "    return att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ True, False,  True, False,  True, False,  True, False,  True],\n",
       "           [False, False, False, False, False, False, False, False, False],\n",
       "           [ True, False,  True, False,  True, False,  True, False,  True],\n",
       "           [False, False, False, False, False, False, False, False, False],\n",
       "           [ True, False,  True, False,  True, False,  True, False,  True],\n",
       "           [False, False, False, False, False, False, False, False, False],\n",
       "           [ True, False,  True, False,  True, False,  True, False,  True]]]]),\n",
       " tensor([[[[ True,  True, False, False, False,  True,  True,  True,  True],\n",
       "           [ True,  True, False, False, False,  True,  True,  True,  True],\n",
       "           [ True,  True, False, False, False,  True,  True,  True,  True],\n",
       "           [ True,  True,  True,  True,  True, False, False, False, False],\n",
       "           [ True,  True,  True,  True,  True, False, False, False, False],\n",
       "           [ True,  True,  True,  True,  True, False, False, False, False],\n",
       "           [ True,  True,  True,  True,  True, False, False, False, False]]]]))"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_mask = torch.zeros(1, 1, _test_seqlen, _test_seqlen + _test_prevlen, dtype=bool)\n",
    "mix_mask[:, :, ::2, ::2] = 1\n",
    "\n",
    "mix_mask_2 = torch.zeros(1, 1, _test_seqlen, _test_seqlen + _test_prevlen, dtype=bool)\n",
    "mix_mask_2[:, :, :, :_test_prevlen] = 1\n",
    "mix_mask_2[:, :, 3:, _test_prevlen : _test_prevlen+3] = 1\n",
    "mix_mask_2[:, :, :3, _test_prevlen+3 :] = 1\n",
    "\n",
    "mix_mask, mix_mask_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_mask | mix_mask_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attn_module.ext_attention_weights_fn = lambda att : _test_mask_attention(att, mix_mask_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x7fc33d6065e0>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(test_attn_module.ext_attention_weights_fn)\n",
    "print(test_attn_module.ext_attention_logits_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_out = test_attn_module.forward(\n",
    "    _test_h,\n",
    "    mask=_test_mask,\n",
    "    prefix=_test_prefix,\n",
    "    output_attentions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ex = dict(a_ex_list[a_ex_id])\n",
    "a_ex = ctu.add_clean_prediction(mt_uskg, a_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enc_sentence': 'What is the accelerate of the car make amc hornet sportabout (sw)?; structed knowledge: | car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker ( amc ) , fullname , country | model_list : modelid , maker , model ( amc ) | car_names : makeid , model ( amc ) , make ( amc hornet , amc hornet sportabout (sw) ) | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " 'seq_out': \"select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t2.make = 'amc hornet sportabout (sw)';\",\n",
       " 'dec_prompt': 'select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t2.',\n",
       " 'expect': 'make',\n",
       " 'expect_type': 'column',\n",
       " 'db_id': 'car_1',\n",
       " 'expect_input_ranges': [(121, 145)],\n",
       " 'expect_table': 'car_names',\n",
       " 'answer': 'make',\n",
       " 'base_score': 0.9998800754547119,\n",
       " 'answers_t': [19509],\n",
       " 'correct_prediction': True,\n",
       " 'category': {'sql_hardness': 'medium',\n",
       "  'node_role': 'where',\n",
       "  'text_match': 'exact'}}"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = ctu.make_basic_result_dict(a_ex)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sentence = a_ex['enc_sentence']\n",
    "dec_prompt = a_ex['dec_prompt']\n",
    "expect = a_ex['expect']\n",
    "answer = result['answer']\n",
    "answers_t = result['answers_t']\n",
    "\n",
    "inp = ctu.make_inputs_t5(\n",
    "    mt_uskg.tokenizer,\n",
    "    [enc_sentence] * 2,\n",
    "    [dec_prompt] * 2,\n",
    "    answer=expect)\n",
    "\n",
    "text_range = a_ex['text_range']\n",
    "struct_range = a_ex['struct_range']\n",
    "\n",
    "self_ranges = a_ex['self_ranges']\n",
    "context_ranges = a_ex['context_ranges']\n",
    "\n",
    "self_tok_indices = [i for s, e in self_ranges for i in range(s, e)]\n",
    "context_tok_indices = corrupt_tok_indices = [i for s, e in context_ranges for i in range(s, e)]\n",
    "text_tok_indices = list(range(*text_range))\n",
    "struct_tok_indices = list(range(*struct_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "_score = ctu.trace_with_repatch_uskg(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     states_to_patch=[(tnum, ctu.layername_uskg(mt_uskg.model, 'encoder', mt_uskg.num_enc_layers - 1))\n",
    "#                     for tnum in range(*struct_range)],\n",
    "    states_to_patch=[],\n",
    "    states_to_unpatch=[],\n",
    "    answers_t=answers_t,\n",
    "    tokens_to_mix=context_tok_indices,\n",
    "    tokens_to_mix_individual_indices=True,\n",
    "    replace=True,\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([19509], 'make', 0.9987825751304626)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_t, answer, _score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "_score = ctu.trace_attention_manip_uskg_multi_token(\n",
    "# _probs = ctu.run_attention_manip_uskg_multi_token(\n",
    "    model=mt_uskg.model,\n",
    "    inp=inp,\n",
    "#     answer_len=len(answers_t),\n",
    "    answers_t=answers_t,\n",
    "#     states_to_patch=[],\n",
    "    layers_to_mix=[ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in range(24)],\n",
    "    src_tokens_to_mix=text_tok_indices + struct_tok_indices, # src doesn't have prefix \n",
    "#     src_tokens_to_mix=[-1],\n",
    "    tgt_tokens_to_mix=list(range(10)) + [i + 10 for i in text_tok_indices + struct_tok_indices],  # tgt has prefix \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.6447], device='cuda:0'),\n",
       "indices=tensor([4350], device='cuda:0'))"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_probs.max(dim=-1)\n",
    "# _score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name'"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer.decode([4350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctu.layername_uskg(mt_uskg.model, 'encoder', 0, 'self_attn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit - attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder mask (toy example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex['text_range'] = (0, 3)\n",
    "ex['struct_range'] = (5, 10)\n",
    "ex['enc_sentence'] = \"what is? empty empty Table column column Table column\"\n",
    "ex['struct_node_ranges_dict'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " '?',\n",
       " 'empty',\n",
       " 'empty',\n",
       " 'Table',\n",
       " 'column',\n",
       " 'column',\n",
       " 'Table',\n",
       " 'column']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_uskg.tokenizer_uskg.tokenize(ex['enc_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'question', 'db_id', 'db_path', 'db_table_names', 'db_column_names', 'db_column_types', 'db_primary_keys', 'db_foreign_keys', 'rat_sql_graph', 'serialized_schema', 'struct_in', 'text_in', 'seq_out', 'text_range', 'struct_range', 'enc_sentence', 'struct_node_ranges_dict'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 3), (5, 10))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['text_range'], ex['struct_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_mask = build_enc_self_attention_mask_for_section_pair(\n",
    "    ex,\n",
    "    q_sect='text',\n",
    "    k_sect='text',\n",
    "    seq_len=None,\n",
    "    mt=mt_uskg,\n",
    "    prefix_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]]])\n"
     ]
    }
   ],
   "source": [
    "print(mix_mask.to(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_mask = build_dec_cross_attention_mask_for_section(\n",
    "    ex,\n",
    "    k_sect='others',\n",
    "    seq_len=None,\n",
    "    mt=mt_uskg,\n",
    "    prefix_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "print(mix_mask.to(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What is the accelerate of the car make amc hornet sportabout (sw)?',\n",
       " '| car_1 | continents : contid , continent | countries : countryid , countryname , continent | car_makers : id , maker ( amc ) , fullname , country | model_list : modelid , maker , model ( amc ) | car_names : makeid , model ( amc ) , make ( amc hornet , amc hornet sportabout (sw) ) | cars_data : id , mpg , cylinders , edispl , horsepower , weight , accelerate , year',\n",
       " \"select t1.accelerate from cars_data as t1 join car_names as t2 on t1.id = t2.makeid where t2.make = 'amc hornet sportabout (sw)';\")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex['text_in'], \\\n",
    "ex['struct_in'], \\\n",
    "ex['seq_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select t1.accelerate from cars_data as t1 join car_names as t2 on t1.make = t2.makeid where t2.make = \"amc hornet sportabout\"'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select t1.name from car as t1 join t2 on t1.car = t2.caravan (select t2.caravan from car as t1 join t2 on t1.car = t2.caravan where t1.year = _2010'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_layers = [ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in range(24)]\n",
    "section_pairs = [('all', 'all')]\n",
    "\n",
    "with add_enc_attention_edit(mt=mt_uskg,\n",
    "                        ex=ex,\n",
    "                        mix_layers=mix_layers,\n",
    "                        section_pairs=section_pairs,\n",
    "                        attn_corrupt_type='weights'):\n",
    "\n",
    "    pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select t1.accelerate from cars_data as t1 join car_names as t2 on t1.make = t2.makeid where t2.make = \"amc hornet sportabout\" (sw)'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_layers = [ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in range(6, 18)]\n",
    "section_pairs = [('struct', 'text')]\n",
    "\n",
    "with add_enc_attention_edit(mt=mt_uskg,\n",
    "                        ex=ex,\n",
    "                        mix_layers=mix_layers,\n",
    "                        section_pairs=section_pairs,\n",
    "                        attn_corrupt_type='weights'):\n",
    "\n",
    "    pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False, False, False, False, False, False, False, False,\n",
      "           False, False, False]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cars_data: accelerated = 0-60 seconds, t1.speed = 0-60 seconds, t1.year = 00 and t1.year = 00'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_layers = [ctu.layername_uskg(mt_uskg.model, 'decoder', l, 'cross_attn') for l in range(24)]\n",
    "sections = ['prefix']\n",
    "\n",
    "with add_dec_cross_attention_edit(mt=mt_uskg,\n",
    "                        ex=ex,\n",
    "                        mix_layers=mix_layers,\n",
    "                        sections=sections,\n",
    "                        attn_corrupt_type='weights'):\n",
    "\n",
    "    pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_in = ex['text_in']\n",
    "# struct_in = ex['struct_in']\n",
    "\n",
    "# enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "# enc_tokenized = mt_uskg.tokenizer(enc_sentence)\n",
    "# ex['enc_sentence'] = enc_sentence\n",
    "# ex['enc_tokenized'] = enc_tokenized\n",
    "\n",
    "# parsed_struct_in = ctu.parse_struct_in(struct_in)\n",
    "\n",
    "# token_ranges_dict = ctu.find_struct_name_ranges(mt_uskg.tokenizer, ex)\n",
    "# text_range = ex['text_range']\n",
    "# struct_range = ex['struct_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = mt_uskg.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['mar', 'y', ':', 'has', '', 'a', 'little', 'lamb', 'b', 'b'],\n",
       " ['mar', 'y', ':', 'has', '', 'a', 'little', 'lamb', 'b', 'b'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"mary: has a little  lambbb\"\n",
    "s_ = \"mary: has a little lambbb\"\n",
    "tokenizer.tokenize(s), tokenizer.tokenize(s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3157, 63, 10, 65, 3, 9, 385, 17871, 115, 115, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer(s)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'mar'),\n",
       " (1, 'y'),\n",
       " (2, ':'),\n",
       " (3, 'has'),\n",
       " (4, ''),\n",
       " (5, 'a'),\n",
       " (6, 'little'),\n",
       " (7, 'lamb'),\n",
       " (8, 'b'),\n",
       " (9, 'b'),\n",
       " (10, '</s>')]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(t.tokens()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenSpan(start=7, end=10)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.word_to_tokens(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lambbb'"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.decode_sentences(tokenizer, t['input_ids'][7:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"(a(a)a)\".rindex(\")\"), \"(a(a)a)\".index(\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = \"\"\"| concert_singer | singer : singer_id , name ( First Last ) , country ( France , Germany , United States ) , \\\n",
    "song_name , song_release_year , age , is_male\"\"\"\n",
    "\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test struct_in parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "_struct_in = \"\"\"| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , \\\n",
    "average | singer : singer_id , name ( First Last ) , country ( France , Germany , United States ) , \\\n",
    "song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , \\\n",
    "stadium_id , year ( 2008 , 2012 , 2022 ) | singer_in_concert : concert_id , singer_id\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 'stadium'),\n",
       "  [[(5, 'stadium_id'), []],\n",
       "   [(7, 'location'), []],\n",
       "   [(9, 'name'), []],\n",
       "   [(11, 'capacity'), []],\n",
       "   [(13, 'highest'), []],\n",
       "   [(15, 'lowest'), []],\n",
       "   [(17, 'average'), []]]),\n",
       " ((19, 'singer'),\n",
       "  [[(21, 'singer_id'), []],\n",
       "   [(23, 'name'), [(25, 'First Last')]],\n",
       "   [(29, 'country'), [(31, 'France'), (33, 'Germany'), (35, 'United States')]],\n",
       "   [(39, 'song_name'), []],\n",
       "   [(41, 'song_release_year'), []],\n",
       "   [(43, 'age'), []],\n",
       "   [(45, 'is_male'), []]]),\n",
       " ((47, 'concert'),\n",
       "  [[(49, 'concert_id'), []],\n",
       "   [(51, 'concert_name'), []],\n",
       "   [(53, 'theme'), []],\n",
       "   [(55, 'stadium_id'), []],\n",
       "   [(57, 'year'), [(59, '2008'), (61, '2012'), (63, '2022')]]]),\n",
       " ((66, 'singer_in_concert'),\n",
       "  [[(68, 'concert_id'), []], [(70, 'singer_id'), []]])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctu.parse_struct_in(_struct_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "_text_in = text_in\n",
    "\n",
    "enc_sentence = f\"{_text_in}; structed knowledge: {_struct_in}\"\n",
    "enc_tokenized = mt_uskg.tokenizer(enc_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ranges_dict = ctu.find_struct_name_ranges(mt_uskg.tokenizer, enc_tokenized['input_ids'], _struct_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_ranges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d_key, d in token_ranges_dict.items():\n",
    "    for name, ranges in d.items():\n",
    "        for s, e in ranges:\n",
    "            recs_name = ctu.decode_sentences(mt_uskg.tokenizer, enc_tokenized['input_ids'][s:e])\n",
    "            print(f'{d_key}\\t{name}\\t{recs_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False,  True]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cond = torch.eye(2).to(dtype=bool).view(1, 1, 4)\n",
    "_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 1.],\n",
       "         [1., 0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0., 1.],\n",
       "         [1., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ones = torch.ones(2, 2, 4)\n",
    "_zeros = torch.zeros(2, 2, 4)\n",
    "\n",
    "torch.where(_cond, _ones, _zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from play_pred()\n",
    "\n",
    "def pred_sql_with_edit(mt, ex, f_hook_fn=None, p_hook_fn=None):\n",
    "    text_in = ex['text_in']\n",
    "    struct_in = ex['struct_in']\n",
    "\n",
    "    txt = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "    \n",
    "    tokenized_txt = mt.tokenizer_uskg([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    device = mt.model.device\n",
    "    \n",
    "    all_hooks = []\n",
    "    if f_hook_fn is not None:\n",
    "        f_hook = mt.model.register_forward_hook(f_hook_fn)\n",
    "        all_hooks.append(f_hook)\n",
    "    if p_hook_fn is not None:\n",
    "        p_hook = mt.model.register_forward_pre_hook(p_hook_fn)\n",
    "        all_hooks.append(p_hook)\n",
    "    \n",
    "    pred = mt.tokenizer_uskg.batch_decode(\n",
    "      mt.model.generate(\n",
    "        torch.tensor(tokenized_txt.data['input_ids'], dtype=int, device=device),\n",
    "        torch.tensor(tokenized_txt.data['attention_mask'], dtype=int, device=device),\n",
    "        num_beams=1, \n",
    "        max_length=256\n",
    "        ), \n",
    "      skip_special_tokens=True \n",
    "    )\n",
    "    \n",
    "    for hook in all_hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return pred[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused implementation \n",
    "\n",
    "def build_enc_self_attention_global_mask(\n",
    "    ex,\n",
    "    seq_len=None,\n",
    "    mt=None,\n",
    "    prefix_len=10,\n",
    "    use_self_node=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    BBB\n",
    "    For attention-related experiments: build encoder self-attention masks across sections\n",
    "    (\"global\" means it is not specific to certain node, i.e. \"self\" means each node itself)\n",
    "\n",
    "    Args:\n",
    "    a_ex (Dict)\n",
    "    seq_len (int): sequence length\n",
    "    prefix_len (int): prefix length\n",
    "    use_self_node (bool): whether to include \"self\" and \"context\" related sections\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    enc_sentence = ex['enc_sentence']\n",
    "\n",
    "    text_range = ex['text_range']\n",
    "    struct_range = ex['struct_range']\n",
    "    text_st, text_ed = text_range\n",
    "    struct_st, struct_ed = struct_range\n",
    "    # text_tok_indices = list(range(*text_range))\n",
    "    # struct_tok_indices = list(range(*struct_range))\n",
    "    \n",
    "    token_ranges_dict = ex['struct_node_ranges_dict']\n",
    "\n",
    "#     if use_self_node:\n",
    "#         expect_input_ranges = a_ex['expect_input_ranges']\n",
    "#         tok_indices = [i for s, e in expect_input_ranges for i in range(s, e)]\n",
    "\n",
    "#         self_ranges = a_ex['self_ranges']\n",
    "#         struct_context_ranges = a_ex['context_ranges']\n",
    "\n",
    "#         self_tok_indices = [i for s, e in self_ranges for i in range(s, e)]\n",
    "#         struct_context_tok_indices = [i for s, e in struct_context_ranges for i in range(s, e)]\n",
    "\n",
    "#         self_tok_indices_tgt_side = [i + prefix_len for i in self_tok_indices]\n",
    "#         struct_context_tok_indices_tgt_side = [i + prefix_len for i in struct_context_tok_indices]\n",
    "\n",
    "    if seq_len is None:\n",
    "        # need to tokenize and decide seq_len\n",
    "        # TODO: untested!\n",
    "        assert mt is not None\n",
    "        _tok_ids = mt.tokenizer.encode(enc_sentence, add_special_tokens=True)\n",
    "        seq_len = len(_tok_ids)\n",
    "\n",
    "    att_mix_mask_dict = dict()\n",
    "    # key: (q_sect, k_sect)\n",
    "    # mix_mask: (batch, head, src_len, tgt_len)\n",
    "    \n",
    "    t2s_mask = torch.zeros(1, 1, seq_len, seq_len + prefix_len).bool()\n",
    "    t2s_mask[:, :, text_st : text_ed, struct_st + prefix_len : struct_ed + prefix_len] = True\n",
    "    att_mix_mask_dict[('text', 'struct')] = t2s_mask\n",
    "\n",
    "    s2t_mask = torch.zeros_like(t2s_mask).bool()\n",
    "    s2t_mask[:, :, struct_st : struct_ed, text_st + prefix_len : text_ed + prefix_len] = True\n",
    "    att_mix_mask_dict[('struct', 'text')] = s2t_mask\n",
    "\n",
    "#     att_mix_mask_dict['t<->s'] = t2s_mask | s2t_mask\n",
    "\n",
    "    t2p_mask = torch.zeros_like(t2s_mask).bool()\n",
    "    t2p_mask[:, :, text_st : text_ed, :prefix_len] = True\n",
    "    att_mix_mask_dict[('text', 'prefix')] = t2p_mask\n",
    "\n",
    "    s2p_mask = torch.zeros_like(t2s_mask).bool()\n",
    "    s2p_mask[:, :, struct_st : struct_ed, :prefix_len] = True\n",
    "    att_mix_mask_dict[('struct', 'prefix')] = s2p_mask\n",
    "\n",
    "#     att_mix_mask_dict['ts->p'] = t2p_mask | s2p_mask\n",
    "\n",
    "    # ADDED: section self-attention\n",
    "    t2t_mask = torch.zeros_like(t2s_mask).bool()\n",
    "    t2t_mask[:, :, text_st : text_ed, text_st + prefix_len : text_ed + prefix_len] = True\n",
    "    att_mix_mask_dict[('text', 'text')] = t2t_mask\n",
    "\n",
    "    s2s_mask = torch.zeros_like(t2s_mask).bool()\n",
    "    s2s_mask[:, :, struct_st : struct_ed, struct_st + prefix_len : struct_ed + prefix_len] = True\n",
    "    att_mix_mask_dict[('struct', 'struct')] = s2s_mask\n",
    "\n",
    "    if use_self_node:\n",
    "        # TODO: self->self, self->context \n",
    "        # iterate through node ranges \n",
    "        NotImplemented\n",
    "        \n",
    "        \n",
    "#         # ADDED: regarding struct context\n",
    "#         # Notice that it's ok to have 1 list in indexing, but not ok to have 2\n",
    "#         # If there are 2 lists, it will become a \"gather()\" which treats the 2 lists in a zipped way\n",
    "#         s2c_mask = torch.zeros_like(t2s_mask).bool()\n",
    "#         s2c_mask[:, :, struct_st : struct_ed, struct_context_tok_indices_tgt_side] = True\n",
    "#         att_mix_mask_dict['s->c'] = s2c_mask\n",
    "\n",
    "#         c2p_mask = torch.zeros_like(t2s_mask).bool()\n",
    "#         c2p_mask[:, :, struct_context_tok_indices, :prefix_len] = True\n",
    "#         att_mix_mask_dict['c->p'] = c2p_mask\n",
    "\n",
    "#         # c2t: skipped, as already see even s2t is not so effective\n",
    "\n",
    "#         c2s_mask = torch.zeros_like(t2s_mask).bool()\n",
    "#         c2s_mask[:, :, struct_context_tok_indices, struct_st + prefix_len : struct_ed + prefix_len] = True\n",
    "#         att_mix_mask_dict['c->s'] = c2s_mask\n",
    "\n",
    "#         c2c_mask = c2s_mask.clone()\n",
    "#         c2c_mask[:, :, :, self_tok_indices_tgt_side] = False\n",
    "#         assert c2c_mask.sum().item() == len(struct_context_tok_indices) ** 2, \\\n",
    "#             (c2c_mask.sum().item(), len(struct_context_tok_indices) ** 2)\n",
    "#         att_mix_mask_dict['c->c'] = c2c_mask\n",
    "\n",
    "    att_mix_mask_dict[('all', 'all')] = torch.ones_like(t2s_mask).bool()\n",
    "\n",
    "    return att_mix_mask_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old hook-based implementation of exp A1.0 \n",
    "\n",
    "# def add_attention_edit_hook(mt, ex, mix_layers, section_pairs, attn_corrupt_type='weights'):\n",
    "#     \"\"\" \n",
    "#     Args:\n",
    "#         mix_layers: List[int], the layers to edit\n",
    "#         section_pairs: List[Tuple(str, str)]: corrupting attention from which to which\n",
    "#             sections: \"text\", \"struct\", \"prefix\", \"eos\"; \"self\", \"context\"\n",
    "#             (notice that \"self\" is per-node; \"context->context\" is invalid here)\n",
    "#         attn_corrupt_type: (same as trace exp) weights / logits \n",
    "#     \"\"\"\n",
    "    \n",
    "#     ex = copy.deepcopy(ex)\n",
    "    \n",
    "#     text_in = ex['text_in']\n",
    "#     struct_in = ex['struct_in']\n",
    "\n",
    "#     enc_sentence = f\"{text_in}; structed knowledge: {struct_in}\"\n",
    "#     enc_tokenized = mt.tokenizer(enc_sentence)\n",
    "#     ex['enc_sentence'] = enc_sentence\n",
    "#     ex['enc_tokenized'] = enc_tokenized\n",
    "    \n",
    "#     parsed_struct_in = ctu.parse_struct_in(struct_in)\n",
    "    \n",
    "#     token_ranges_dict = ctu.find_struct_name_ranges(mt.tokenizer, ex)\n",
    "#     text_range = ex['text_range']\n",
    "#     struct_range = ex['struct_range']\n",
    "#     # token_ranges_dict = ex['struct_node_ranges_dict']\n",
    "\n",
    "#     all_mix_masks = [build_enc_self_attention_mask_for_section_pair(\n",
    "#             ex,\n",
    "#             q_sect,\n",
    "#             k_sect,\n",
    "#             mt=mt) for q_sect, k_sect in section_pairs]\n",
    "    \n",
    "#     mix_mask = torch.logical_or(*all_mix_masks) if len(all_mix_masks) > 1 else all_mix_masks[0]\n",
    "    \n",
    "#     def _attn_w_fn(attn):\n",
    "#         _mix_mask = mix_mask.to(device=attn.device)\n",
    "#         _zero = torch.tensor(0, dtype=attn.dtype, device=attn.device)\n",
    "#         attn = torch.where(_mix_mask, _zero, attn)     # no need to keep batch_idx=0 clean here\n",
    "#         return attn\n",
    "\n",
    "#     def _attn_lg_fn(attn):\n",
    "#         _mix_mask = mix_mask.to(device=attn.device)\n",
    "#         _neg = torch.tensor(-1e9, dtype=attn.dtype, device=attn.device)\n",
    "#         attn = torch.where(_mix_mask, _neg, attn)\n",
    "#         return attn\n",
    "\n",
    "#     def p_hook_fn(m, inp):\n",
    "#         if attn_corrupt_type == 'weights':\n",
    "#             m.ext_attention_weights_fn = _attn_w_fn\n",
    "#         elif attn_corrupt_type == 'logits':\n",
    "#             m.ext_attention_logits_fn = _attn_lg_fn\n",
    "#         else:\n",
    "#             raise ValueError(attn_corrupt_type)\n",
    "\n",
    "#     def f_hook_fn(m, inp, outp):\n",
    "#         m.ext_attention_weights_fn = None\n",
    "#         m.ext_attention_logits_fn = None\n",
    "\n",
    "#     all_hooks = []\n",
    "    \n",
    "#     for layer in mix_layers:\n",
    "#         m = nethook.get_module(mt.model, layer)\n",
    "#         # print(m)\n",
    "#         p_hook = m.register_forward_pre_hook(p_hook_fn)\n",
    "#         f_hook = m.register_forward_hook(f_hook_fn)\n",
    "#         all_hooks.extend([p_hook, f_hook])\n",
    "\n",
    "#     return all_hooks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old test for exp A1.0 hook-implementation \n",
    "\n",
    "# mix_layers = [ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in range(24)]\n",
    "# section_pairs = [('all', 'all')]\n",
    "\n",
    "# all_hooks = add_attention_edit_hook(\n",
    "#     mt=mt_uskg,\n",
    "#     ex=ex,\n",
    "#     mix_layers=mix_layers,\n",
    "#     section_pairs=section_pairs,\n",
    "#     attn_corrupt_type='weights')\n",
    "\n",
    "# pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "\n",
    "# for hook in all_hooks:\n",
    "#     hook.remove()\n",
    "\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer = all; Sect = t<->s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mix_layers = [ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in range(24)]\n",
    "section_pairs = [('struct', 'text'), ('text', 'struct')]\n",
    "\n",
    "all_preds = []\n",
    "eval_sql_results = []\n",
    "\n",
    "for ex_id, ex in enumerate(tqdm(processed_spider_dev[::111])):\n",
    "    with add_enc_attention_edit(mt=mt_uskg,\n",
    "                            ex=ex,\n",
    "                            mix_layers=mix_layers,\n",
    "                            section_pairs=section_pairs,\n",
    "                            attn_corrupt_type='weights'):\n",
    "        pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "    \n",
    "    eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "    \n",
    "    all_preds.append(pred)\n",
    "    eval_sql_results.append(eval_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 0.3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "avg_exact, avg_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = os.path.join(a1_0_result_dir, 'predictions-layer=all-sect=t<->s.txt')\n",
    "eval_path = os.path.join(a1_0_result_dir, 'evals-layer=all-sect=t<->s.jsonl')\n",
    "\n",
    "with open(pred_path, 'w') as f:\n",
    "    for p in all_preds:\n",
    "        f.write(p + '\\n')\n",
    "\n",
    "with open(eval_path, 'w') as f:\n",
    "    for e in eval_sql_results:\n",
    "        f.write(json.dumps(e, indent=None) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer = all; Sect = t->s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mix_layers = [ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in range(24)]\n",
    "section_pairs = [('text', 'struct')]\n",
    "\n",
    "all_preds = []\n",
    "eval_sql_results = []\n",
    "\n",
    "for ex_id, ex in enumerate(tqdm(processed_spider_dev[::111])):\n",
    "    with add_enc_attention_edit(mt=mt_uskg,\n",
    "                            ex=ex,\n",
    "                            mix_layers=mix_layers,\n",
    "                            section_pairs=section_pairs,\n",
    "                            attn_corrupt_type='weights'):\n",
    "        pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "    \n",
    "    eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "        \n",
    "    all_preds.append(pred)\n",
    "    eval_sql_results.append(eval_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.4)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "avg_exact, avg_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = os.path.join(a1_0_result_dir, 'predictions-layer=all-sect=t->s.txt')\n",
    "eval_path = os.path.join(a1_0_result_dir, 'evals-layer=all-sect=t->s.jsonl')\n",
    "\n",
    "with open(pred_path, 'w') as f:\n",
    "    for p in all_preds:\n",
    "        f.write(p + '\\n')\n",
    "\n",
    "with open(eval_path, 'w') as f:\n",
    "    for e in eval_sql_results:\n",
    "        f.write(json.dumps(e, indent=None) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer = all; Sect = s->t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mix_layers = [ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in range(24)]\n",
    "section_pairs = [('struct', 'text')]\n",
    "\n",
    "all_preds = []\n",
    "eval_sql_results = []\n",
    "\n",
    "for ex_id, ex in enumerate(tqdm(processed_spider_dev[::111])):\n",
    "    with add_enc_attention_edit(mt=mt_uskg,\n",
    "                            ex=ex,\n",
    "                            mix_layers=mix_layers,\n",
    "                            section_pairs=section_pairs,\n",
    "                            attn_corrupt_type='weights'):\n",
    "        pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "    \n",
    "    eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "        \n",
    "    all_preds.append(pred)\n",
    "    eval_sql_results.append(eval_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.4)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "avg_exact, avg_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = os.path.join(a1_0_result_dir, 'predictions-layer=all-sect=s->t.txt')\n",
    "eval_path = os.path.join(a1_0_result_dir, 'evals-layer=all-sect=s->t.jsonl')\n",
    "\n",
    "with open(pred_path, 'w') as f:\n",
    "    for p in all_preds:\n",
    "        f.write(p + '\\n')\n",
    "\n",
    "with open(eval_path, 'w') as f:\n",
    "    for e in eval_sql_results:\n",
    "        f.write(json.dumps(e, indent=None) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer = low; Sect = s->t\n",
    "- Using \"s->t\" since it's shown ineffective (though better than t->s) and give the potential of isolated (non-contextualized) struct embedding in applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mix_layers = [ctu.layername_uskg(mt_uskg.model, 'encoder', l, 'self_attn') for l in range(12)]\n",
    "section_pairs = [('struct', 'text')]\n",
    "\n",
    "all_preds = []\n",
    "eval_sql_results = []\n",
    "\n",
    "for ex_id, ex in enumerate(tqdm(processed_spider_dev[::111])):\n",
    "    with add_enc_attention_edit(mt=mt_uskg,\n",
    "                            ex=ex,\n",
    "                            mix_layers=mix_layers,\n",
    "                            section_pairs=section_pairs,\n",
    "                            attn_corrupt_type='weights'):\n",
    "        pred = pred_sql(mt_uskg, ex, padding=False)\n",
    "    \n",
    "    eval_res = evaluate_sql(evaluator, db_name=ex['db_id'], gold=ex['seq_out'], predicted=pred)\n",
    "            \n",
    "    all_preds.append(pred)\n",
    "    eval_sql_results.append(eval_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.5)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_exact = np.mean([d['exact'] for d in eval_sql_results])\n",
    "avg_exec = np.mean([d['exec'] for d in eval_sql_results])\n",
    "avg_exact, avg_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = os.path.join(a1_0_result_dir, 'predictions-layer=low-sect=s->t.txt')\n",
    "eval_path = os.path.join(a1_0_result_dir, 'evals-layer=low-sect=s->t.jsonl')\n",
    "\n",
    "with open(pred_path, 'w') as f:\n",
    "    for p in all_preds:\n",
    "        f.write(p + '\\n')\n",
    "\n",
    "with open(eval_path, 'w') as f:\n",
    "    for e in eval_sql_results:\n",
    "        f.write(json.dumps(e, indent=None) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "706px",
    "left": "38px",
    "top": "171px",
    "width": "259px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
